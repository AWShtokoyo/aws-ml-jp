{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39d73d16-fce1-4340-8857-9a7815f0773f",
   "metadata": {},
   "source": [
    "# 日本語 BERT Base Model Fine-tuning & Deployment on Inferentia2/Trainium\n",
    "本 Notebook の元ネタのブログはこちらから\n",
    "+ https://aws.amazon.com/jp/blogs/news/aws-trainium-amazon-ec2-trn1-ml-training-part1/\n",
    "+ https://aws.amazon.com/jp/blogs/news/aws-trainium-amazon-ec2-trn1-ml-training-part2/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0641e6b-67e8-4f10-86ef-1ce00c20162b",
   "metadata": {},
   "source": [
    "## 事前準備\n",
    "本 notebook　は Neuron 2.13.2 環境下で動作確認しています"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eaf706fb-f236-4b87-870f-4e7f81806dcc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: pip in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (23.2.1)\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: transformers[ja]==4.27.4 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (4.27.4)\n",
      "Requirement already satisfied: datasets in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (2.14.4)\n",
      "Requirement already satisfied: filelock in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from transformers[ja]==4.27.4) (3.12.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from transformers[ja]==4.27.4) (0.16.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from transformers[ja]==4.27.4) (1.21.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from transformers[ja]==4.27.4) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from transformers[ja]==4.27.4) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from transformers[ja]==4.27.4) (2023.8.8)\n",
      "Requirement already satisfied: requests in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from transformers[ja]==4.27.4) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from transformers[ja]==4.27.4) (0.13.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from transformers[ja]==4.27.4) (4.66.1)\n",
      "Requirement already satisfied: fugashi>=1.0 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from transformers[ja]==4.27.4) (1.3.0)\n",
      "Requirement already satisfied: ipadic<2.0,>=1.0.0 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from transformers[ja]==4.27.4) (1.0.0)\n",
      "Requirement already satisfied: unidic-lite>=1.0.7 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from transformers[ja]==4.27.4) (1.0.8)\n",
      "Requirement already satisfied: unidic>=1.0.2 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from transformers[ja]==4.27.4) (1.1.0)\n",
      "Requirement already satisfied: sudachipy>=0.6.6 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from transformers[ja]==4.27.4) (0.6.7)\n",
      "Requirement already satisfied: sudachidict-core>=20220729 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from transformers[ja]==4.27.4) (20230711)\n",
      "Requirement already satisfied: rhoknp>=1.1.0 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from transformers[ja]==4.27.4) (1.3.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from datasets) (13.0.0)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from datasets) (2.0.3)\n",
      "Requirement already satisfied: xxhash in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from datasets) (3.3.0)\n",
      "Requirement already satisfied: multiprocess in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from datasets) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from datasets) (3.8.5)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from aiohttp->datasets) (3.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers[ja]==4.27.4) (4.7.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from packaging>=20.0->transformers[ja]==4.27.4) (3.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from requests->transformers[ja]==4.27.4) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from requests->transformers[ja]==4.27.4) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from requests->transformers[ja]==4.27.4) (2023.7.22)\n",
      "Requirement already satisfied: wasabi<1.0.0,>=0.6.0 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from unidic>=1.0.2->transformers[ja]==4.27.4) (0.10.1)\n",
      "Requirement already satisfied: plac<2.0.0,>=1.1.3 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from unidic>=1.0.2->transformers[ja]==4.27.4) (1.3.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "\u001b[33mWARNING: Skipping wandb as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -U pip\n",
    "!pip install -U transformers[ja]==4.27.4 datasets\n",
    "!pip uninstall -y wandb # if installed, needs to setup wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8aa82ee-39d2-4e6c-9294-b920a1284890",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aws-neuronx-runtime-discovery 2.9\n",
      "libneuronxla                  0.5.440\n",
      "neuronx-cc                    2.9.0.40+07376825f\n",
      "neuronx-distributed           0.3.0\n",
      "neuronx-hwm                   2.9.0.2+f79d59e7b\n",
      "pytorch-lightning             1.8.6\n",
      "sentence-transformers         2.2.2\n",
      "torch                         1.13.1\n",
      "torch-neuronx                 1.13.1.1.10.1\n",
      "torch-xla                     1.13.1+torchneurona\n",
      "torchmetrics                  0.10.3\n",
      "torchvision                   0.14.1\n",
      "transformers                  4.27.4\n",
      "transformers-neuronx          0.6.106\n"
     ]
    }
   ],
   "source": [
    "!pip list | grep \"neuron\\|torch\\|transformers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf23e3b2-6c97-411a-b0f0-e324b9f0f2a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ii  aws-neuronx-collectives                2.16.16.0-e59c7bb3e               amd64        neuron_ccom built using CMake\n",
      "ii  aws-neuronx-dkms                       2.12.18.0                         amd64        aws-neuronx driver in DKMS format.\n",
      "hi  aws-neuronx-oci-hook                   2.2.16.0                          amd64        neuron_oci_hook built using CMake\n",
      "ii  aws-neuronx-runtime-lib                2.16.14.0-61fdc395f               amd64        neuron_runtime built using CMake\n",
      "ii  aws-neuronx-tools                      2.13.4.0                          amd64        Neuron profile and debug tools\n"
     ]
    }
   ],
   "source": [
    "!dpkg --list | grep neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2168c77-6ac7-4f71-95c7-06e6df4386fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!sudo rmmod neuron; sudo modprobe neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87668961-d047-4603-a3b1-a05544713952",
   "metadata": {},
   "source": [
    "## データセットの準備\n",
    "本テストでは、Huggingface Hub で利用可能な以下のセンチメント（感情）データセットのうち、日本語のサブセットを使用します。\n",
    "https://huggingface.co/datasets/tyqiangz/multilingual-sentiments\n",
    "\n",
    "本テストではテキストデータをPositiveかNegativeに分類する 2 クラスの分類問題として扱うことにします。元々のデータセットは positive(LABEL_0)、neutral(LABEL_1)、negative(LABEL_2)としてラベル付けされていますが、neutralのデータは使用しないこととし、ラベルをpositive(LABEL_0)、negative(LABEL_1)として再定義します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53189385-ef49-45a3-b87a-9955c91395ec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '箱を開けただけで予想以上に匂いが甘くて 私には合わないかもです。 でも もったいないので今使ってるのが無くなったら使ってみます。', 'labels': 1}\n",
      "{'text': '色々な変換アダプが売ってますけど長さも結構ありますし重宝しています。主にタブレットから→テレビに使ってるんですけど子供達に動画を見せる時タブレットを直接見せるよりテレビで大きな画面で見せた方が良いですね。なんの設定も要らないので直接繋げればすぐ使える所も気に入っています。', 'labels': 0}\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertJapaneseTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Prepare dataset\n",
    "dataset = load_dataset(\"tyqiangz/multilingual-sentiments\", \"japanese\")\n",
    "dataset = dataset.remove_columns([\"source\"])\n",
    "dataset = dataset.filter(lambda dataset: dataset[\"label\"] != 1)\n",
    "dataset = dataset.map(lambda dataset: {\"labels\": int(dataset[\"label\"] == 2)}, remove_columns=[\"label\"])\n",
    "\n",
    "print(dataset[\"train\"][20000])\n",
    "print(dataset[\"train\"][50000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0481f80-52cf-4255-a76e-c49a948247ec",
   "metadata": {},
   "source": [
    "次に、文章テキストのままだとモデルのトレーニングはできないため、テキストを意味のある単位で分割（トークナイズ）した上で数値に変換します。トークナイザーには MeCab ベースの BertJapaneseTokenizer を利用しました。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3f7c32e-c0c0-4a1e-92c2-4b6490f68917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50c01ab334cc45c1b923a7b426ad55d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/4000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc94dc93a6e3482c94316eb1c2259cc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/256 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MODEL_NAME = \"cl-tohoku/bert-base-japanese-whole-word-masking\"\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", max_length=128, truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"text\"])\n",
    "\n",
    "train_dataset = tokenized_datasets[\"train\"].shuffle().select(range(4000))\n",
    "eval_dataset = tokenized_datasets[\"test\"].shuffle().select(range(256))\n",
    "\n",
    "# Save dataset\n",
    "train_dataset.save_to_disk(\"./train/\")\n",
    "eval_dataset.save_to_disk(\"./test/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4005724-1c52-4426-b7cb-16996462d29d",
   "metadata": {},
   "source": [
    "実際にどのように変換されているのか、以下のスクリプトを実行し確認してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "229a9403-e9b7-407e-a6ce-71ecd69697d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '箱を開けただけで予想以上に匂いが甘くて 私には合わないかもです。 でも もったいないので今使ってるのが無くなったら使ってみます。', 'labels': 1}\n",
      "Tokenize: ['箱', 'を', '開け', 'た', 'だけ', 'で', '予想', '以上', 'に', '匂い', 'が', '甘', '##く', 'て', '私', 'に', 'は', '合わ', 'ない', 'かも', 'です', '。', 'でも', 'もっ', '##たい', '##ない', 'ので', '今', '使っ', 'てる', 'の', 'が', '無くなっ', 'たら', '使っ', 'て', 'み', 'ます', '。']\n",
      "Encode: [2, 3684, 11, 9709, 10, 687, 12, 4663, 695, 7, 23071, 14, 5063, 28504, 16, 1325, 7, 9, 13964, 80, 4830, 2992, 8, 962, 1750, 4541, 3721, 947, 744, 2110, 7134, 5, 14, 15446, 3318, 2110, 16, 546, 2610, 8, 3]\n"
     ]
    }
   ],
   "source": [
    "index = 20000\n",
    "print(dataset[\"train\"][index])\n",
    "print('Tokenize:', tokenizer.tokenize(dataset[\"train\"]['text'][index]))\n",
    "print('Encode:', tokenizer.encode(dataset[\"train\"]['text'][index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b00108-8105-4190-b6ad-27effb7092d5",
   "metadata": {},
   "source": [
    "## Trainer API を使用した トレーニング（ファインチューニング）実行\n",
    "Transformers には Trainer という便利なクラスがあり、Torch Neuron からも利用可能です。 ここでは Trainer API を利用してトレーニングを実行していきます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36508a19-4f9d-483c-b118-bbf66b2e85ef",
   "metadata": {},
   "source": [
    "### neuron_parallel_compile による事前コンパイル\n",
    "トレーニングの各ステップでは、グラフがトレースされ、トレースされたグラフが以前のものと異なる場合は、再度計算グラフのコンパイルが発生します。大規模なモデルの場合、各グラフのコンパイル時間が長くなることがあり、トレーニング時間の中で占めるコンパイル時間がボトルネックとなってしまう場合もあり得ます。このコンパイル時間を短縮するため、PyTorch Neuron では neuron_parallel_compile ユーティリティが提供されています。neuron_parallel_compile は、スクリプトの試行からグラフを抽出し並列事前コンパイルを実施、コンパイル結果（NEFF : Neuron Executable File Format）をキャッシュとしてディスク上に保持します。\n",
    "\n",
    "では実際に事前コンパイルを実行してみましょう。以下の内容でbert-jp-precompile.pyというファイル名の Python スクリプトを作成し実行します。スクリプトは基本的にこの後実行するトレーニングスクリプトと同じ内容ですが、neuron_parallel_compileはグラフの高速コンパイルのみを目的とし実際の演算は実行されず、出力結果は無効となります。トレーニング実行中も必要に応じてグラフはコンパイルされるため、この事前コンパイルのプロセスはスキップし、次のトレーニング処理に直接進んでも問題はありません。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6acfe7-0dc2-4341-af96-f837bc627583",
   "metadata": {},
   "source": [
    "コンパイル時間を短縮するためデータセット、epoch 数を制限している点にご注意ください。コンパイル結果は `/var/tmp/neuron-compile-cache/` 以下に保存されています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70c8f769-91a2-4966-8b55-2e1df787d716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting bert-jp-precompile.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile bert-jp-precompile.py\n",
    "\n",
    "from transformers import BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_from_disk\n",
    "import torch, torch_xla.core.xla_model as xm\n",
    "import os\n",
    "\n",
    "os.environ[\"NEURON_CC_FLAGS\"] = \"--model-type=transformer\"\n",
    "\n",
    "device = xm.xla_device()\n",
    "\n",
    "MODEL_NAME = \"cl-tohoku/bert-base-japanese-whole-word-masking\"\n",
    "model = BertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2).to(device)\n",
    "\n",
    "train_dataset = load_from_disk(\"./train/\").with_format(\"torch\")\n",
    "train_dataset = train_dataset.select(range(64))\n",
    "\n",
    "eval_dataset = load_from_disk(\"./test/\").with_format(\"torch\")\n",
    "eval_dataset = eval_dataset.select(range(64))\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    num_train_epochs = 2,\n",
    "    learning_rate = 5e-5,\n",
    "    per_device_train_batch_size = 8,\n",
    "    per_device_eval_batch_size = 8,\n",
    "    output_dir = \"./results\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = eval_dataset,\n",
    ")\n",
    "\n",
    "train_result = trainer.train()\n",
    "\n",
    "eval_result = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c4c135-cd4e-45a3-a2af-99f2a0fbe921",
   "metadata": {},
   "source": [
    "環境変数`XLA_USE_BF16=1`　を設定することで、BF16 + Stochastic Rounding で実行します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6db73bea-dcee-4d21-badc-ea7844418195",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-05 08:41:16.000784:  3606836  INFO ||PARALLEL_COMPILE||: Removing existing workdir /tmp/parallel_compile_workdir\n",
      "2023-09-05 08:41:16.000788:  3606836  INFO ||PARALLEL_COMPILE||: Running trial run (add option to terminate trial run early; also ignore trial run's generated outputs, i.e. loss, checkpoints)\n",
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]2023-09-05 08:41:32.000986: INFO ||NCC_WRAPPER||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "2023-09-05 08:41:32.000986: INFO ||NCC_WRAPPER||: Extracting graphs (/var/tmp/neuron-compile-cache/neuronxcc-2.9.0.40+07376825f/MODULE_15770068861141756492+abb26765/model.hlo.pb) for ahead-of-time parallel compilation. No compilation was done.\n",
      "  6%|▋         | 1/16 [00:00<00:08,  1.86it/s]2023-09-05 08:41:34.000189: INFO ||NCC_WRAPPER||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "2023-09-05 08:41:34.000190: INFO ||NCC_WRAPPER||: Extracting graphs (/var/tmp/neuron-compile-cache/neuronxcc-2.9.0.40+07376825f/MODULE_16268669036401171959+abb26765/model.hlo.pb) for ahead-of-time parallel compilation. No compilation was done.\n",
      " 12%|█▎        | 2/16 [00:02<00:19,  1.38s/it]2023-09-05 08:41:36.000231: INFO ||NCC_WRAPPER||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "2023-09-05 08:41:36.000232: INFO ||NCC_WRAPPER||: Extracting graphs (/var/tmp/neuron-compile-cache/neuronxcc-2.9.0.40+07376825f/MODULE_14409091931058726813+abb26765/model.hlo.pb) for ahead-of-time parallel compilation. No compilation was done.\n",
      " 94%|█████████▍| 15/16 [00:05<00:00,  6.56it/s]2023-09-05 08:41:38.000935: INFO ||NCC_WRAPPER||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "2023-09-05 08:41:38.000936: INFO ||NCC_WRAPPER||: Extracting graphs (/var/tmp/neuron-compile-cache/neuronxcc-2.9.0.40+07376825f/MODULE_12573384035295665077+abb26765/model.hlo.pb) for ahead-of-time parallel compilation. No compilation was done.\n",
      "100%|██████████| 16/16 [00:06<00:00,  2.44it/s]2023-09-05 08:41:40.000221: INFO ||NCC_WRAPPER||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "2023-09-05 08:41:40.000223: INFO ||NCC_WRAPPER||: Extracting graphs (/var/tmp/neuron-compile-cache/neuronxcc-2.9.0.40+07376825f/MODULE_7741650751003820744+abb26765/model.hlo.pb) for ahead-of-time parallel compilation. No compilation was done.\n",
      "2023-09-05 08:41:41.000659: INFO ||NCC_WRAPPER||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "2023-09-05 08:41:41.000660: INFO ||NCC_WRAPPER||: Extracting graphs (/var/tmp/neuron-compile-cache/neuronxcc-2.9.0.40+07376825f/MODULE_2296725661428058300+abb26765/model.hlo.pb) for ahead-of-time parallel compilation. No compilation was done.\n",
      "2023-09-05 08:41:41.000962: INFO ||NCC_WRAPPER||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "2023-09-05 08:41:41.000962: INFO ||NCC_WRAPPER||: Extracting graphs (/var/tmp/neuron-compile-cache/neuronxcc-2.9.0.40+07376825f/MODULE_13837218930383410181+abb26765/model.hlo.pb) for ahead-of-time parallel compilation. No compilation was done.\n",
      "\n",
      "{'train_runtime': 6.5761, 'train_samples_per_second': 19.464, 'train_steps_per_second': 2.433, 'train_loss': 0.5, 'epoch': 2.0}\n",
      "100%|██████████| 8/8 [00:00<00:00, 36.53it/s]\n",
      "2023-09-05 08:41:43.000082:  3606836  INFO ||PARALLEL_COMPILE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "2023-09-05 08:41:43.000088:  3606836  INFO ||PARALLEL_COMPILE||: Current remaining items are 7, locked are 0, failed are 0, done are 0, total is 7\n",
      "2023-09-05 08:41:43.000088:  3606836  INFO ||PARALLEL_COMPILE||: Current remaining items are 7, locked are 0, failed are 0, done are 0, total is 7\n",
      "2023-09-05 08:41:43.000089:  3606836  INFO ||PARALLEL_COMPILE||: Current remaining items are 7, locked are 0, failed are 0, done are 0, total is 7\n",
      "2023-09-05 08:41:43.000089:  3606836  INFO ||PARALLEL_COMPILE||: Current remaining items are 6, locked are 1, failed are 0, done are 0, total is 7\n",
      "2023-09-05 08:41:43.000089:  3606836  INFO ||PARALLEL_COMPILE||: Current remaining items are 5, locked are 2, failed are 0, done are 0, total is 7\n",
      "2023-09-05 08:41:43.000090:  3606836  INFO ||PARALLEL_COMPILE||: Current remaining items are 5, locked are 2, failed are 0, done are 0, total is 7\n",
      "2023-09-05 08:41:43.000090:  3606836  INFO ||PARALLEL_COMPILE||: Current remaining items are 5, locked are 2, failed are 0, done are 0, total is 7\n",
      "2023-09-05 08:41:43.000090:  3606836  INFO ||PARALLEL_COMPILE||: Current remaining items are 5, locked are 2, failed are 0, done are 0, total is 7\n",
      "2023-09-05 08:41:43.000092:  3606836  INFO ||PARALLEL_COMPILE||: Current remaining items are 3, locked are 4, failed are 0, done are 0, total is 7\n",
      "2023-09-05 08:41:43.000095:  3606836  INFO ||PARALLEL_COMPILE||: Current remaining items are 1, locked are 6, failed are 0, done are 0, total is 7\n",
      "2023-09-05 08:41:43.000096:  3606836  INFO ||PARALLEL_COMPILE||: Current remaining items are 1, locked are 6, failed are 0, done are 0, total is 7\n",
      "2023-09-05 08:41:43.000101:  3606836  INFO ||PARALLEL_COMPILE||: Current remaining items are 0, locked are 7, failed are 0, done are 0, total is 7\n",
      "2023-09-05 08:41:43.000101:  3606836  INFO ||PARALLEL_COMPILE||: Progress Status: No more compile jobs!\n",
      ".......\n",
      "Compiler status PASS\n",
      "2023-09-05 08:41:44.000643:  3606836  INFO ||PARALLEL_COMPILE||: Current remaining items are 0, locked are 6, failed are 0, done are 1, total is 7\n",
      "2023-09-05 08:41:44.000643:  3606836  INFO ||PARALLEL_COMPILE||: Progress Status: No more compile jobs!\n",
      "\n",
      "Compiler status PASS\n",
      "2023-09-05 08:41:45.000148:  3606836  INFO ||PARALLEL_COMPILE||: Current remaining items are 0, locked are 5, failed are 0, done are 2, total is 7\n",
      "2023-09-05 08:41:45.000148:  3606836  INFO ||PARALLEL_COMPILE||: Progress Status: No more compile jobs!\n",
      ".....\n",
      "Compiler status PASS\n",
      "2023-09-05 08:42:13.000667:  3606836  INFO ||PARALLEL_COMPILE||: Current remaining items are 0, locked are 4, failed are 0, done are 3, total is 7\n",
      "2023-09-05 08:42:13.000667:  3606836  INFO ||PARALLEL_COMPILE||: Progress Status: No more compile jobs!\n",
      "\n",
      "Compiler status PASS\n",
      "2023-09-05 08:42:19.000451:  3606836  INFO ||PARALLEL_COMPILE||: Current remaining items are 0, locked are 3, failed are 0, done are 4, total is 7\n",
      "2023-09-05 08:42:19.000451:  3606836  INFO ||PARALLEL_COMPILE||: Progress Status: No more compile jobs!\n",
      ".................................\n",
      "Compiler status PASS\n",
      "2023-09-05 08:45:56.000286:  3606836  INFO ||PARALLEL_COMPILE||: Current remaining items are 0, locked are 2, failed are 0, done are 5, total is 7\n",
      "2023-09-05 08:45:56.000286:  3606836  INFO ||PARALLEL_COMPILE||: Progress Status: No more compile jobs!\n",
      "......\n",
      "Compiler status PASS\n",
      "2023-09-05 08:47:02.000650:  3606836  INFO ||PARALLEL_COMPILE||: Current remaining items are 0, locked are 1, failed are 0, done are 6, total is 7\n",
      "2023-09-05 08:47:02.000650:  3606836  INFO ||PARALLEL_COMPILE||: Progress Status: No more compile jobs!\n",
      ".\n",
      "Compiler status PASS\n",
      "2023-09-05 08:47:05.000918:  3606836  INFO ||PARALLEL_COMPILE||: Current remaining items are 0, locked are 0, failed are 0, done are 7, total is 7\n",
      "2023-09-05 08:47:05.000918:  3606836  INFO ||PARALLEL_COMPILE||: Progress Status: No more compile jobs!\n",
      "2023-09-05 08:47:05.000919:  3606836  INFO ||PARALLEL_COMPILE||: {\n",
      "    \"compilation_summary\": {\n",
      "        \"SUCCESS\": 7\n",
      "    },\n",
      "    \"compilation_report\": {\n",
      "        \"/var/tmp/neuron-compile-cache/neuronxcc-2.9.0.40+07376825f/MODULE_13837218930383410181+abb26765/model.hlo.pb\": {\n",
      "            \"status\": \"SUCCESS\",\n",
      "            \"retry\": 0\n",
      "        },\n",
      "        \"/var/tmp/neuron-compile-cache/neuronxcc-2.9.0.40+07376825f/MODULE_15770068861141756492+abb26765/model.hlo.pb\": {\n",
      "            \"status\": \"SUCCESS\",\n",
      "            \"retry\": 0\n",
      "        },\n",
      "        \"/var/tmp/neuron-compile-cache/neuronxcc-2.9.0.40+07376825f/MODULE_2296725661428058300+abb26765/model.hlo.pb\": {\n",
      "            \"status\": \"SUCCESS\",\n",
      "            \"retry\": 0\n",
      "        },\n",
      "        \"/var/tmp/neuron-compile-cache/neuronxcc-2.9.0.40+07376825f/MODULE_12573384035295665077+abb26765/model.hlo.pb\": {\n",
      "            \"status\": \"SUCCESS\",\n",
      "            \"retry\": 0\n",
      "        },\n",
      "        \"/var/tmp/neuron-compile-cache/neuronxcc-2.9.0.40+07376825f/MODULE_16268669036401171959+abb26765/model.hlo.pb\": {\n",
      "            \"status\": \"SUCCESS\",\n",
      "            \"retry\": 0\n",
      "        },\n",
      "        \"/var/tmp/neuron-compile-cache/neuronxcc-2.9.0.40+07376825f/MODULE_7741650751003820744+abb26765/model.hlo.pb\": {\n",
      "            \"status\": \"SUCCESS\",\n",
      "            \"retry\": 0\n",
      "        },\n",
      "        \"/var/tmp/neuron-compile-cache/neuronxcc-2.9.0.40+07376825f/MODULE_14409091931058726813+abb26765/model.hlo.pb\": {\n",
      "            \"status\": \"SUCCESS\",\n",
      "            \"retry\": 0\n",
      "        }\n",
      "    }\n",
      "}\n",
      "2023-09-05 08:47:05.000920:  3606836  INFO ||PARALLEL_COMPILE||: Total graphs: 7\n",
      "2023-09-05 08:47:05.000920:  3606836  INFO ||PARALLEL_COMPILE||: Total successful compilations: 7\n",
      "2023-09-05 08:47:05.000920:  3606836  INFO ||PARALLEL_COMPILE||: Total failed compilations: 0\n",
      "\n",
      "real\t5m49.296s\n",
      "user\t37m23.401s\n",
      "sys\t3m5.148s\n"
     ]
    }
   ],
   "source": [
    "!time XLA_USE_BF16=1 neuron_parallel_compile python3 bert-jp-precompile.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049a9a17-1e83-4628-a6f4-29fb5dff2d04",
   "metadata": {},
   "source": [
    "### シングルワーカーでのトレーニング実行\n",
    "次に実際にトレーニングを実行してみます。事前コンパイルを実行した場合でも、追加のコンパイルが発生することがあります。一通りのコンパイルが終了した後、2度目以降の実行では、Neuron コアの恩恵を受けた高速トレーニングを体験できます。以下の内容で bert-jp-single.py というファイル名の Python スクリプトを作成し実行してみましょう。\n",
    "\n",
    "先程の事前コンパイルとは異なり、今回は実際にトレーニングを実行するため、用意したデータセット全てに対して epoch = 10 で実行します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78747ec6-3e55-4b1a-b57f-a8f656b069a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting bert-jp-single.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile bert-jp-single.py\n",
    "\n",
    "from transformers import BertForSequenceClassification, BertJapaneseTokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_from_disk\n",
    "import torch, torch_xla.core.xla_model as xm\n",
    "import os\n",
    "\n",
    "os.environ[\"NEURON_CC_FLAGS\"] = \"--model-type=transformer\"\n",
    "\n",
    "device = xm.xla_device()\n",
    "\n",
    "MODEL_NAME = \"cl-tohoku/bert-base-japanese-whole-word-masking\"\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = BertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2).to(device)\n",
    "\n",
    "train_dataset = load_from_disk(\"./train/\").with_format(\"torch\")\n",
    "eval_dataset = load_from_disk(\"./test/\").with_format(\"torch\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    num_train_epochs = 10,\n",
    "    learning_rate = 5e-5,\n",
    "    per_device_train_batch_size = 8,\n",
    "    per_device_eval_batch_size = 8,\n",
    "    output_dir = \"./results\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = eval_dataset,\n",
    "    tokenizer = tokenizer,\n",
    ")\n",
    "\n",
    "train_result = trainer.train()\n",
    "print(train_result)\n",
    "\n",
    "eval_result = trainer.evaluate()\n",
    "print(eval_result)\n",
    "\n",
    "trainer.save_model(\"./results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13e1319d-30c2-4b9d-be36-07ae7a64fa57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|                                                  | 0/5000 [00:00<?, ?it/s]2023-09-05 08:47:19.000567: INFO ||NCC_WRAPPER||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "2023-09-05 08:47:19.000569: INFO ||NCC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.9.0.40+07376825f/MODULE_15770068861141756492+abb26765/model.neff. Exiting with a successfully compiled graph.\n",
      "  0%|                                          | 1/5000 [00:00<45:03,  1.85it/s]2023-09-05 08:47:20.000766: INFO ||NCC_WRAPPER||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "2023-09-05 08:47:20.000869: INFO ||NCC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.9.0.40+07376825f/MODULE_16268669036401171959+abb26765/model.neff. Exiting with a successfully compiled graph.\n",
      "  0%|                                        | 2/5000 [00:02<2:00:41,  1.45s/it]2023-09-05 08:47:23.000737: INFO ||NCC_WRAPPER||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "2023-09-05 08:47:23.000857: INFO ||NCC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.9.0.40+07376825f/MODULE_14409091931058726813+abb26765/model.neff. Exiting with a successfully compiled graph.\n",
      " 10%|████                                    | 500/5000 [00:41<04:50, 15.50it/s]2023-09-05 08:48:00.000830: INFO ||NCC_WRAPPER||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      ".\n",
      "Compiler status PASS\n",
      "{'loss': 0.28, 'learning_rate': 4.5e-05, 'epoch': 1.0}                          \n",
      " 10%|████                                    | 500/5000 [00:51<04:50, 15.50it/s]2023-09-05 08:48:18.000412: INFO ||NCC_WRAPPER||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      ".\n",
      "Compiler status PASS\n",
      "{'loss': 0.136, 'learning_rate': 4e-05, 'epoch': 2.0}                           \n",
      "{'loss': 0.0715, 'learning_rate': 3.5e-05, 'epoch': 3.0}                        \n",
      "{'loss': 0.0325, 'learning_rate': 3e-05, 'epoch': 4.0}                          \n",
      "{'loss': 0.0141, 'learning_rate': 2.5e-05, 'epoch': 5.0}                        \n",
      "{'loss': 0.0078, 'learning_rate': 2e-05, 'epoch': 6.0}                          \n",
      "{'loss': 0.0059, 'learning_rate': 1.5e-05, 'epoch': 7.0}                        \n",
      "{'loss': 0.0027, 'learning_rate': 1e-05, 'epoch': 8.0}                          \n",
      "{'loss': 0.0025, 'learning_rate': 5e-06, 'epoch': 9.0}                          \n",
      "{'loss': 0.0001, 'learning_rate': 0.0, 'epoch': 10.0}                           \n",
      "{'train_runtime': 453.9321, 'train_samples_per_second': 88.119, 'train_steps_per_second': 11.015, 'train_loss': 0.05530634765625, 'epoch': 10.0}\n",
      "100%|███████████████████████████████████████| 5000/5000 [07:33<00:00, 11.02it/s]\n",
      "TrainOutput(global_step=5000, training_loss=0.05530634765625, metrics={'train_runtime': 453.9321, 'train_samples_per_second': 88.119, 'train_steps_per_second': 11.015, 'train_loss': 0.05530634765625, 'epoch': 10.0})\n",
      "2023-09-05 08:54:53.000474: INFO ||NCC_WRAPPER||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "2023-09-05 08:54:53.000492: INFO ||NCC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.9.0.40+07376825f/MODULE_2296725661428058300+abb26765/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-09-05 08:54:53.000811: INFO ||NCC_WRAPPER||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "2023-09-05 08:54:53.000811: INFO ||NCC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.9.0.40+07376825f/MODULE_13837218930383410181+abb26765/model.neff. Exiting with a successfully compiled graph.\n",
      "100%|███████████████████████████████████████████| 32/32 [00:01<00:00, 30.59it/s]\n",
      "{'eval_loss': 0.4651678800582886, 'eval_runtime': 1.7975, 'eval_samples_per_second': 142.424, 'eval_steps_per_second': 17.803, 'epoch': 10.0}\n",
      "\n",
      "real\t7m53.063s\n",
      "user\t8m30.958s\n",
      "sys\t1m34.920s\n"
     ]
    }
   ],
   "source": [
    "!time XLA_USE_BF16=1 python3 bert-jp-single.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aae6d30-f843-45fd-b6ef-65a65f61763e",
   "metadata": {},
   "source": [
    "ステップ数 5000 のトレーニングが 9~10分程で完了しました。\n",
    "\n",
    "トレーニング実行中に、AWS Neuron で提供される `neuron-top` ツールを利用すると、Neuron コア及び vCPU の利用率、アクセラレータメモリ、ホストメモリの利用状況等を確認することができます。inf2.xlarge には、一つの Inferentia2 チップ、チップ内に二つの Neuron コアが搭載されています。結果から、二つある Neuron コア（NC0 及び NC1）のうち一つの Neuron コアのみが利用されていることが分かります。まだ最適化の余地はありそうです。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4bb724-139b-4a2b-9e2c-c66023c65768",
   "metadata": {},
   "source": [
    "生成されたモデルから期待通りの出力が得られるか確認しておきましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e5ea0d5-d506-42b5-9f24-0438bc5e742c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'LABEL_0', 'score': 0.9999337196350098}]\n",
      "[{'label': 'LABEL_1', 'score': 0.9999854564666748}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"text-classification\", model = \"./results/\")\n",
    "\n",
    "print(classifier(\"大変すばらしい商品でした。感激です。\"))\n",
    "print(classifier(\"期待していた商品とは異なりました。残念です。\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfdbb2d-d477-42e3-9967-f64fba883b0d",
   "metadata": {},
   "source": [
    "期待通りの出力を得られることが確認できたようです。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28270199-5a61-4ec1-a045-833abc6c16e7",
   "metadata": {},
   "source": [
    "## torchrun を用いたマルチワーカーでのトレーニング実行\n",
    "それでは、先程のトレーニングスクリプトに変更を加え、二つある Neuron コアを有効活用してみましょう。複数の Neuron コアを利用したマルチワーカーで実行するためには `torchrun` コマンドを利用します。`torchrun` コマンドに対して、オプション `--nproc_per_node` で利用する Neuron コアの数（並列実行するワーカー数）を指定します。trn1.2xlarge (inf2.xlarge) では 2 を、trn1.32xlargeの場合は 2, 8, 32 が指定可能です。\n",
    "\n",
    "`torchrun` を利用したデータパラレルトレーニングを実行するにあたって、先程のスクリプトに一部変更を加えた `bert-jp-dual.py` というファイル名のスクリプトを作成し実行します。\n",
    "\n",
    "それでは変更後のスクリプトを利用して　inf2.xlarge 上の二つ Neuron コアを利用したトレーニングを実行してみましょう。シングルワーカーでのトレーニング結果と比較し `Total train batch size` の値が倍の 16 に、`Total optimization steps` が半分の 2500 となっている点を確認できると思います。\n",
    "\n",
    "シングルワーカー時の手順同様、まずは事前コンパイルを実行し、その後に実際のトレーニングを実行します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f2837ccc-aac9-47d7-803e-3c75afccc426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting bert-jp-dual-precompile.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile bert-jp-dual-precompile.py\n",
    "\n",
    "from transformers import BertForSequenceClassification, BertJapaneseTokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_from_disk\n",
    "import torch, torch_xla.distributed.xla_backend\n",
    "import os\n",
    "\n",
    "os.environ[\"NEURON_CC_FLAGS\"] = \"--model-type=transformer\"\n",
    "\n",
    "MODEL_NAME = \"cl-tohoku/bert-base-japanese-whole-word-masking\"\n",
    "model = BertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
    "\n",
    "train_dataset = load_from_disk(\"./train/\").with_format(\"torch\")\n",
    "train_dataset = train_dataset.select(range(64))\n",
    "\n",
    "eval_dataset = load_from_disk(\"./test/\").with_format(\"torch\")\n",
    "eval_dataset = eval_dataset.select(range(64))\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    num_train_epochs = 2,\n",
    "    learning_rate = 5e-5,\n",
    "    per_device_train_batch_size = 8,\n",
    "    per_device_eval_batch_size = 8,\n",
    "    output_dir = \"./results\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = eval_dataset,\n",
    ")\n",
    "\n",
    "train_result = trainer.train()\n",
    "\n",
    "eval_result = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50d164bb-deb5-4c34-bafa-8c66b2291ff5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-05 08:55:01.000915:  3711014  INFO ||PARALLEL_COMPILE||: Removing existing workdir /tmp/parallel_compile_workdir\n",
      "2023-09-05 08:55:01.000919:  3711014  INFO ||PARALLEL_COMPILE||: Running trial run (add option to terminate trial run early; also ignore trial run's generated outputs, i.e. loss, checkpoints)\n",
      "WARNING:torch.distributed.run:\n",
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]2023-09-05 08:55:18.000007: INFO ||NCC_WRAPPER||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "2023-09-05 08:55:18.000008: INFO ||NCC_WRAPPER||: Extracting graphs (/var/tmp/neuron-compile-cache/neuronxcc-2.9.0.40+07376825f/MODULE_15770068861141756492+d41d8cd9/model.hlo.pb) for ahead-of-time parallel compilation. No compilation was done.\n",
      " 12%|█▎        | 1/8 [00:00<00:01,  3.52it/s]2023-09-05 08:55:19.000218: INFO ||NCC_WRAPPER||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "2023-09-05 08:55:19.000219: INFO ||NCC_WRAPPER||: Extracting graphs (/var/tmp/neuron-compile-cache/neuronxcc-2.9.0.40+07376825f/MODULE_4976758461586571876+d41d8cd9/model.hlo.pb) for ahead-of-time parallel compilation. No compilation was done.\n",
      " 25%|██▌       | 2/8 [00:02<00:07,  1.29s/it]2023-09-05 08:55:21.000265: INFO ||NCC_WRAPPER||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "2023-09-05 08:55:21.000266: INFO ||NCC_WRAPPER||: Extracting graphs (/var/tmp/neuron-compile-cache/neuronxcc-2.9.0.40+07376825f/MODULE_1129850769929414091+d41d8cd9/model.hlo.pb) for ahead-of-time parallel compilation. No compilation was done.\n",
      " 88%|████████▊ | 7/8 [00:04<00:00,  2.13it/s]2023-09-05 08:55:23.000085: INFO ||NCC_WRAPPER||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "2023-09-05 08:55:23.000086: INFO ||NCC_WRAPPER||: Extracting graphs (/var/tmp/neuron-compile-cache/neuronxcc-2.9.0.40+07376825f/MODULE_12573384035295665077+d41d8cd9/model.hlo.pb) for ahead-of-time parallel compilation. No compilation was done.\n",
      "2023-09-05 08:55:23.000992: INFO ||NCC_WRAPPER||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "2023-09-05 08:55:23.000993: INFO ||NCC_WRAPPER||: Extracting graphs (/var/tmp/neuron-compile-cache/neuronxcc-2.9.0.40+07376825f/MODULE_16102519548021684275+d41d8cd9/model.hlo.pb) for ahead-of-time parallel compilation. No compilation was done.\n",
      "{'train_runtime': 5.4073, 'train_samples_per_second': 23.672, 'train_steps_per_second': 1.479, 'train_loss': -6.612390279769897e-08, 'epoch': 2.0}\n",
      "100%|██████████| 8/8 [00:06<00:00,  1.17it/s]\n",
      "2023-09-05 08:55:25.000814: INFO ||NCC_WRAPPER||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "2023-09-05 08:55:25.000815: INFO ||NCC_WRAPPER||: Extracting graphs (/var/tmp/neuron-compile-cache/neuronxcc-2.9.0.40+07376825f/MODULE_9481639979634615951+d41d8cd9/model.hlo.pb) for ahead-of-time parallel compilation. No compilation was done.\n",
      "2023-09-05 08:55:27.000294: INFO ||NCC_WRAPPER||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "2023-09-05 08:55:27.000296: INFO ||NCC_WRAPPER||: Extracting graphs (/var/tmp/neuron-compile-cache/neuronxcc-2.9.0.40+07376825f/MODULE_2296725661428058300+d41d8cd9/model.hlo.pb) for ahead-of-time parallel compilation. No compilation was done.\n",
      "2023-09-05 08:55:27.000534: INFO ||NCC_WRAPPER||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "2023-09-05 08:55:27.000535: INFO ||NCC_WRAPPER||: Extracting graphs (/var/tmp/neuron-compile-cache/neuronxcc-2.9.0.40+07376825f/MODULE_13837218930383410181+d41d8cd9/model.hlo.pb) for ahead-of-time parallel compilation. No compilation was done.\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]/opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages/numpy/core/_methods.py:179: RuntimeWarning: overflow encountered in reduce\n",
      "  ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n",
      "/opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages/numpy/core/_methods.py:179: RuntimeWarning: overflow encountered in reduce\n",
      "  ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n",
      "100%|██████████| 4/4 [00:00<00:00, 46.77it/s]\n",
      "2023-09-05 08:55:33.000520:  3711014  INFO ||PARALLEL_COMPILE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "2023-09-05 08:55:35.000216:  3711014  INFO ||PARALLEL_COMPILE||: Current remaining items are 8, locked are 0, failed are 0, done are 9, total is 17\n",
      "2023-09-05 08:55:36.000177:  3711014  INFO ||PARALLEL_COMPILE||: Current remaining items are 8, locked are 0, failed are 0, done are 9, total is 17\n",
      "2023-09-05 08:55:36.000234:  3711014  INFO ||PARALLEL_COMPILE||: Current remaining items are 8, locked are 0, failed are 0, done are 9, total is 17\n",
      "2023-09-05 08:55:36.000284:  3711014  INFO ||PARALLEL_COMPILE||: Current remaining items are 8, locked are 0, failed are 0, done are 9, total is 17\n",
      "2023-09-05 08:55:36.000287:  3711014  INFO ||PARALLEL_COMPILE||: Current remaining items are 8, locked are 0, failed are 0, done are 9, total is 17\n",
      "2023-09-05 08:55:36.000310:  3711014  INFO ||PARALLEL_COMPILE||: Current remaining items are 8, locked are 0, failed are 0, done are 9, total is 17\n",
      "2023-09-05 08:55:36.000325:  3711014  INFO ||PARALLEL_COMPILE||: Current remaining items are 8, locked are 0, failed are 0, done are 9, total is 17\n",
      "2023-09-05 08:55:36.000342:  3711014  INFO ||PARALLEL_COMPILE||: Current remaining items are 8, locked are 0, failed are 0, done are 9, total is 17\n",
      ".....2023-09-05 08:55:37.000401:  3711014  INFO ||PARALLEL_COMPILE||: Current remaining items are 4, locked are 4, failed are 0, done are 9, total is 17\n",
      "2023-09-05 08:55:37.000420:  3711014  INFO ||PARALLEL_COMPILE||: Current remaining items are 3, locked are 5, failed are 0, done are 9, total is 17\n",
      "2023-09-05 08:55:37.000436:  3711014  INFO ||PARALLEL_COMPILE||: Current remaining items are 3, locked are 5, failed are 0, done are 9, total is 17\n",
      "\n",
      "Compiler status PASS\n",
      "...2023-09-05 08:55:38.000097:  3711014  INFO ||PARALLEL_COMPILE||: Current remaining items are 0, locked are 7, failed are 0, done are 10, total is 17\n",
      "2023-09-05 08:55:38.000097:  3711014  INFO ||PARALLEL_COMPILE||: Progress Status: No more compile jobs!\n",
      "\n",
      "Compiler status PASS\n",
      "2023-09-05 08:55:39.000036:  3711014  INFO ||PARALLEL_COMPILE||: Current remaining items are 0, locked are 6, failed are 0, done are 11, total is 17\n",
      "2023-09-05 08:55:39.000036:  3711014  INFO ||PARALLEL_COMPILE||: Progress Status: No more compile jobs!\n",
      "......\n",
      "Compiler status PASS\n",
      "2023-09-05 08:56:07.000957:  3711014  INFO ||PARALLEL_COMPILE||: Current remaining items are 0, locked are 5, failed are 0, done are 12, total is 17\n",
      "2023-09-05 08:56:07.000957:  3711014  INFO ||PARALLEL_COMPILE||: Progress Status: No more compile jobs!\n",
      "\n",
      "Compiler status PASS\n",
      "2023-09-05 08:56:15.000828:  3711014  INFO ||PARALLEL_COMPILE||: Current remaining items are 0, locked are 4, failed are 0, done are 13, total is 17\n",
      "2023-09-05 08:56:15.000828:  3711014  INFO ||PARALLEL_COMPILE||: Progress Status: No more compile jobs!\n",
      "............................\n",
      "Compiler status PASS\n",
      "2023-09-05 08:58:27.000288:  3711014  INFO ||PARALLEL_COMPILE||: Current remaining items are 0, locked are 3, failed are 0, done are 14, total is 17\n",
      "2023-09-05 08:58:27.000288:  3711014  INFO ||PARALLEL_COMPILE||: Progress Status: No more compile jobs!\n",
      "......\n",
      "Compiler status PASS\n",
      "2023-09-05 08:59:13.000738:  3711014  INFO ||PARALLEL_COMPILE||: Current remaining items are 0, locked are 2, failed are 0, done are 15, total is 17\n",
      "2023-09-05 08:59:13.000738:  3711014  INFO ||PARALLEL_COMPILE||: Progress Status: No more compile jobs!\n",
      "......\n",
      "Compiler status PASS\n",
      "\n",
      "Compiler status PASS\n",
      "2023-09-05 09:00:17.000105:  3711014  INFO ||PARALLEL_COMPILE||: Current remaining items are 0, locked are 1, failed are 0, done are 16, total is 17\n",
      "2023-09-05 09:00:17.000105:  3711014  INFO ||PARALLEL_COMPILE||: Progress Status: No more compile jobs!\n",
      "2023-09-05 09:00:17.000347:  3711014  INFO ||PARALLEL_COMPILE||: Current remaining items are 0, locked are 0, failed are 0, done are 17, total is 17\n",
      "2023-09-05 09:00:17.000347:  3711014  INFO ||PARALLEL_COMPILE||: Progress Status: No more compile jobs!\n",
      "2023-09-05 09:00:17.000348:  3711014  INFO ||PARALLEL_COMPILE||: {\n",
      "    \"compilation_summary\": {\n",
      "        \"SUCCESS\": 8\n",
      "    },\n",
      "    \"compilation_report\": {\n",
      "        \"/var/tmp/neuron-compile-cache/neuronxcc-2.9.0.40+07376825f/MODULE_13837218930383410181+d41d8cd9/model.hlo.pb\": {\n",
      "            \"status\": \"SUCCESS\",\n",
      "            \"retry\": 0\n",
      "        },\n",
      "        \"/var/tmp/neuron-compile-cache/neuronxcc-2.9.0.40+07376825f/MODULE_15770068861141756492+d41d8cd9/model.hlo.pb\": {\n",
      "            \"status\": \"SUCCESS\",\n",
      "            \"retry\": 0\n",
      "        },\n",
      "        \"/var/tmp/neuron-compile-cache/neuronxcc-2.9.0.40+07376825f/MODULE_2296725661428058300+d41d8cd9/model.hlo.pb\": {\n",
      "            \"status\": \"SUCCESS\",\n",
      "            \"retry\": 0\n",
      "        },\n",
      "        \"/var/tmp/neuron-compile-cache/neuronxcc-2.9.0.40+07376825f/MODULE_12573384035295665077+d41d8cd9/model.hlo.pb\": {\n",
      "            \"status\": \"SUCCESS\",\n",
      "            \"retry\": 0\n",
      "        },\n",
      "        \"/var/tmp/neuron-compile-cache/neuronxcc-2.9.0.40+07376825f/MODULE_16102519548021684275+d41d8cd9/model.hlo.pb\": {\n",
      "            \"status\": \"SUCCESS\",\n",
      "            \"retry\": 0\n",
      "        },\n",
      "        \"/var/tmp/neuron-compile-cache/neuronxcc-2.9.0.40+07376825f/MODULE_4976758461586571876+d41d8cd9/model.hlo.pb\": {\n",
      "            \"status\": \"SUCCESS\",\n",
      "            \"retry\": 0\n",
      "        },\n",
      "        \"/var/tmp/neuron-compile-cache/neuronxcc-2.9.0.40+07376825f/MODULE_1129850769929414091+d41d8cd9/model.hlo.pb\": {\n",
      "            \"status\": \"SUCCESS\",\n",
      "            \"retry\": 0\n",
      "        },\n",
      "        \"/var/tmp/neuron-compile-cache/neuronxcc-2.9.0.40+07376825f/MODULE_9481639979634615951+d41d8cd9/model.hlo.pb\": {\n",
      "            \"status\": \"SUCCESS\",\n",
      "            \"retry\": 0\n",
      "        }\n",
      "    }\n",
      "}\n",
      "2023-09-05 09:00:17.000349:  3711014  INFO ||PARALLEL_COMPILE||: Total graphs: 8\n",
      "2023-09-05 09:00:17.000349:  3711014  INFO ||PARALLEL_COMPILE||: Total successful compilations: 8\n",
      "2023-09-05 09:00:17.000349:  3711014  INFO ||PARALLEL_COMPILE||: Total failed compilations: 0\n",
      "\n",
      "real\t5m15.615s\n",
      "user\t32m56.464s\n",
      "sys\t3m32.395s\n"
     ]
    }
   ],
   "source": [
    "!time XLA_USE_BF16=1 neuron_parallel_compile torchrun --nproc_per_node=2 bert-jp-dual-precompile.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60bddfc6-2cb6-4383-b095-e6cb58a9e68c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting bert-jp-dual.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile bert-jp-dual.py\n",
    "\n",
    "from transformers import BertForSequenceClassification, BertJapaneseTokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_from_disk\n",
    "import torch, torch_xla.distributed.xla_backend\n",
    "import os\n",
    "\n",
    "os.environ[\"NEURON_CC_FLAGS\"] = \"--model-type=transformer\"\n",
    "\n",
    "MODEL_NAME = \"cl-tohoku/bert-base-japanese-whole-word-masking\"\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = BertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
    "\n",
    "train_dataset = load_from_disk(\"./train/\").with_format(\"torch\")\n",
    "eval_dataset = load_from_disk(\"./test/\").with_format(\"torch\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    num_train_epochs = 10,\n",
    "    learning_rate = 5e-5,\n",
    "    per_device_train_batch_size = 8,\n",
    "    per_device_eval_batch_size = 8,\n",
    "    output_dir = \"./results\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = eval_dataset,\n",
    "    tokenizer = tokenizer,\n",
    ")\n",
    "\n",
    "train_result = trainer.train()\n",
    "print(train_result)\n",
    "\n",
    "eval_result = trainer.evaluate()\n",
    "print(eval_result)\n",
    "\n",
    "trainer.save_model(\"./results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f73d8ec0-162b-4d6a-bebf-a0b991b08213",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:torch.distributed.run:\n",
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|                                                  | 0/2500 [00:00<?, ?it/s]2023-09-05 09:00:29.000054: INFO ||NCC_WRAPPER||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "2023-09-05 09:00:29.000056: INFO ||NCC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.9.0.40+07376825f/MODULE_15770068861141756492+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "  0%|                                          | 1/2500 [00:00<12:13,  3.41it/s]2023-09-05 09:00:30.000261: INFO ||NCC_WRAPPER||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "2023-09-05 09:00:30.000342: INFO ||NCC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.9.0.40+07376825f/MODULE_4976758461586571876+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-Sep-05 09:00:31.0210 3715897:3716032 [0] nccl_net_ofi_init:1415 CCOM WARN NET/OFI aws-ofi-nccl initialization failed\n",
      "2023-Sep-05 09:00:31.0210 3715897:3716032 [0] init.cc:138 CCOM WARN OFI plugin initNet() failed is EFA enabled?\n",
      "  0%|                                          | 2/2500 [00:02<56:06,  1.35s/it]2023-09-05 09:00:33.000320: INFO ||NCC_WRAPPER||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "2023-09-05 09:00:33.000417: INFO ||NCC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.9.0.40+07376825f/MODULE_1129850769929414091+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      " 20%|████████                                | 500/2500 [00:42<02:33, 13.05it/s]2023-09-05 09:01:12.000049: INFO ||NCC_WRAPPER||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      ".\n",
      "Compiler status PASS\n",
      "2023-09-05 09:01:18.000492: INFO ||NCC_WRAPPER||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      ".\n",
      "Compiler status PASS\n",
      "{'loss': 0.2185, 'learning_rate': 4e-05, 'epoch': 2.0}                          \n",
      " 20%|████████                                | 500/2500 [00:55<02:33, 13.05it/s]2023-09-05 09:01:24.000974: INFO ||NCC_WRAPPER||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      " 20%|████████                                | 500/2500 [01:00<02:33, 13.05it/s].Non-output memory location with no reader {_convert-t86}@SB<0,0>(1x4)#Internal DebugInfo: <_convert||UNDEF||[1, 1, 1]>\n",
      "\n",
      "Compiler status PASS\n",
      "2023-09-05 09:01:43.000164: INFO ||NCC_WRAPPER||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      ".\n",
      "Compiler status PASS\n",
      "{'loss': 0.0425, 'learning_rate': 3e-05, 'epoch': 4.0}                          \n",
      "{'loss': 0.0117, 'learning_rate': 2e-05, 'epoch': 6.0}                          \n",
      "{'loss': 0.0042, 'learning_rate': 1e-05, 'epoch': 8.0}                          \n",
      "{'loss': 0.0019, 'learning_rate': 0.0, 'epoch': 10.0}                           \n",
      "100%|███████████████████████████████████████| 2500/2500 [04:30<00:00, 14.63it/s]2023-09-05 09:04:59.000411: INFO ||NCC_WRAPPER||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      ".Non-output memory location with no reader {_convert-t105}@SB<0,0>(1x4)#Internal DebugInfo: <_convert||UNDEF||[1, 1, 1]>\n",
      "\n",
      "Compiler status PASS\n",
      "{'train_runtime': 270.3879, 'train_samples_per_second': 147.936, 'train_steps_per_second': 9.246, 'train_loss': 0.05576015625, 'epoch': 10.0}\n",
      "TrainOutput(global_step=2500, training_loss=0.05576015625, metrics={'train_runtime': 270.381, 'train_samples_per_second': 147.939, 'train_steps_per_second': 9.246, 'total_flos': 2627832790384640.0, 'train_loss': 0.05576015625, 'epoch': 10.0})\n",
      "100%|███████████████████████████████████████| 2500/2500 [04:36<00:00,  9.03it/s]\n",
      "TrainOutput(global_step=2500, training_loss=0.05576015625, metrics={'train_runtime': 270.3879, 'train_samples_per_second': 147.936, 'train_steps_per_second': 9.246, 'train_loss': 0.05576015625, 'epoch': 10.0})\n",
      "2023-09-05 09:05:06.000176: INFO ||NCC_WRAPPER||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "2023-09-05 09:05:06.000192: INFO ||NCC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.9.0.40+07376825f/MODULE_2296725661428058300+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-09-05 09:05:06.000512: INFO ||NCC_WRAPPER||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "2023-09-05 09:05:06.000513: INFO ||NCC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.9.0.40+07376825f/MODULE_13837218930383410181+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      " 81%|██████████████████████████████████▉        | 13/16 [00:00<00:00, 34.15it/s]{'eval_loss': 0.42790210247039795, 'eval_runtime': 1.1649, 'eval_samples_per_second': 219.769, 'eval_steps_per_second': 13.736, 'epoch': 10.0}\n",
      "100%|███████████████████████████████████████████| 16/16 [00:00<00:00, 34.23it/s]\n",
      "{'eval_loss': 0.42790210247039795, 'eval_runtime': 1.1635, 'eval_samples_per_second': 220.031, 'eval_steps_per_second': 13.752, 'epoch': 10.0}\n",
      "\n",
      "real\t4m56.347s\n",
      "user\t6m15.686s\n",
      "sys\t0m21.887s\n"
     ]
    }
   ],
   "source": [
    "!time XLA_USE_BF16=1 torchrun --nproc_per_node=2 bert-jp-dual.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582fc974-a3fa-45ad-ba0b-1d3ac99d8f33",
   "metadata": {},
   "source": [
    "トレーニング実行中の neuron-top の出力も確認してみましょう。今度は二つの Neuron コアが利用されている事が確認できると思います。トレーニングに要する実行時間も 5~6 分に削減されました。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293c4c41-9152-4b02-91fa-d5897079427f",
   "metadata": {},
   "source": [
    "## 推論実行\n",
    "先ほどは生成されたモデルから期待通りの出力が得られるかどうかCPU上で推論実行し、結果を確認しました。ここでは生成されたモデルをinf2.xlarge上で推論実行します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b881d0f-28b5-49e5-849f-3f0266daa97c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU paraphrase logits: [[ 4.7050467 -4.1804614]]\n",
      "CPU paraphrase logits: [[-4.7470703  4.467029 ]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_neuronx\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import transformers\n",
    "\n",
    "from transformers import BertForSequenceClassification, BertJapaneseTokenizer\n",
    "\n",
    "def encode(tokenizer, *inputs, max_length=128, batch_size=1):\n",
    "    tokens = tokenizer.encode_plus(\n",
    "        *inputs,\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    return (\n",
    "        torch.repeat_interleave(tokens['input_ids'], batch_size, 0),\n",
    "        torch.repeat_interleave(tokens['attention_mask'], batch_size, 0),\n",
    "        torch.repeat_interleave(tokens['token_type_ids'], batch_size, 0),\n",
    "    )\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./results/\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"./results/\", torchscript=True)\n",
    "\n",
    "\n",
    "sequence = \"大変すばらしい商品でした。感激です。\"\n",
    "paraphrase = encode(tokenizer, sequence)\n",
    "cpu_paraphrase_logits = model(*paraphrase)[0]\n",
    "print('CPU paraphrase logits:', cpu_paraphrase_logits.detach().numpy())\n",
    "\n",
    "sequence = \"期待していた商品とは異なりました。残念です。\"\n",
    "paraphrase = encode(tokenizer, sequence)\n",
    "cpu_paraphrase_logits = model(*paraphrase)[0]\n",
    "print('CPU paraphrase logits:', cpu_paraphrase_logits.detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e608f7d-16cb-4dc0-b7ac-253129e64df2",
   "metadata": {},
   "source": [
    "### Compile the model for Neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "09bc7d66-7fdb-42d8-91f3-98708982fee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'model.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ee754ea0-97ae-4cd4-8184-4cd3b7fa6fb8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-05T09:05:21Z Running DoNothing\n",
      "2023-09-05T09:05:21Z DoNothing finished after 0.000 seconds\n",
      "2023-09-05T09:05:21Z Running CanonicalizeIR\n",
      "2023-09-05T09:05:21Z CanonicalizeIR finished after 0.020 seconds\n",
      "2023-09-05T09:05:21Z Running ExpandBatchNorm\n",
      "2023-09-05T09:05:21Z ExpandBatchNorm finished after 0.021 seconds\n",
      "2023-09-05T09:05:21Z Running ResolveComplicatePredicates\n",
      "2023-09-05T09:05:22Z ResolveComplicatePredicates finished after 0.016 seconds\n",
      "2023-09-05T09:05:22Z Running AffinePredicateResolution\n",
      "2023-09-05T09:05:22Z AffinePredicateResolution finished after 0.018 seconds\n",
      "2023-09-05T09:05:22Z Running EliminateDivs\n",
      "2023-09-05T09:05:22Z EliminateDivs finished after 0.018 seconds\n",
      "2023-09-05T09:05:22Z Running PerfectLoopNest\n",
      "2023-09-05T09:05:22Z PerfectLoopNest finished after 0.017 seconds\n",
      "2023-09-05T09:05:22Z Running Simplifier\n",
      "2023-09-05T09:05:22Z Simplifier finished after 0.238 seconds\n",
      "2023-09-05T09:05:22Z Running GenericAccessSimplifier\n",
      "2023-09-05T09:05:22Z GenericAccessSimplifier finished after 0.015 seconds\n",
      "2023-09-05T09:05:22Z Running TCTransform\n",
      "2023-09-05T09:05:22Z TCTransform finished after 0.029 seconds\n",
      "2023-09-05T09:05:22Z Running CommuteConcat\n",
      "2023-09-05T09:05:22Z CommuteConcat finished after 0.015 seconds\n",
      "2023-09-05T09:05:22Z Running TensorOpFusion\n",
      "2023-09-05T09:05:22Z TensorOpFusion finished after 0.025 seconds\n",
      "2023-09-05T09:05:22Z Running TensorOpTransform\n",
      "2023-09-05T09:05:22Z TensorOpTransform finished after 0.083 seconds\n",
      "2023-09-05T09:05:22Z Running LowerTensorOp\n",
      "2023-09-05T09:05:22Z LowerTensorOp finished after 0.027 seconds\n",
      "2023-09-05T09:05:22Z Running MemcpyElimination\n",
      "2023-09-05T09:05:23Z MemcpyElimination finished after 1.170 seconds\n",
      "2023-09-05T09:05:23Z Running LoopFusion\n",
      "2023-09-05T09:05:24Z LoopFusion finished after 0.707 seconds\n",
      "2023-09-05T09:05:24Z Running Simplifier\n",
      "2023-09-05T09:05:24Z Simplifier finished after 0.095 seconds\n",
      "2023-09-05T09:05:24Z Running Delinearization\n",
      "2023-09-05T09:05:24Z Delinearization finished after 0.038 seconds\n",
      "2023-09-05T09:05:24Z Running DeadStoreElimination\n",
      "2023-09-05T09:05:25Z DeadStoreElimination finished after 1.118 seconds\n",
      "2023-09-05T09:05:25Z Running Simplifier\n",
      "2023-09-05T09:05:26Z Simplifier finished after 0.188 seconds\n",
      "2023-09-05T09:05:26Z Running LICM\n",
      "2023-09-05T09:05:26Z LICM finished after 0.044 seconds\n",
      "2023-09-05T09:05:26Z Running Delinearization\n",
      "2023-09-05T09:05:26Z Delinearization finished after 0.020 seconds\n",
      "2023-09-05T09:05:26Z Running LoopFusion\n",
      "2023-09-05T09:05:26Z LoopFusion finished after 0.128 seconds\n",
      "2023-09-05T09:05:26Z Running SimplifySlice\n",
      "2023-09-05T09:05:26Z SimplifySlice finished after 0.007 seconds\n",
      "2023-09-05T09:05:26Z Running LICM\n",
      "2023-09-05T09:05:26Z LICM finished after 0.020 seconds\n",
      "2023-09-05T09:05:26Z Running Simplifier\n",
      "2023-09-05T09:05:26Z Simplifier finished after 0.088 seconds\n",
      "2023-09-05T09:05:26Z Running ValueNumbering\n",
      "2023-09-05T09:05:26Z ValueNumbering finished after 0.033 seconds\n",
      "2023-09-05T09:05:26Z Running LICM\n",
      "2023-09-05T09:05:26Z LICM finished after 0.019 seconds\n",
      "2023-09-05T09:05:26Z Running PadElimination\n",
      "2023-09-05T09:05:26Z PadElimination finished after 0.001 seconds\n",
      "2023-09-05T09:05:26Z Running Delinearization\n",
      "2023-09-05T09:05:26Z Delinearization finished after 0.018 seconds\n",
      "2023-09-05T09:05:26Z Running LoopFusion\n",
      "2023-09-05T09:05:26Z LoopFusion finished after 0.127 seconds\n",
      "2023-09-05T09:05:26Z Running GenericAccessSimplifier\n",
      "2023-09-05T09:05:26Z GenericAccessSimplifier finished after 0.008 seconds\n",
      "2023-09-05T09:05:26Z Running Simplifier\n",
      "2023-09-05T09:05:26Z Simplifier finished after 0.088 seconds\n",
      "2023-09-05T09:05:26Z Running LICM\n",
      "2023-09-05T09:05:26Z LICM finished after 0.020 seconds\n",
      "2023-09-05T09:05:26Z Running ValueNumbering\n",
      "2023-09-05T09:05:26Z ValueNumbering finished after 0.030 seconds\n",
      "2023-09-05T09:05:26Z Running TCTransform\n",
      "2023-09-05T09:05:26Z TCTransform finished after 0.007 seconds\n",
      "2023-09-05T09:05:26Z Running CommuteConcat\n",
      "2023-09-05T09:05:26Z CommuteConcat finished after 0.008 seconds\n",
      "2023-09-05T09:05:26Z Running RecognizeOpIdiom\n",
      "2023-09-05T09:05:26Z RecognizeOpIdiom finished after 0.037 seconds\n",
      "2023-09-05T09:05:26Z Running MaskPropagation\n",
      "2023-09-05T09:05:26Z MaskPropagation finished after 0.031 seconds\n",
      "2023-09-05T09:05:26Z Running Recompute\n",
      "2023-09-05T09:05:26Z Recompute finished after 0.002 seconds\n",
      "2023-09-05T09:05:26Z Running DeadCodeElimination\n",
      "2023-09-05T09:05:27Z DeadCodeElimination finished after 0.009 seconds\n",
      "2023-09-05T09:05:27Z Running DoNothing\n",
      "2023-09-05T09:05:27Z DoNothing finished after 0.000 seconds\n",
      "2023-09-05T09:05:27Z Running MutateDataType\n",
      "2023-09-05T09:05:27Z MutateDataType finished after 0.006 seconds\n",
      "2023-09-05T09:05:27Z Running AutoCastTCInputs\n",
      "2023-09-05T09:05:27Z AutoCastTCInputs finished after 0.022 seconds\n",
      "2023-09-05T09:05:27Z Running GenericAccessSimplifier\n",
      "2023-09-05T09:05:27Z GenericAccessSimplifier finished after 0.010 seconds\n",
      "2023-09-05T09:05:27Z Running Simplifier\n",
      "2023-09-05T09:05:27Z Simplifier finished after 0.097 seconds\n",
      "2023-09-05T09:05:27Z Running LegalizeCCOpLayout\n",
      "2023-09-05T09:05:27Z LegalizeCCOpLayout finished after 0.009 seconds\n",
      "2023-09-05T09:05:27Z Running DelinearIndices\n",
      "2023-09-05T09:05:27Z DelinearIndices finished after 0.019 seconds\n",
      "2023-09-05T09:05:27Z Running Delinearization\n",
      "2023-09-05T09:05:27Z Delinearization finished after 0.018 seconds\n",
      "2023-09-05T09:05:27Z Running DelinearIndices\n",
      "2023-09-05T09:05:27Z DelinearIndices finished after 0.018 seconds\n",
      "2023-09-05T09:05:27Z Running DeadCodeElimination\n",
      "2023-09-05T09:05:27Z DeadCodeElimination finished after 0.010 seconds\n",
      "2023-09-05T09:05:27Z Running InferIntrinsicOnCC\n",
      "2023-09-05T09:05:27Z InferIntrinsicOnCC finished after 0.109 seconds\n",
      "2023-09-05T09:05:27Z Running ResolveAccessConflict\n",
      "2023-09-05T09:05:27Z ResolveAccessConflict finished after 0.081 seconds\n",
      "2023-09-05T09:05:27Z Running LICM\n",
      "2023-09-05T09:05:27Z LICM finished after 0.023 seconds\n",
      "2023-09-05T09:05:27Z Running LocalLayoutOpt\n",
      "2023-09-05T09:05:27Z LocalLayoutOpt finished after 0.066 seconds\n",
      "2023-09-05T09:05:27Z Running DelinearIndices\n",
      "2023-09-05T09:05:27Z DelinearIndices finished after 0.021 seconds\n",
      "2023-09-05T09:05:27Z Running OrigLayoutTilingPipeline\n",
      "2023-09-05T09:05:27Z Running GlobalLayoutOpt\n",
      "2023-09-05T09:05:28Z GlobalLayoutOpt finished after 0.749 seconds\n",
      "2023-09-05T09:05:28Z Running CanonicalizeDAG\n",
      "2023-09-05T09:05:28Z CanonicalizeDAG finished after 0.022 seconds\n",
      "2023-09-05T09:05:28Z Running FlattenAxesForTiling\n",
      "2023-09-05T09:05:28Z FlattenAxesForTiling finished after 0.021 seconds\n",
      "2023-09-05T09:05:28Z Running SundaSizeTiling\n",
      "2023-09-05T09:05:29Z SundaSizeTiling finished after 1.205 seconds\n",
      "2023-09-05T09:05:29Z OrigLayoutTilingPipeline finished after 2.017 seconds\n",
      "2023-09-05T09:05:29Z Running TilingProfiler\n",
      "2023-09-05T09:05:29Z TilingProfiler finished after 0.092 seconds\n",
      "2023-09-05T09:05:29Z Running FlattenMacroLoop\n",
      "2023-09-05T09:05:29Z FlattenMacroLoop finished after 0.038 seconds\n",
      "2023-09-05T09:05:29Z Running InferTongaTensor\n",
      "2023-09-05T09:05:30Z InferTongaTensor finished after 0.388 seconds\n",
      "2023-09-05T09:05:30Z Running TongaSimplifier\n",
      "2023-09-05T09:05:30Z TongaSimplifier finished after 0.047 seconds\n",
      "2023-09-05T09:05:30Z Running LICM\n",
      "2023-09-05T09:05:30Z LICM finished after 0.062 seconds\n",
      "2023-09-05T09:05:30Z Running RewriteReplicationMatmul\n",
      "2023-09-05T09:05:30Z RewriteReplicationMatmul finished after 0.020 seconds\n",
      "2023-09-05T09:05:30Z Running FlattenMacroLoop\n",
      "2023-09-05T09:05:30Z FlattenMacroLoop finished after 0.041 seconds\n",
      "2023-09-05T09:05:30Z Running SimplifyMacroPredicates\n",
      "2023-09-05T09:05:30Z SimplifyMacroPredicates finished after 0.168 seconds\n",
      "2023-09-05T09:05:30Z Running DataLocalityOpt\n",
      "2023-09-05T09:05:33Z DataLocalityOpt finished after 2.603 seconds\n",
      "2023-09-05T09:05:33Z Running TongaSimplifier\n",
      "2023-09-05T09:05:33Z TongaSimplifier finished after 0.058 seconds\n",
      "2023-09-05T09:05:33Z Running LegalizeSundaMacro\n",
      "2023-09-05T09:05:33Z LegalizeSundaMacro finished after 0.056 seconds\n",
      "2023-09-05T09:05:33Z Running TongaSimplifier\n",
      "2023-09-05T09:05:33Z TongaSimplifier finished after 0.056 seconds\n",
      "2023-09-05T09:05:33Z Running PerfectLoopNest\n",
      "2023-09-05T09:05:33Z PerfectLoopNest finished after 0.021 seconds\n",
      "2023-09-05T09:05:33Z Running FlattenMacroLoop\n",
      "2023-09-05T09:05:33Z FlattenMacroLoop finished after 0.077 seconds\n",
      "2023-09-05T09:05:33Z Running RewriteWeights\n",
      "2023-09-05T09:05:34Z RewriteWeights finished after 1.379 seconds\n",
      "2023-09-05T09:05:34Z Running ReshapeWeights\n",
      "2023-09-05T09:05:34Z ReshapeWeights finished after 0.012 seconds\n",
      "2023-09-05T09:05:34Z Running FlattenMacroLoop\n",
      "2023-09-05T09:05:34Z FlattenMacroLoop finished after 0.042 seconds\n",
      "2023-09-05T09:05:34Z Running SimplifyMacroPredicates\n",
      "2023-09-05T09:05:35Z SimplifyMacroPredicates finished after 0.231 seconds\n",
      "2023-09-05T09:05:35Z Running InferInitValue\n",
      "2023-09-05T09:05:36Z InferInitValue finished after 1.351 seconds\n",
      "2023-09-05T09:05:36Z Running TongaSimplifier\n",
      "2023-09-05T09:05:36Z TongaSimplifier finished after 0.055 seconds\n",
      "2023-09-05T09:05:36Z Running SimplifyTensor\n",
      "2023-09-05T09:05:36Z SimplifyTensor finished after 0.097 seconds\n",
      "2023-09-05T09:05:36Z Running LICM\n",
      "2023-09-05T09:05:36Z LICM finished after 0.048 seconds\n",
      "2023-09-05T09:05:36Z Running SundaISel\n",
      "2023-09-05T09:05:37Z SundaISel finished after 0.427 seconds\n",
      "2023-09-05T09:05:37Z Running TongaLoopInterchange\n",
      "2023-09-05T09:05:37Z TongaLoopInterchange finished after 0.011 seconds\n",
      "2023-09-05T09:05:37Z Running TongaSimplifyPredicates\n",
      "2023-09-05T09:05:37Z TongaSimplifyPredicates finished after 0.012 seconds\n",
      "2023-09-05T09:05:37Z Running TongaLoopFusion\n",
      "2023-09-05T09:05:37Z TongaLoopFusion finished after 0.222 seconds\n",
      "2023-09-05T09:05:37Z Running TongaLoopInterchange\n",
      "2023-09-05T09:05:37Z TongaLoopInterchange finished after 0.010 seconds\n",
      "2023-09-05T09:05:37Z Running TongaLICM\n",
      "2023-09-05T09:05:37Z TongaLICM finished after 0.026 seconds\n",
      "2023-09-05T09:05:37Z Running FactorizeBlkDims\n",
      "2023-09-05T09:05:37Z FactorizeBlkDims finished after 0.160 seconds\n",
      "2023-09-05T09:05:37Z Running TongaInstComb\n",
      "2023-09-05T09:05:37Z TongaInstComb finished after 0.211 seconds\n",
      "2023-09-05T09:05:37Z Running TongaValueNumbering\n",
      "2023-09-05T09:05:37Z TongaValueNumbering finished after 0.053 seconds\n",
      "2023-09-05T09:05:37Z Running TongaInstComb\n",
      "2023-09-05T09:05:37Z TongaInstComb finished after 0.040 seconds\n",
      "2023-09-05T09:05:37Z Running VectorizeDMA\n",
      "2023-09-05T09:05:37Z VectorizeDMA finished after 0.029 seconds\n",
      "2023-09-05T09:05:37Z Running TongaSimplifyPredicates\n",
      "2023-09-05T09:05:37Z TongaSimplifyPredicates finished after 0.011 seconds\n",
      "2023-09-05T09:05:37Z Running LegalizePartitionReduce\n",
      "2023-09-05T09:05:37Z LegalizePartitionReduce finished after 0.010 seconds\n",
      "2023-09-05T09:05:37Z Running DeConcat\n",
      "2023-09-05T09:05:37Z DeConcat finished after 0.011 seconds\n",
      "2023-09-05T09:05:37Z Running PartialSimdFusion\n",
      "2023-09-05T09:05:38Z PartialSimdFusion finished after 0.102 seconds\n",
      "2023-09-05T09:05:38Z Running TritiumFusion\n",
      "2023-09-05T09:05:38Z TritiumFusion finished after 0.228 seconds\n",
      "2023-09-05T09:05:38Z Running VectorizeMatMult\n",
      "2023-09-05T09:05:38Z VectorizeMatMult finished after 0.007 seconds\n",
      "2023-09-05T09:05:38Z Running PartialLoopFusion\n",
      "2023-09-05T09:05:38Z PartialLoopFusion finished after 0.217 seconds\n",
      "2023-09-05T09:05:38Z Running TongaLICM\n",
      "2023-09-05T09:05:38Z TongaLICM finished after 0.026 seconds\n",
      "2023-09-05T09:05:38Z Running LowerTranspose\n",
      "2023-09-05T09:05:38Z LowerTranspose finished after 0.092 seconds\n",
      "2023-09-05T09:05:38Z Running LateTongaInstComb\n",
      "2023-09-05T09:05:38Z LateTongaInstComb finished after 0.222 seconds\n",
      "2023-09-05T09:05:38Z Running LowerTongaBatchId\n",
      "2023-09-05T09:05:38Z LowerTongaBatchId finished after 0.006 seconds\n",
      "2023-09-05T09:05:38Z Running SplitAccGrp\n",
      "2023-09-05T09:05:38Z SplitAccGrp finished after 0.010 seconds\n",
      "2023-09-05T09:05:38Z Running SpillPSum\n",
      "2023-09-05T09:05:39Z SpillPSum finished after 0.081 seconds\n",
      "2023-09-05T09:05:39Z Running LowerIntrinsics\n",
      "2023-09-05T09:05:39Z LowerIntrinsics finished after 0.010 seconds\n",
      "2023-09-05T09:05:39Z Running LegalizeType\n",
      "2023-09-05T09:05:39Z LegalizeType finished after 0.143 seconds\n",
      "2023-09-05T09:05:39Z Running TongaLICM\n",
      "2023-09-05T09:05:39Z TongaLICM finished after 0.029 seconds\n",
      "2023-09-05T09:05:39Z Running InferPSumTensor\n",
      "2023-09-05T09:05:39Z InferPSumTensor finished after 0.156 seconds\n",
      "2023-09-05T09:05:39Z Running WeightCoalescing\n",
      "2023-09-05T09:05:40Z WeightCoalescing finished after 0.040 seconds\n",
      "2023-09-05T09:05:40Z Running LegalizeSundaAccess\n",
      "2023-09-05T09:05:40Z LegalizeSundaAccess finished after 0.051 seconds\n",
      "2023-09-05T09:05:40Z Running RelaxPredicates\n",
      "2023-09-05T09:05:41Z RelaxPredicates finished after 0.017 seconds\n",
      "2023-09-05T09:05:41Z Running TensorInitialization\n",
      "2023-09-05T09:05:42Z TensorInitialization finished after 0.011 seconds\n",
      "2023-09-05T09:05:42Z Running TongaSimplifyPredicates\n",
      "2023-09-05T09:05:43Z TongaSimplifyPredicates finished after 0.010 seconds\n",
      "2023-09-05T09:05:43Z Running ExpandISAMacro\n",
      "2023-09-05T09:05:44Z ExpandISAMacro finished after 0.110 seconds\n",
      "2023-09-05T09:05:44Z Running SimplifyTongaTensor\n",
      "2023-09-05T09:05:44Z SimplifyTongaTensor finished after 0.043 seconds\n",
      "2023-09-05T09:05:44Z Running DMALocalityOpt\n",
      "2023-09-05T09:05:44Z DMALocalityOpt finished after 0.007 seconds\n",
      "2023-09-05T09:05:44Z Running DataStreaming\n",
      "2023-09-05T09:05:44Z DataStreaming finished after 0.027 seconds\n",
      "2023-09-05T09:05:44Z Running SFKVectorizer\n",
      "2023-09-05T09:05:46Z SFKVectorizer finished after 1.649 seconds\n",
      "2023-09-05T09:05:46Z Running LateLegalizeInst\n",
      "2023-09-05T09:05:46Z LateLegalizeInst finished after 0.013 seconds\n",
      "2023-09-05T09:05:46Z Running CoalesceCCOp\n",
      "2023-09-05T09:05:46Z CoalesceCCOp finished after 0.013 seconds\n",
      "2023-09-05T09:05:46Z Running SimpleAllReduceTiling\n",
      "2023-09-05T09:05:46Z SimpleAllReduceTiling finished after 0.012 seconds\n",
      "2023-09-05T09:05:46Z Running StaticProfiler\n",
      "2023-09-05T09:05:46Z StaticProfiler finished after 0.040 seconds\n",
      "2023-09-05T09:05:46Z Running SplitAPUnionSets\n",
      "2023-09-05T09:05:46Z SplitAPUnionSets finished after 0.272 seconds\n",
      "2023-09-05T09:05:46Z Running SundaLowerGenericAccess\n",
      "2023-09-05T09:05:46Z SundaLowerGenericAccess finished after 0.009 seconds\n",
      "2023-09-05T09:05:46Z Running DumpGraphAndMetadata\n",
      "2023-09-05T09:05:46Z DumpGraphAndMetadata finished after 0.027 seconds\n",
      "2023-09-05T09:05:46Z Running BirCodeGenLoop\n",
      "2023-09-05T09:05:46Z BirCodeGenLoop finished after 0.253 seconds\n",
      "2023-09-05T09:05:47Z Running birverifier\n",
      "2023-09-05T09:05:47Z birverifier finished after 0.025 seconds\n",
      "2023-09-05T09:05:47Z Running expand_replication\n",
      "2023-09-05T09:05:47Z expand_replication finished after 0.000 seconds\n",
      "2023-09-05T09:05:47Z Running unroll\n",
      "2023-09-05T09:05:47Z unroll finished after 0.064 seconds\n",
      "2023-09-05T09:05:47Z Running error_injector\n",
      "2023-09-05T09:05:47Z error_injector finished after 0.000 seconds\n",
      "2023-09-05T09:05:47Z Running constant_propagate\n",
      "2023-09-05T09:05:47Z constant_propagate finished after 0.009 seconds\n",
      "2023-09-05T09:05:47Z Running vn_splitter\n",
      "2023-09-05T09:05:47Z vn_splitter finished after 0.006 seconds\n",
      "2023-09-05T09:05:47Z Running lower_ac\n",
      "2023-09-05T09:05:47Z lower_ac finished after 0.001 seconds\n",
      "2023-09-05T09:05:47Z Running mm_packing\n",
      "2023-09-05T09:05:47Z mm_packing finished after 0.005 seconds\n",
      "2023-09-05T09:05:47Z Running pre_sched\n",
      "2023-09-05T09:05:47Z pre_sched finished after 0.046 seconds\n",
      "2023-09-05T09:05:47Z Running coloring_allocator_psum\n",
      "2023-09-05T09:05:47Z coloring_allocator_psum finished after 0.025 seconds\n",
      "2023-09-05T09:05:47Z Running dma_optimization_psum\n",
      "2023-09-05T09:05:47Z dma_optimization_psum finished after 0.004 seconds\n",
      "2023-09-05T09:05:47Z Running address_rotation_psum\n",
      "2023-09-05T09:05:47Z address_rotation_psum finished after 0.021 seconds\n",
      "2023-09-05T09:05:47Z Running coloring_allocator_sb\n",
      "2023-09-05T09:05:47Z coloring_allocator_sb finished after 0.022 seconds\n",
      "2023-09-05T09:05:47Z Running dma_optimization_sb\n",
      "2023-09-05T09:05:48Z dma_optimization_sb finished after 0.026 seconds\n",
      "2023-09-05T09:05:48Z Running address_rotation_sb\n",
      "2023-09-05T09:05:48Z address_rotation_sb finished after 0.025 seconds\n",
      "2023-09-05T09:05:48Z Running coloring_allocator_dram\n",
      "2023-09-05T09:05:48Z coloring_allocator_dram finished after 0.010 seconds\n",
      "2023-09-05T09:05:48Z Running address_rotation_dram\n",
      "2023-09-05T09:05:48Z address_rotation_dram finished after 0.004 seconds\n",
      "2023-09-05T09:05:48Z Running tensorcopy_accel\n",
      "2023-09-05T09:05:48Z tensorcopy_accel finished after 0.000 seconds\n",
      "2023-09-05T09:05:48Z Running peephole_opts\n",
      "2023-09-05T09:05:48Z peephole_opts finished after 0.004 seconds\n",
      "2023-09-05T09:05:48Z Running lower_kernel\n",
      "2023-09-05T09:05:48Z lower_kernel finished after 0.000 seconds\n",
      "2023-09-05T09:05:48Z Running build_fdeps\n",
      "2023-09-05T09:05:48Z build_fdeps finished after 0.012 seconds\n",
      "2023-09-05T09:05:48Z Running remove_redundancies\n",
      "2023-09-05T09:05:48Z remove_redundancies finished after 0.002 seconds\n",
      "2023-09-05T09:05:48Z Running anti_dependency_analyzer\n",
      "2023-09-05T09:05:48Z anti_dependency_analyzer finished after 0.107 seconds\n",
      "2023-09-05T09:05:48Z Running post_sched\n",
      "2023-09-05T09:05:48Z post_sched finished after 0.095 seconds\n",
      "2023-09-05T09:05:48Z Running address_rotation_sb\n",
      "2023-09-05T09:05:48Z address_rotation_sb finished after 0.069 seconds\n",
      "2023-09-05T09:05:48Z Running anti_dependency_analyzer\n",
      "2023-09-05T09:05:48Z anti_dependency_analyzer finished after 0.058 seconds\n",
      "2023-09-05T09:05:48Z Running dep_opt\n",
      "2023-09-05T09:05:48Z dep_opt finished after 0.036 seconds\n",
      "2023-09-05T09:05:48Z Running report_stats\n",
      "2023-09-05T09:05:48Z report_stats finished after 0.001 seconds\n",
      "2023-09-05T09:05:48Z Running assign_trigger_engine\n",
      "2023-09-05T09:05:48Z assign_trigger_engine finished after 0.001 seconds\n",
      "2023-09-05T09:05:48Z Running alloc_queues\n",
      "2023-09-05T09:05:48Z alloc_queues finished after 0.001 seconds\n",
      "2023-09-05T09:05:48Z Running dep_reduction\n",
      "2023-09-05T09:05:48Z dep_reduction finished after 0.054 seconds\n",
      "2023-09-05T09:05:48Z Running bir_racecheck\n",
      "2023-09-05T09:05:48Z bir_racecheck finished after 0.064 seconds\n",
      "2023-09-05T09:05:48Z Running lower_dma\n",
      "2023-09-05T09:05:48Z lower_dma finished after 0.006 seconds\n",
      "2023-09-05T09:05:48Z Running alloc_semaphores\n",
      "2023-09-05T09:05:48Z alloc_semaphores finished after 0.005 seconds\n",
      "2023-09-05T09:05:48Z Running expand_inst_late\n",
      "2023-09-05T09:05:48Z expand_inst_late finished after 0.000 seconds\n",
      "2023-09-05T09:05:48Z Running lower_sync\n",
      "2023-09-05T09:05:48Z lower_sync finished after 0.003 seconds\n",
      "2023-09-05T09:05:48Z Running lower_act\n",
      "2023-09-05T09:05:48Z lower_act finished after 0.001 seconds\n",
      "2023-09-05T09:05:48Z Running lower_dve\n",
      "2023-09-05T09:05:48Z lower_dve finished after 0.006 seconds\n",
      "2023-09-05T09:05:48Z Running lower_ap\n",
      "2023-09-05T09:05:48Z lower_ap finished after 0.001 seconds\n",
      "2023-09-05T09:05:48Z Running alloc_regs\n",
      "2023-09-05T09:05:48Z alloc_regs finished after 0.000 seconds\n",
      "2023-09-05T09:05:48Z Running birverifier\n",
      "2023-09-05T09:05:48Z birverifier finished after 0.018 seconds\n",
      "2023-09-05T09:05:48Z Running codegen\n",
      "2023-09-05T09:05:48Z codegen finished after 0.069 seconds\n",
      "2023-09-05T09:05:48Z Running neff_packager\n",
      "2023-09-05T09:05:48Z neff_packager finished after 0.001 seconds\n",
      "2023-09-05T09:05:52Z Wrote /tmp/tmpcw96l2h8/graph.neff\n",
      "2023-09-05T09:05:52Z Compiler status PASS\n"
     ]
    }
   ],
   "source": [
    "model_neuron = torch_neuronx.trace(model, paraphrase)\n",
    "\n",
    "# Save the TorchScript for inference deployment\n",
    "torch.jit.save(model_neuron, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f805991-1929-4899-aa6d-46682b7e3acb",
   "metadata": {},
   "source": [
    "### Run inference and compare results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b81e935f-0eab-4128-85db-047e5022e2c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neuron paraphrase logits: [[ 4.7064433 -4.1799984]]\n",
      "Neuron paraphrase logits: [[-4.7476482  4.4669704]]\n"
     ]
    }
   ],
   "source": [
    "model_neuron = torch.jit.load(filename)\n",
    "\n",
    "sequence = \"大変すばらしい商品でした。感激です。\"\n",
    "paraphrase = encode(tokenizer, sequence)\n",
    "neuron_paraphrase_logits = model_neuron(*paraphrase)[0]\n",
    "print('Neuron paraphrase logits:', neuron_paraphrase_logits.detach().numpy())\n",
    "\n",
    "sequence = \"期待していた商品とは異なりました。残念です。\"\n",
    "paraphrase = encode(tokenizer, sequence)\n",
    "neuron_paraphrase_logits = model_neuron(*paraphrase)[0]\n",
    "print('Neuron paraphrase logits:', neuron_paraphrase_logits.detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e3299b-f8ec-4083-8813-54b693270cb0",
   "metadata": {},
   "source": [
    "CPUで推論実行した結果と同様の結果が得られている事が確認できました。推論性能を評価する方法は以下のサンプルをご参照下さい。\n",
    "+ https://github.com/aws-neuron/aws-neuron-sdk/blob/master/src/examples/pytorch/torch-neuronx/bert-base-cased-finetuned-mrpc-inference-on-trn1-tutorial.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
