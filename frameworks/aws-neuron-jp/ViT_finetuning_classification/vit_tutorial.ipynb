{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4718e6d-7761-415a-87a1-8946df7087f2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ViT Model Fine-tuning & Deployment on Inferentia2/Trainium\n",
    "\n",
    "ViT モデルは、テキストベースのタスク用に設計された transformer アーキテクチャに基づくビジュアルモデルです。\n",
    "\n",
    "ImageNet-21K データセットで事前学習された　ViT モデルを、Beans データセットでファインチューニングします。\n",
    "このモデルでは、数エポック学習することで、Beans(葉)の健康状態を3つのカテゴリに分類して予測可能です。 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362c27b4-1c5f-4295-8b48-ad423283f1b1",
   "metadata": {},
   "source": [
    "## 事前準備\n",
    "本 notebookは Neuron 2.13.2 環境下で動作確認しています"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "115c03c9-5b62-4cd6-9a93-657a95e337ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: pip in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (23.2.1)\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: transformers==4.31.0 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (4.31.0)\n",
      "Requirement already satisfied: accelerate in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (0.22.0)\n",
      "Requirement already satisfied: evaluate in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (0.4.0)\n",
      "Requirement already satisfied: gradio in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (3.42.0)\n",
      "Requirement already satisfied: filelock in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from transformers==4.31.0) (3.12.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from transformers==4.31.0) (0.16.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from transformers==4.31.0) (1.21.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from transformers==4.31.0) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from transformers==4.31.0) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from transformers==4.31.0) (2023.8.8)\n",
      "Requirement already satisfied: requests in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from transformers==4.31.0) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from transformers==4.31.0) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from transformers==4.31.0) (0.3.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from transformers==4.31.0) (4.66.1)\n",
      "Requirement already satisfied: psutil in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from accelerate) (1.13.1)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from evaluate) (2.14.5)\n",
      "Requirement already satisfied: dill in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from evaluate) (0.3.7)\n",
      "Requirement already satisfied: pandas in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from evaluate) (2.0.3)\n",
      "Requirement already satisfied: xxhash in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from evaluate) (3.3.0)\n",
      "Requirement already satisfied: multiprocess in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from evaluate) (0.70.15)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from evaluate) (2023.6.0)\n",
      "Requirement already satisfied: responses<0.19 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from evaluate) (0.18.0)\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from gradio) (23.2.1)\n",
      "Requirement already satisfied: altair<6.0,>=4.2.0 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from gradio) (5.1.1)\n",
      "Requirement already satisfied: fastapi in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from gradio) (0.101.1)\n",
      "Requirement already satisfied: ffmpy in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from gradio) (0.3.1)\n",
      "Requirement already satisfied: gradio-client==0.5.0 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from gradio) (0.5.0)\n",
      "Requirement already satisfied: httpx in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from gradio) (0.24.1)\n",
      "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from gradio) (6.0.1)\n",
      "Requirement already satisfied: jinja2<4.0 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from gradio) (3.1.2)\n",
      "Requirement already satisfied: markupsafe~=2.0 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from gradio) (2.1.3)\n",
      "Requirement already satisfied: matplotlib~=3.0 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from gradio) (3.7.2)\n",
      "Requirement already satisfied: orjson~=3.0 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from gradio) (3.9.5)\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from gradio) (10.0.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from gradio) (2.2.0)\n",
      "Requirement already satisfied: pydub in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: python-multipart in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from gradio) (0.0.6)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from gradio) (4.7.1)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from gradio) (0.23.2)\n",
      "Requirement already satisfied: websockets<12.0,>=10.0 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from gradio) (11.0.3)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from altair<6.0,>=4.2.0->gradio) (4.19.0)\n",
      "Requirement already satisfied: toolz in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from altair<6.0,>=4.2.0->gradio) (0.12.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from datasets>=2.0.0->evaluate) (13.0.0)\n",
      "Requirement already satisfied: aiohttp in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from datasets>=2.0.0->evaluate) (3.8.5)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from importlib-resources<7.0,>=1.3->gradio) (3.16.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from matplotlib~=3.0->gradio) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from matplotlib~=3.0->gradio) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from matplotlib~=3.0->gradio) (4.42.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from matplotlib~=3.0->gradio) (1.4.4)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from matplotlib~=3.0->gradio) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from pandas->evaluate) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from pandas->evaluate) (2023.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4->gradio) (0.5.0)\n",
      "Requirement already satisfied: pydantic-core==2.6.0 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4->gradio) (2.6.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from requests->transformers==4.31.0) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from requests->transformers==4.31.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from requests->transformers==4.31.0) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from requests->transformers==4.31.0) (2023.7.22)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (11.7.99)\n",
      "Requirement already satisfied: setuptools in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10.0->accelerate) (68.1.0)\n",
      "Requirement already satisfied: wheel in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10.0->accelerate) (0.41.1)\n",
      "Requirement already satisfied: click>=7.0 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from uvicorn>=0.14.0->gradio) (8.1.7)\n",
      "Requirement already satisfied: h11>=0.8 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from uvicorn>=0.14.0->gradio) (0.14.0)\n",
      "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from fastapi->gradio) (0.27.0)\n",
      "Requirement already satisfied: httpcore<0.18.0,>=0.15.0 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from httpx->gradio) (0.17.3)\n",
      "Requirement already satisfied: sniffio in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from httpx->gradio) (1.3.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from httpcore<0.18.0,>=0.15.0->httpx->gradio) (3.7.1)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (2023.7.1)\n",
      "Requirement already satisfied: pkgutil-resolve-name>=1.3.10 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (1.3.10)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.9.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
      "Requirement already satisfied: exceptiongroup in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from anyio<5.0,>=3.0->httpcore<0.18.0,>=0.15.0->httpx->gradio) (1.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U pip\n",
    "!pip install -U transformers==4.31.0 accelerate evaluate gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3324f68f-1307-4ef4-99c8-b0d2414bbea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aws-neuronx-runtime-discovery 2.9\n",
      "libneuronxla                  0.5.413\n",
      "neuronx-cc                    2.8.0.25+a3ad0f342\n",
      "neuronx-hwm                   2.8.0.3+2b7c6da39\n",
      "torch                         1.13.1\n",
      "torch-neuronx                 1.13.1.1.9.0\n",
      "torch-xla                     1.13.1+torchneuron8\n",
      "torchvision                   0.14.1\n",
      "transformers                  4.31.0\n"
     ]
    }
   ],
   "source": [
    "!pip list | grep \"neuron\\|torch\\|transformers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf201a69-ce0a-493d-86dd-565d2c067a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi  aws-neuronx-collectives                2.15.16.0-db4e2d9a9               amd64        neuron_ccom built using CMake\n",
      "hi  aws-neuronx-dkms                       2.11.9.0                          amd64        aws-neuronx driver in DKMS format.\n",
      "hi  aws-neuronx-oci-hook                   2.2.16.0                          amd64        neuron_oci_hook built using CMake\n",
      "hi  aws-neuronx-runtime-lib                2.15.14.0-279f319f2               amd64        neuron_runtime built using CMake\n",
      "hi  aws-neuronx-tools                      2.12.2.0                          amd64        Neuron profile and debug tools\n"
     ]
    }
   ],
   "source": [
    "!dpkg --list | grep neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d12f43b-ee0c-4bcc-bda7-b62526202391",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo rmmod neuron; sudo modprobe neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccde899b-8e68-4cfe-8708-6a586e077082",
   "metadata": {},
   "source": [
    "## Trainer API を使用した トレーニング（ファインチューニング）実行\n",
    "Transformers には Trainer という便利なクラスがあり、Torch Neuron からも利用可能です。 ここでは Trainer API を利用してトレーニングを実行していきます。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1daf4b96-f36b-4d67-aeee-2bc5d3f160b1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m#!/usr/bin/env python\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m# coding=utf-8\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m# Copyright 2021 The HuggingFace Inc. team. All rights reserved.\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m#\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m# you may not use this file except in compliance with the License.\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m# You may obtain a copy of the License at\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m#\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m#     http://www.apache.org/licenses/LICENSE-2.0\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m#\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m# Unless required by applicable law or agreed to in writing, software\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m# distributed under the License is distributed on an \"AS IS\" BASIS,\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m# See the License for the specific language governing permissions and\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mlogging\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mdataclasses\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m dataclass, field\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtyping\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Optional\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mevaluate\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mdatasets\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m load_dataset\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mPIL\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Image\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtorchvision\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mtransforms\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m (\u001b[37m\u001b[39;49;00m\n",
      "    CenterCrop,\u001b[37m\u001b[39;49;00m\n",
      "    Compose,\u001b[37m\u001b[39;49;00m\n",
      "    Normalize,\u001b[37m\u001b[39;49;00m\n",
      "    RandomHorizontalFlip,\u001b[37m\u001b[39;49;00m\n",
      "    RandomResizedCrop,\u001b[37m\u001b[39;49;00m\n",
      "    Resize,\u001b[37m\u001b[39;49;00m\n",
      "    ToTensor,\u001b[37m\u001b[39;49;00m\n",
      ")\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m (\u001b[37m\u001b[39;49;00m\n",
      "    MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING,\u001b[37m\u001b[39;49;00m\n",
      "    AutoConfig,\u001b[37m\u001b[39;49;00m\n",
      "    AutoImageProcessor,\u001b[37m\u001b[39;49;00m\n",
      "    AutoModelForImageClassification,\u001b[37m\u001b[39;49;00m\n",
      "    HfArgumentParser,\u001b[37m\u001b[39;49;00m\n",
      "    Trainer,\u001b[37m\u001b[39;49;00m\n",
      "    TrainingArguments,\u001b[37m\u001b[39;49;00m\n",
      "    set_seed,\u001b[37m\u001b[39;49;00m\n",
      ")\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mtrainer_utils\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m get_last_checkpoint\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mutils\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m check_min_version, send_example_telemetry\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mutils\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mversions\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m require_version\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[33m\"\"\" Fine-tuning a 🤗 Transformers model for image classification\"\"\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "logger = logging.getLogger(\u001b[31m__name__\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m# Will error if the minimal version of Transformers is not installed. Remove at your own risks.\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "check_min_version(\u001b[33m\"\u001b[39;49;00m\u001b[33m4.27.0\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "require_version(\u001b[33m\"\u001b[39;49;00m\u001b[33mdatasets>=1.8.0\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mTo fix: pip install -r examples/pytorch/image-classification/requirements.txt\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "MODEL_CONFIG_CLASSES = \u001b[36mlist\u001b[39;49;00m(MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING.keys())\u001b[37m\u001b[39;49;00m\n",
      "MODEL_TYPES = \u001b[36mtuple\u001b[39;49;00m(conf.model_type \u001b[34mfor\u001b[39;49;00m conf \u001b[35min\u001b[39;49;00m MODEL_CONFIG_CLASSES)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mpil_loader\u001b[39;49;00m(path: \u001b[36mstr\u001b[39;49;00m):\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(path, \u001b[33m\"\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\u001b[37m\u001b[39;49;00m\n",
      "        im = Image.open(f)\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mreturn\u001b[39;49;00m im.convert(\u001b[33m\"\u001b[39;49;00m\u001b[33mRGB\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[90m@dataclass\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mDataTrainingArguments\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\n",
      "\u001b[33m    Arguments pertaining to what data we are going to input our model for training and eval.\u001b[39;49;00m\n",
      "\u001b[33m    Using `HfArgumentParser` we can turn this class into argparse arguments to be able to specify\u001b[39;49;00m\n",
      "\u001b[33m    them on the command line.\u001b[39;49;00m\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    dataset_name: Optional[\u001b[36mstr\u001b[39;49;00m] = field(\u001b[37m\u001b[39;49;00m\n",
      "        default=\u001b[34mNone\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        metadata={\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mhelp\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33mName of a dataset from the hub (could be your own, possibly private dataset hosted on the hub).\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        },\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "    dataset_config_name: Optional[\u001b[36mstr\u001b[39;49;00m] = field(\u001b[37m\u001b[39;49;00m\n",
      "        default=\u001b[34mNone\u001b[39;49;00m, metadata={\u001b[33m\"\u001b[39;49;00m\u001b[33mhelp\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33mThe configuration name of the dataset to use (via the datasets library).\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m}\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "    train_dir: Optional[\u001b[36mstr\u001b[39;49;00m] = field(default=\u001b[34mNone\u001b[39;49;00m, metadata={\u001b[33m\"\u001b[39;49;00m\u001b[33mhelp\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33mA folder containing the training data.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m})\u001b[37m\u001b[39;49;00m\n",
      "    validation_dir: Optional[\u001b[36mstr\u001b[39;49;00m] = field(default=\u001b[34mNone\u001b[39;49;00m, metadata={\u001b[33m\"\u001b[39;49;00m\u001b[33mhelp\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33mA folder containing the validation data.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m})\u001b[37m\u001b[39;49;00m\n",
      "    train_val_split: Optional[\u001b[36mfloat\u001b[39;49;00m] = field(\u001b[37m\u001b[39;49;00m\n",
      "        default=\u001b[34m0.15\u001b[39;49;00m, metadata={\u001b[33m\"\u001b[39;49;00m\u001b[33mhelp\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33mPercent to split off of train for validation.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m}\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "    max_train_samples: Optional[\u001b[36mint\u001b[39;49;00m] = field(\u001b[37m\u001b[39;49;00m\n",
      "        default=\u001b[34mNone\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        metadata={\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mhelp\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: (\u001b[37m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mFor debugging purposes or quicker training, truncate the number of training examples to this \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mvalue if set.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "            )\u001b[37m\u001b[39;49;00m\n",
      "        },\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "    max_eval_samples: Optional[\u001b[36mint\u001b[39;49;00m] = field(\u001b[37m\u001b[39;49;00m\n",
      "        default=\u001b[34mNone\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        metadata={\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mhelp\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: (\u001b[37m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mFor debugging purposes or quicker training, truncate the number of evaluation examples to this \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mvalue if set.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "            )\u001b[37m\u001b[39;49;00m\n",
      "        },\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__post_init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m):\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mif\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.dataset_name \u001b[35mis\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m \u001b[35mand\u001b[39;49;00m (\u001b[36mself\u001b[39;49;00m.train_dir \u001b[35mis\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m \u001b[35mand\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.validation_dir \u001b[35mis\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m):\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[34mraise\u001b[39;49;00m \u001b[36mValueError\u001b[39;49;00m(\u001b[37m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mYou must specify either a dataset name from the hub or a train and/or validation directory.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "            )\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[90m@dataclass\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mModelArguments\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\n",
      "\u001b[33m    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\u001b[39;49;00m\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    model_name_or_path: \u001b[36mstr\u001b[39;49;00m = field(\u001b[37m\u001b[39;49;00m\n",
      "        default=\u001b[33m\"\u001b[39;49;00m\u001b[33mgoogle/vit-base-patch16-224-in21k\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        metadata={\u001b[33m\"\u001b[39;49;00m\u001b[33mhelp\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33mPath to pretrained model or model identifier from huggingface.co/models\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m},\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "    model_type: Optional[\u001b[36mstr\u001b[39;49;00m] = field(\u001b[37m\u001b[39;49;00m\n",
      "        default=\u001b[34mNone\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        metadata={\u001b[33m\"\u001b[39;49;00m\u001b[33mhelp\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33mIf training from scratch, pass a model type from the list: \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m + \u001b[33m\"\u001b[39;49;00m\u001b[33m, \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.join(MODEL_TYPES)},\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "    config_name: Optional[\u001b[36mstr\u001b[39;49;00m] = field(\u001b[37m\u001b[39;49;00m\n",
      "        default=\u001b[34mNone\u001b[39;49;00m, metadata={\u001b[33m\"\u001b[39;49;00m\u001b[33mhelp\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33mPretrained config name or path if not the same as model_name\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m}\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "    cache_dir: Optional[\u001b[36mstr\u001b[39;49;00m] = field(\u001b[37m\u001b[39;49;00m\n",
      "        default=\u001b[34mNone\u001b[39;49;00m, metadata={\u001b[33m\"\u001b[39;49;00m\u001b[33mhelp\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33mWhere do you want to store the pretrained models downloaded from s3\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m}\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "    model_revision: \u001b[36mstr\u001b[39;49;00m = field(\u001b[37m\u001b[39;49;00m\n",
      "        default=\u001b[33m\"\u001b[39;49;00m\u001b[33mmain\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        metadata={\u001b[33m\"\u001b[39;49;00m\u001b[33mhelp\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33mThe specific model version to use (can be a branch name, tag name or commit id).\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m},\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "    image_processor_name: \u001b[36mstr\u001b[39;49;00m = field(default=\u001b[34mNone\u001b[39;49;00m, metadata={\u001b[33m\"\u001b[39;49;00m\u001b[33mhelp\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33mName or path of preprocessor config.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m})\u001b[37m\u001b[39;49;00m\n",
      "    use_auth_token: \u001b[36mbool\u001b[39;49;00m = field(\u001b[37m\u001b[39;49;00m\n",
      "        default=\u001b[34mFalse\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        metadata={\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mhelp\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: (\u001b[37m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mWill use the token generated when running `huggingface-cli login` (necessary to use this script \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mwith private models).\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "            )\u001b[37m\u001b[39;49;00m\n",
      "        },\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "    ignore_mismatched_sizes: \u001b[36mbool\u001b[39;49;00m = field(\u001b[37m\u001b[39;49;00m\n",
      "        default=\u001b[34mFalse\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        metadata={\u001b[33m\"\u001b[39;49;00m\u001b[33mhelp\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33mWill enable to load a pretrained model whose head dimensions are different.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m},\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mcollate_fn\u001b[39;49;00m(examples):\u001b[37m\u001b[39;49;00m\n",
      "    pixel_values = torch.stack([example[\u001b[33m\"\u001b[39;49;00m\u001b[33mpixel_values\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] \u001b[34mfor\u001b[39;49;00m example \u001b[35min\u001b[39;49;00m examples])\u001b[37m\u001b[39;49;00m\n",
      "    labels = torch.tensor([example[\u001b[33m\"\u001b[39;49;00m\u001b[33mlabels\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] \u001b[34mfor\u001b[39;49;00m example \u001b[35min\u001b[39;49;00m examples])\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mreturn\u001b[39;49;00m {\u001b[33m\"\u001b[39;49;00m\u001b[33mpixel_values\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: pixel_values, \u001b[33m\"\u001b[39;49;00m\u001b[33mlabels\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: labels}\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmain\u001b[39;49;00m():\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# See all possible arguments in src/transformers/training_args.py\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# or by passing the --help flag to this script.\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# We now keep distinct sets of args, for a cleaner separation of concerns.\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mif\u001b[39;49;00m \u001b[36mlen\u001b[39;49;00m(sys.argv) == \u001b[34m2\u001b[39;49;00m \u001b[35mand\u001b[39;49;00m sys.argv[\u001b[34m1\u001b[39;49;00m].endswith(\u001b[33m\"\u001b[39;49;00m\u001b[33m.json\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m):\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[37m# If we pass only one argument to the script and it's the path to a json file,\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[37m# let's parse it to get our arguments.\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[\u001b[34m1\u001b[39;49;00m]))\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34melse\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "        model_args, data_args, training_args = parser.parse_args_into_dataclasses()\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# Sending telemetry. Tracking the example usage helps us better allocate resources to maintain them. The\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# information sent is the one passed as arguments along with your Python/PyTorch versions.\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    send_example_telemetry(\u001b[33m\"\u001b[39;49;00m\u001b[33mrun_image_classification\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, model_args, data_args)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# Setup logging\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    logging.basicConfig(\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[36mformat\u001b[39;49;00m=\u001b[33m\"\u001b[39;49;00m\u001b[33m%(asctime)s\u001b[39;49;00m\u001b[33m - \u001b[39;49;00m\u001b[33m%(levelname)s\u001b[39;49;00m\u001b[33m - \u001b[39;49;00m\u001b[33m%(name)s\u001b[39;49;00m\u001b[33m - \u001b[39;49;00m\u001b[33m%(message)s\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        datefmt=\u001b[33m\"\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33mm/\u001b[39;49;00m\u001b[33m%d\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33mY \u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33mH:\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33mM:\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33mS\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        handlers=[logging.StreamHandler(sys.stdout)],\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mif\u001b[39;49;00m training_args.should_log:\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[37m# The default of training_args.log_level is passive, so we set log level at info here to have that default.\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        transformers.utils.logging.set_verbosity_info()\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    log_level = training_args.get_process_log_level()\u001b[37m\u001b[39;49;00m\n",
      "    logger.setLevel(log_level)\u001b[37m\u001b[39;49;00m\n",
      "    transformers.utils.logging.set_verbosity(log_level)\u001b[37m\u001b[39;49;00m\n",
      "    transformers.utils.logging.enable_default_handler()\u001b[37m\u001b[39;49;00m\n",
      "    transformers.utils.logging.enable_explicit_format()\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# Log on each process the small summary:\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    logger.warning(\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mProcess rank: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mtraining_args.local_rank\u001b[33m}\u001b[39;49;00m\u001b[33m, device: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mtraining_args.device\u001b[33m}\u001b[39;49;00m\u001b[33m, n_gpu: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mtraining_args.n_gpu\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        + \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mdistributed training: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00m\u001b[36mbool\u001b[39;49;00m(training_args.local_rank\u001b[37m \u001b[39;49;00m!=\u001b[37m \u001b[39;49;00m-\u001b[34m1\u001b[39;49;00m)\u001b[33m}\u001b[39;49;00m\u001b[33m, 16-bits training: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mtraining_args.fp16\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mTraining/evaluation parameters \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mtraining_args\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# Detecting last checkpoint.\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    last_checkpoint = \u001b[34mNone\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mif\u001b[39;49;00m os.path.isdir(training_args.output_dir) \u001b[35mand\u001b[39;49;00m training_args.do_train \u001b[35mand\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m training_args.overwrite_output_dir:\u001b[37m\u001b[39;49;00m\n",
      "        last_checkpoint = get_last_checkpoint(training_args.output_dir)\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mif\u001b[39;49;00m last_checkpoint \u001b[35mis\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m \u001b[35mand\u001b[39;49;00m \u001b[36mlen\u001b[39;49;00m(os.listdir(training_args.output_dir)) > \u001b[34m0\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[34mraise\u001b[39;49;00m \u001b[36mValueError\u001b[39;49;00m(\u001b[37m\u001b[39;49;00m\n",
      "                \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mOutput directory (\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mtraining_args.output_dir\u001b[33m}\u001b[39;49;00m\u001b[33m) already exists and is not empty. \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mUse --overwrite_output_dir to overcome.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "            )\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34melif\u001b[39;49;00m last_checkpoint \u001b[35mis\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m \u001b[35mand\u001b[39;49;00m training_args.resume_from_checkpoint \u001b[35mis\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "            logger.info(\u001b[37m\u001b[39;49;00m\n",
      "                \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mCheckpoint detected, resuming training at \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mlast_checkpoint\u001b[33m}\u001b[39;49;00m\u001b[33m. To avoid this behavior, change \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mthe `--output_dir` or add `--overwrite_output_dir` to train from scratch.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "            )\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# Set seed before initializing model.\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    set_seed(training_args.seed)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# Initialize our dataset and prepare it for the 'image-classification' task.\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mif\u001b[39;49;00m data_args.dataset_name \u001b[35mis\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "        dataset = load_dataset(\u001b[37m\u001b[39;49;00m\n",
      "            data_args.dataset_name,\u001b[37m\u001b[39;49;00m\n",
      "            data_args.dataset_config_name,\u001b[37m\u001b[39;49;00m\n",
      "            cache_dir=model_args.cache_dir,\u001b[37m\u001b[39;49;00m\n",
      "            task=\u001b[33m\"\u001b[39;49;00m\u001b[33mimage-classification\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "            use_auth_token=\u001b[34mTrue\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m model_args.use_auth_token \u001b[34melse\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        )\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34melse\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "        data_files = {}\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mif\u001b[39;49;00m data_args.train_dir \u001b[35mis\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "            data_files[\u001b[33m\"\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = os.path.join(data_args.train_dir, \u001b[33m\"\u001b[39;49;00m\u001b[33m**\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mif\u001b[39;49;00m data_args.validation_dir \u001b[35mis\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "            data_files[\u001b[33m\"\u001b[39;49;00m\u001b[33mvalidation\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = os.path.join(data_args.validation_dir, \u001b[33m\"\u001b[39;49;00m\u001b[33m**\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "        dataset = load_dataset(\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mimagefolder\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "            data_files=data_files,\u001b[37m\u001b[39;49;00m\n",
      "            cache_dir=model_args.cache_dir,\u001b[37m\u001b[39;49;00m\n",
      "            task=\u001b[33m\"\u001b[39;49;00m\u001b[33mimage-classification\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        )\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# If we don't have a validation split, split off a percentage of train as validation.\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    data_args.train_val_split = \u001b[34mNone\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mvalidation\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[35min\u001b[39;49;00m dataset.keys() \u001b[34melse\u001b[39;49;00m data_args.train_val_split\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mif\u001b[39;49;00m \u001b[36misinstance\u001b[39;49;00m(data_args.train_val_split, \u001b[36mfloat\u001b[39;49;00m) \u001b[35mand\u001b[39;49;00m data_args.train_val_split > \u001b[34m0.0\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "        split = dataset[\u001b[33m\"\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m].train_test_split(data_args.train_val_split)\u001b[37m\u001b[39;49;00m\n",
      "        dataset[\u001b[33m\"\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = split[\u001b[33m\"\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\u001b[37m\u001b[39;49;00m\n",
      "        dataset[\u001b[33m\"\u001b[39;49;00m\u001b[33mvalidation\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = split[\u001b[33m\"\u001b[39;49;00m\u001b[33mtest\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# Prepare label mappings.\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# We'll include these in the model's config to get human readable labels in the Inference API.\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    labels = dataset[\u001b[33m\"\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m].features[\u001b[33m\"\u001b[39;49;00m\u001b[33mlabels\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m].names\u001b[37m\u001b[39;49;00m\n",
      "    label2id, id2label = {}, {}\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mfor\u001b[39;49;00m i, label \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(labels):\u001b[37m\u001b[39;49;00m\n",
      "        label2id[label] = \u001b[36mstr\u001b[39;49;00m(i)\u001b[37m\u001b[39;49;00m\n",
      "        id2label[\u001b[36mstr\u001b[39;49;00m(i)] = label\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# Load the accuracy metric from the datasets package\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    metric = evaluate.load(\u001b[33m\"\u001b[39;49;00m\u001b[33maccuracy\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# Define our compute_metrics function. It takes an `EvalPrediction` object (a namedtuple with a\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# predictions and label_ids field) and has to return a dictionary string to float.\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mcompute_metrics\u001b[39;49;00m(p):\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m        \u001b[39;49;00m\u001b[33m\"\"\"Computes accuracy on a batch of predictions\"\"\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mreturn\u001b[39;49;00m metric.compute(predictions=np.argmax(p.predictions, axis=\u001b[34m1\u001b[39;49;00m), references=p.label_ids)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    config = AutoConfig.from_pretrained(\u001b[37m\u001b[39;49;00m\n",
      "        model_args.config_name \u001b[35mor\u001b[39;49;00m model_args.model_name_or_path,\u001b[37m\u001b[39;49;00m\n",
      "        num_labels=\u001b[36mlen\u001b[39;49;00m(labels),\u001b[37m\u001b[39;49;00m\n",
      "        label2id=label2id,\u001b[37m\u001b[39;49;00m\n",
      "        id2label=id2label,\u001b[37m\u001b[39;49;00m\n",
      "        finetuning_task=\u001b[33m\"\u001b[39;49;00m\u001b[33mimage-classification\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        cache_dir=model_args.cache_dir,\u001b[37m\u001b[39;49;00m\n",
      "        revision=model_args.model_revision,\u001b[37m\u001b[39;49;00m\n",
      "        use_auth_token=\u001b[34mTrue\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m model_args.use_auth_token \u001b[34melse\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "    model = AutoModelForImageClassification.from_pretrained(\u001b[37m\u001b[39;49;00m\n",
      "        model_args.model_name_or_path,\u001b[37m\u001b[39;49;00m\n",
      "        from_tf=\u001b[36mbool\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m.ckpt\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[35min\u001b[39;49;00m model_args.model_name_or_path),\u001b[37m\u001b[39;49;00m\n",
      "        config=config,\u001b[37m\u001b[39;49;00m\n",
      "        cache_dir=model_args.cache_dir,\u001b[37m\u001b[39;49;00m\n",
      "        revision=model_args.model_revision,\u001b[37m\u001b[39;49;00m\n",
      "        use_auth_token=\u001b[34mTrue\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m model_args.use_auth_token \u001b[34melse\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        ignore_mismatched_sizes=model_args.ignore_mismatched_sizes,\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "    image_processor = AutoImageProcessor.from_pretrained(\u001b[37m\u001b[39;49;00m\n",
      "        model_args.image_processor_name \u001b[35mor\u001b[39;49;00m model_args.model_name_or_path,\u001b[37m\u001b[39;49;00m\n",
      "        cache_dir=model_args.cache_dir,\u001b[37m\u001b[39;49;00m\n",
      "        revision=model_args.model_revision,\u001b[37m\u001b[39;49;00m\n",
      "        use_auth_token=\u001b[34mTrue\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m model_args.use_auth_token \u001b[34melse\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# Define torchvision transforms to be applied to each image.\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mif\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mshortest_edge\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[35min\u001b[39;49;00m image_processor.size:\u001b[37m\u001b[39;49;00m\n",
      "        size = image_processor.size[\u001b[33m\"\u001b[39;49;00m\u001b[33mshortest_edge\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34melse\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "        size = (image_processor.size[\u001b[33m\"\u001b[39;49;00m\u001b[33mheight\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], image_processor.size[\u001b[33m\"\u001b[39;49;00m\u001b[33mwidth\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[37m\u001b[39;49;00m\n",
      "    normalize = Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\u001b[37m\u001b[39;49;00m\n",
      "    _train_transforms = Compose(\u001b[37m\u001b[39;49;00m\n",
      "        [\u001b[37m\u001b[39;49;00m\n",
      "            RandomResizedCrop(size),\u001b[37m\u001b[39;49;00m\n",
      "            RandomHorizontalFlip(),\u001b[37m\u001b[39;49;00m\n",
      "            ToTensor(),\u001b[37m\u001b[39;49;00m\n",
      "            normalize,\u001b[37m\u001b[39;49;00m\n",
      "        ]\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "    _val_transforms = Compose(\u001b[37m\u001b[39;49;00m\n",
      "        [\u001b[37m\u001b[39;49;00m\n",
      "            Resize(size),\u001b[37m\u001b[39;49;00m\n",
      "            CenterCrop(size),\u001b[37m\u001b[39;49;00m\n",
      "            ToTensor(),\u001b[37m\u001b[39;49;00m\n",
      "            normalize,\u001b[37m\u001b[39;49;00m\n",
      "        ]\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mtrain_transforms\u001b[39;49;00m(example_batch):\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m        \u001b[39;49;00m\u001b[33m\"\"\"Apply _train_transforms across a batch.\"\"\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        example_batch[\u001b[33m\"\u001b[39;49;00m\u001b[33mpixel_values\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = [\u001b[37m\u001b[39;49;00m\n",
      "            _train_transforms(pil_img.convert(\u001b[33m\"\u001b[39;49;00m\u001b[33mRGB\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)) \u001b[34mfor\u001b[39;49;00m pil_img \u001b[35min\u001b[39;49;00m example_batch[\u001b[33m\"\u001b[39;49;00m\u001b[33mimage\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\u001b[37m\u001b[39;49;00m\n",
      "        ]\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mreturn\u001b[39;49;00m example_batch\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mval_transforms\u001b[39;49;00m(example_batch):\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m        \u001b[39;49;00m\u001b[33m\"\"\"Apply _val_transforms across a batch.\"\"\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        example_batch[\u001b[33m\"\u001b[39;49;00m\u001b[33mpixel_values\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = [_val_transforms(pil_img.convert(\u001b[33m\"\u001b[39;49;00m\u001b[33mRGB\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)) \u001b[34mfor\u001b[39;49;00m pil_img \u001b[35min\u001b[39;49;00m example_batch[\u001b[33m\"\u001b[39;49;00m\u001b[33mimage\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]]\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mreturn\u001b[39;49;00m example_batch\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mif\u001b[39;49;00m training_args.do_train:\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mif\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m \u001b[35min\u001b[39;49;00m dataset:\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[34mraise\u001b[39;49;00m \u001b[36mValueError\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m--do_train requires a train dataset\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mif\u001b[39;49;00m data_args.max_train_samples \u001b[35mis\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "            dataset[\u001b[33m\"\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = (\u001b[37m\u001b[39;49;00m\n",
      "                dataset[\u001b[33m\"\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m].shuffle(seed=training_args.seed).select(\u001b[36mrange\u001b[39;49;00m(data_args.max_train_samples))\u001b[37m\u001b[39;49;00m\n",
      "            )\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[37m# Set the training transforms\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        dataset[\u001b[33m\"\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m].set_transform(train_transforms)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mif\u001b[39;49;00m training_args.do_eval:\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mif\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mvalidation\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m \u001b[35min\u001b[39;49;00m dataset:\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[34mraise\u001b[39;49;00m \u001b[36mValueError\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m--do_eval requires a validation dataset\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mif\u001b[39;49;00m data_args.max_eval_samples \u001b[35mis\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "            dataset[\u001b[33m\"\u001b[39;49;00m\u001b[33mvalidation\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = (\u001b[37m\u001b[39;49;00m\n",
      "                dataset[\u001b[33m\"\u001b[39;49;00m\u001b[33mvalidation\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m].shuffle(seed=training_args.seed).select(\u001b[36mrange\u001b[39;49;00m(data_args.max_eval_samples))\u001b[37m\u001b[39;49;00m\n",
      "            )\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[37m# Set the validation transforms\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        dataset[\u001b[33m\"\u001b[39;49;00m\u001b[33mvalidation\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m].set_transform(val_transforms)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# Initalize our trainer\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    trainer = Trainer(\u001b[37m\u001b[39;49;00m\n",
      "        model=model,\u001b[37m\u001b[39;49;00m\n",
      "        args=training_args,\u001b[37m\u001b[39;49;00m\n",
      "        train_dataset=dataset[\u001b[33m\"\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] \u001b[34mif\u001b[39;49;00m training_args.do_train \u001b[34melse\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        eval_dataset=dataset[\u001b[33m\"\u001b[39;49;00m\u001b[33mvalidation\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] \u001b[34mif\u001b[39;49;00m training_args.do_eval \u001b[34melse\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        compute_metrics=compute_metrics,\u001b[37m\u001b[39;49;00m\n",
      "        tokenizer=image_processor,\u001b[37m\u001b[39;49;00m\n",
      "        data_collator=collate_fn,\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# Training\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mif\u001b[39;49;00m training_args.do_train:\u001b[37m\u001b[39;49;00m\n",
      "        checkpoint = \u001b[34mNone\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mif\u001b[39;49;00m training_args.resume_from_checkpoint \u001b[35mis\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "            checkpoint = training_args.resume_from_checkpoint\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34melif\u001b[39;49;00m last_checkpoint \u001b[35mis\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "            checkpoint = last_checkpoint\u001b[37m\u001b[39;49;00m\n",
      "        train_result = trainer.train(resume_from_checkpoint=checkpoint)\u001b[37m\u001b[39;49;00m\n",
      "        trainer.save_model()\u001b[37m\u001b[39;49;00m\n",
      "        trainer.log_metrics(\u001b[33m\"\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, train_result.metrics)\u001b[37m\u001b[39;49;00m\n",
      "        trainer.save_metrics(\u001b[33m\"\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, train_result.metrics)\u001b[37m\u001b[39;49;00m\n",
      "        trainer.save_state()\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# Evaluation\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mif\u001b[39;49;00m training_args.do_eval:\u001b[37m\u001b[39;49;00m\n",
      "        metrics = trainer.evaluate()\u001b[37m\u001b[39;49;00m\n",
      "        trainer.log_metrics(\u001b[33m\"\u001b[39;49;00m\u001b[33meval\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, metrics)\u001b[37m\u001b[39;49;00m\n",
      "        trainer.save_metrics(\u001b[33m\"\u001b[39;49;00m\u001b[33meval\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, metrics)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# Write model card and (optionally) push to hub\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    kwargs = {\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mfinetuned_from\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: model_args.model_name_or_path,\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mtasks\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33mimage-classification\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mdataset\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: data_args.dataset_name,\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mtags\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: [\u001b[33m\"\u001b[39;49;00m\u001b[33mimage-classification\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mvision\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m],\u001b[37m\u001b[39;49;00m\n",
      "    }\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mif\u001b[39;49;00m training_args.push_to_hub:\u001b[37m\u001b[39;49;00m\n",
      "        trainer.push_to_hub(**kwargs)\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34melse\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "        trainer.create_model_card(**kwargs)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "    main()\u001b[37m\u001b[39;49;00m\n"
     ]
    }
   ],
   "source": [
    "!pygmentize run_image_classification.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8861405a-694d-4018-ae79-abe6c26d5aa5",
   "metadata": {},
   "source": [
    "- HuggingFace transformers API を使用して ViT モデルをファインチューニングします。\n",
    "- Neuron コア上で実行されるデータ型は、より効率を高めるために `fp32` ではなく `bf16` を使用します。\n",
    "- コンパイルされたモデルアーティファクトが保存されるモデルキャッシュディレクトリ（`./compiler_cache`）を指定します。\n",
    "- PyTorchの `torchrun` コマンドを使用してトレーニングジョブを起動します。\n",
    "- Inferentia2 (もしくは Trainium) チップを １つ搭載した　Inf2.xlarge (もしくは Trn1.2xlarge) 上での実行を想定しています。各チップは ２ つの Neuron コアを搭載しているため `num_workers=2` と設定、結果、トレーニングジョブは 2つの Neuron コア上で実行されます。\n",
    "- モデルを 10 エポック学習し、エポックごとにモデルのチェックポイントを保存します。保存できるチェックポイントは 1 つまでです。ロギング情報は 10 回ごとに出力します。\n",
    "-　`./output` ディレクトリには、ファインチューニングで生成されたモデルの重み、Config、その他のアーティファクトが格納されます"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27686c3c-1829-465d-b656-1c399755d401",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:torch.distributed.run:\n",
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "Please use the TrainiumTrainer from optimum[neuron] instead of the Transformers library to perform training on AWS Trainium instances. More information here: https://github.com/huggingface/optimum-neuron\n",
      "Please use the TrainiumTrainer from optimum[neuron] instead of the Transformers library to perform training on AWS Trainium instances. More information here: https://github.com/huggingface/optimum-neuron\n",
      "torch.distributed process group is initialized, but parallel_mode != ParallelMode.DISTRIBUTED. In order to use Torch DDP, launch your script with `python -m torch.distributed.launch\n",
      "torch.distributed process group is initialized, but parallel_mode != ParallelMode.DISTRIBUTED. In order to use Torch DDP, launch your script with `python -m torch.distributed.launch\n",
      "WARNING:__main__:Process rank: 0, device: xla:1, n_gpu: 0distributed training: True, 16-bits training: False\n",
      "INFO:__main__:Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=0,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=2e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=output/runs/Sep07_05-13-04_ip-172-31-11-245,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=10,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=10.0,\n",
      "optim=adamw_hf,\n",
      "optim_args=None,\n",
      "output_dir=output,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=16,\n",
      "per_device_train_batch_size=16,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=False,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=output,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=500,\n",
      "save_strategy=epoch,\n",
      "save_total_limit=1,\n",
      "seed=1337,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "/opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages/datasets/load.py:2089: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'token=None' instead.\n",
      "  warnings.warn(\n",
      "/opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages/datasets/load.py:2103: FutureWarning: 'task' was deprecated in version 2.13.0 and will be removed in 3.0.0.\n",
      "\n",
      "  warnings.warn(\n",
      "WARNING:__main__:Process rank: 1, device: xla:0, n_gpu: 0distributed training: True, 16-bits training: False\n",
      "/opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages/datasets/load.py:2089: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'token=None' instead.\n",
      "  warnings.warn(\n",
      "/opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages/datasets/load.py:2103: FutureWarning: 'task' was deprecated in version 2.13.0 and will be removed in 3.0.0.\n",
      "\n",
      "  warnings.warn(\n",
      "[INFO|configuration_utils.py:712] 2023-09-07 05:13:05,958 >> loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--google--vit-base-patch16-224-in21k/snapshots/7cbdb7ee3a6bcdf99dae654893f66519c480a0f8/config.json\n",
      "[INFO|configuration_utils.py:768] 2023-09-07 05:13:05,958 >> Model config ViTConfig {\n",
      "  \"_name_or_path\": \"google/vit-base-patch16-224-in21k\",\n",
      "  \"architectures\": [\n",
      "    \"ViTModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"finetuning_task\": \"image-classification\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"angular_leaf_spot\",\n",
      "    \"1\": \"bean_rust\",\n",
      "    \"2\": \"healthy\"\n",
      "  },\n",
      "  \"image_size\": 224,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"angular_leaf_spot\": \"0\",\n",
      "    \"bean_rust\": \"1\",\n",
      "    \"healthy\": \"2\"\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"qkv_bias\": true,\n",
      "  \"transformers_version\": \"4.31.0\"\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:2603] 2023-09-07 05:13:05,960 >> loading weights file pytorch_model.bin from cache at /home/ubuntu/.cache/huggingface/hub/models--google--vit-base-patch16-224-in21k/snapshots/7cbdb7ee3a6bcdf99dae654893f66519c480a0f8/pytorch_model.bin\n",
      "[WARNING|modeling_utils.py:3331] 2023-09-07 05:13:06,560 >> Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[INFO|modeling_utils.py:3319] 2023-09-07 05:13:06,565 >> Some weights of the model checkpoint at google/vit-base-patch16-224-in21k were not used when initializing ViTForImageClassification: ['pooler.dense.weight', 'pooler.dense.bias']\n",
      "- This IS expected if you are initializing ViTForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:3331] 2023-09-07 05:13:06,566 >> Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[INFO|image_processing_utils.py:339] 2023-09-07 05:13:06,671 >> loading configuration file preprocessor_config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--google--vit-base-patch16-224-in21k/snapshots/7cbdb7ee3a6bcdf99dae654893f66519c480a0f8/preprocessor_config.json\n",
      "[INFO|configuration_utils.py:712] 2023-09-07 05:13:06,762 >> loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--google--vit-base-patch16-224-in21k/snapshots/7cbdb7ee3a6bcdf99dae654893f66519c480a0f8/config.json\n",
      "[INFO|configuration_utils.py:768] 2023-09-07 05:13:06,762 >> Model config ViTConfig {\n",
      "  \"_name_or_path\": \"google/vit-base-patch16-224-in21k\",\n",
      "  \"architectures\": [\n",
      "    \"ViTModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"image_size\": 224,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"qkv_bias\": true,\n",
      "  \"transformers_version\": \"4.31.0\"\n",
      "}\n",
      "\n",
      "[INFO|image_processing_utils.py:572] 2023-09-07 05:13:06,852 >> size should be a dictionary on of the following set of keys: ({'height', 'width'}, {'shortest_edge'}, {'longest_edge', 'shortest_edge'}, {'longest_edge'}), got 224. Converted to {'height': 224, 'width': 224}.\n",
      "[INFO|image_processing_utils.py:389] 2023-09-07 05:13:06,852 >> Image processor ViTImageProcessor {\n",
      "  \"do_normalize\": true,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"image_mean\": [\n",
      "    0.5,\n",
      "    0.5,\n",
      "    0.5\n",
      "  ],\n",
      "  \"image_processor_type\": \"ViTImageProcessor\",\n",
      "  \"image_std\": [\n",
      "    0.5,\n",
      "    0.5,\n",
      "    0.5\n",
      "  ],\n",
      "  \"resample\": 2,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"size\": {\n",
      "    \"height\": 224,\n",
      "    \"width\": 224\n",
      "  }\n",
      "}\n",
      "\n",
      "/opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "[INFO|trainer.py:1686] 2023-09-07 05:13:17,356 >> ***** Running training *****\n",
      "[INFO|trainer.py:1687] 2023-09-07 05:13:17,356 >>   Num examples = 528\n",
      "[INFO|trainer.py:1688] 2023-09-07 05:13:17,356 >>   Num Epochs = 10\n",
      "[INFO|trainer.py:1689] 2023-09-07 05:13:17,356 >>   Instantaneous batch size per device = 16\n",
      "[INFO|trainer.py:1692] 2023-09-07 05:13:17,356 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1693] 2023-09-07 05:13:17,356 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1694] 2023-09-07 05:13:17,356 >>   Total optimization steps = 330\n",
      "[INFO|trainer.py:1695] 2023-09-07 05:13:17,357 >>   Number of trainable parameters = 85,800,963\n",
      "  0%|                                                   | 0/330 [00:00<?, ?it/s]2023-09-07 05:13:17.000493: INFO ||NCC_WRAPPER||: Compile cache path: ./compiler_cache\n",
      "2023-09-07 05:13:17.000495: INFO ||NCC_WRAPPER||: Using a cached neff at ./compiler_cache/neuronxcc-2.8.0.25+a3ad0f342/MODULE_10186977725140102164+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-Sep-07 05:13:17.0528 50288:50327 [0] nccl_net_ofi_init:1415 CCOM WARN NET/OFI aws-ofi-nccl initialization failed\n",
      "2023-Sep-07 05:13:17.0528 50288:50327 [0] init.cc:138 CCOM WARN OFI plugin initNet() failed is EFA enabled?\n",
      "  0%|▏                                          | 1/330 [00:00<04:03,  1.35it/s]2023-09-07 05:13:19.000062: INFO ||NCC_WRAPPER||: Compile cache path: ./compiler_cache\n",
      "2023-09-07 05:13:19.000314: INFO ||NCC_WRAPPER||: Using a cached neff at ./compiler_cache/neuronxcc-2.8.0.25+a3ad0f342/MODULE_7440857343363989218+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "  1%|▎                                          | 2/330 [00:02<08:12,  1.50s/it]2023-09-07 05:13:23.000897: INFO ||NCC_WRAPPER||: Compile cache path: ./compiler_cache\n",
      "2023-09-07 05:13:24.000162: INFO ||NCC_WRAPPER||: Using a cached neff at ./compiler_cache/neuronxcc-2.8.0.25+a3ad0f342/MODULE_6358742223045178700+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "  3%|█▎                                        | 10/330 [00:11<02:07,  2.51it/s]2023-09-07 05:13:29.000286: INFO ||NCC_WRAPPER||: Compile cache path: ./compiler_cache\n",
      "2023-09-07 05:13:29.000287: INFO ||NCC_WRAPPER||: Using a cached neff at ./compiler_cache/neuronxcc-2.8.0.25+a3ad0f342/MODULE_569571592325889951+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "{'loss': 1.025, 'learning_rate': 1.9393939393939395e-05, 'epoch': 0.3}          \n",
      "  3%|█▎                                        | 10/330 [00:11<02:07,  2.51it/s]2023-09-07 05:13:29.000454: INFO ||NCC_WRAPPER||: Compile cache path: ./compiler_cache\n",
      "2023-09-07 05:13:29.000455: INFO ||NCC_WRAPPER||: Using a cached neff at ./compiler_cache/neuronxcc-2.8.0.25+a3ad0f342/MODULE_931450859643720011+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "{'loss': 0.8656, 'learning_rate': 1.8787878787878792e-05, 'epoch': 0.61}        \n",
      "{'loss': 0.7344, 'learning_rate': 1.8181818181818182e-05, 'epoch': 0.91}        \n",
      " 10%|████▏                                     | 33/330 [00:15<00:39,  7.49it/s][INFO|trainer.py:2777] 2023-09-07 05:13:32,588 >> Saving model checkpoint to output/checkpoint-33\n",
      "[INFO|configuration_utils.py:458] 2023-09-07 05:13:32,590 >> Configuration saved in output/checkpoint-33/config.json\n",
      "[INFO|modeling_utils.py:1851] 2023-09-07 05:13:33,308 >> Model weights saved in output/checkpoint-33/pytorch_model.bin\n",
      "[INFO|image_processing_utils.py:234] 2023-09-07 05:13:33,309 >> Image processor saved in output/checkpoint-33/preprocessor_config.json\n",
      "[INFO|trainer.py:2894] 2023-09-07 05:13:34,670 >> Deleting older checkpoint [output/checkpoint-330] due to args.save_total_limit\n",
      "2023-09-07 05:13:34.000812: INFO ||NCC_WRAPPER||: Compile cache path: ./compiler_cache\n",
      "2023-09-07 05:13:34.000813: INFO ||NCC_WRAPPER||: Using a cached neff at ./compiler_cache/neuronxcc-2.8.0.25+a3ad0f342/MODULE_2784763037766117603+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "{'loss': 0.6047, 'learning_rate': 1.7575757575757576e-05, 'epoch': 1.21}        \n",
      "{'loss': 0.4953, 'learning_rate': 1.6969696969696972e-05, 'epoch': 1.52}        \n",
      "{'loss': 0.4023, 'learning_rate': 1.6363636363636366e-05, 'epoch': 1.82}        \n",
      " 20%|████████▍                                 | 66/330 [00:22<00:33,  7.89it/s][INFO|trainer.py:2777] 2023-09-07 05:13:39,652 >> Saving model checkpoint to output/checkpoint-66\n",
      "[INFO|configuration_utils.py:458] 2023-09-07 05:13:39,654 >> Configuration saved in output/checkpoint-66/config.json\n",
      "[INFO|modeling_utils.py:1851] 2023-09-07 05:13:40,315 >> Model weights saved in output/checkpoint-66/pytorch_model.bin\n",
      "[INFO|image_processing_utils.py:234] 2023-09-07 05:13:40,316 >> Image processor saved in output/checkpoint-66/preprocessor_config.json\n",
      "[INFO|trainer.py:2894] 2023-09-07 05:13:41,527 >> Deleting older checkpoint [output/checkpoint-33] due to args.save_total_limit\n",
      "{'loss': 0.3484, 'learning_rate': 1.575757575757576e-05, 'epoch': 2.12}         \n",
      "{'loss': 0.2797, 'learning_rate': 1.5151515151515153e-05, 'epoch': 2.42}        \n",
      "{'loss': 0.2445, 'learning_rate': 1.4545454545454546e-05, 'epoch': 2.73}        \n",
      " 30%|████████████▌                             | 99/330 [00:28<00:29,  7.96it/s][INFO|trainer.py:2777] 2023-09-07 05:13:46,380 >> Saving model checkpoint to output/checkpoint-99\n",
      "[INFO|configuration_utils.py:458] 2023-09-07 05:13:46,382 >> Configuration saved in output/checkpoint-99/config.json\n",
      "[INFO|modeling_utils.py:1851] 2023-09-07 05:13:46,977 >> Model weights saved in output/checkpoint-99/pytorch_model.bin\n",
      "[INFO|image_processing_utils.py:234] 2023-09-07 05:13:46,978 >> Image processor saved in output/checkpoint-99/preprocessor_config.json\n",
      "[INFO|trainer.py:2894] 2023-09-07 05:13:48,045 >> Deleting older checkpoint [output/checkpoint-66] due to args.save_total_limit\n",
      "{'loss': 0.2625, 'learning_rate': 1.3939393939393942e-05, 'epoch': 3.03}        \n",
      "{'loss': 0.2117, 'learning_rate': 1.3333333333333333e-05, 'epoch': 3.33}        \n",
      "{'loss': 0.1664, 'learning_rate': 1.2727272727272728e-05, 'epoch': 3.64}        \n",
      "{'loss': 0.1762, 'learning_rate': 1.2121212121212122e-05, 'epoch': 3.94}        \n",
      " 40%|████████████████▎                        | 131/330 [00:35<00:32,  6.10it/s][INFO|trainer.py:2777] 2023-09-07 05:13:53,019 >> Saving model checkpoint to output/checkpoint-132\n",
      "[INFO|configuration_utils.py:458] 2023-09-07 05:13:53,023 >> Configuration saved in output/checkpoint-132/config.json\n",
      "[INFO|modeling_utils.py:1851] 2023-09-07 05:13:53,676 >> Model weights saved in output/checkpoint-132/pytorch_model.bin\n",
      "[INFO|image_processing_utils.py:234] 2023-09-07 05:13:53,676 >> Image processor saved in output/checkpoint-132/preprocessor_config.json\n",
      "[INFO|trainer.py:2894] 2023-09-07 05:13:54,913 >> Deleting older checkpoint [output/checkpoint-99] due to args.save_total_limit\n",
      "{'loss': 0.1566, 'learning_rate': 1.1515151515151517e-05, 'epoch': 4.24}        \n",
      "{'loss': 0.1512, 'learning_rate': 1.0909090909090909e-05, 'epoch': 4.55}        \n",
      "{'loss': 0.1457, 'learning_rate': 1.0303030303030304e-05, 'epoch': 4.85}        \n",
      " 50%|████████████████████▌                    | 165/330 [00:42<00:21,  7.73it/s][INFO|trainer.py:2777] 2023-09-07 05:13:59,778 >> Saving model checkpoint to output/checkpoint-165\n",
      "[INFO|configuration_utils.py:458] 2023-09-07 05:13:59,780 >> Configuration saved in output/checkpoint-165/config.json\n",
      "[INFO|modeling_utils.py:1851] 2023-09-07 05:14:00,433 >> Model weights saved in output/checkpoint-165/pytorch_model.bin\n",
      "[INFO|image_processing_utils.py:234] 2023-09-07 05:14:00,433 >> Image processor saved in output/checkpoint-165/preprocessor_config.json\n",
      "[INFO|trainer.py:2894] 2023-09-07 05:14:01,579 >> Deleting older checkpoint [output/checkpoint-132] due to args.save_total_limit\n",
      "{'loss': 0.1504, 'learning_rate': 9.696969696969698e-06, 'epoch': 5.15}         \n",
      "{'loss': 0.1418, 'learning_rate': 9.090909090909091e-06, 'epoch': 5.45}         \n",
      "{'loss': 0.1309, 'learning_rate': 8.484848484848486e-06, 'epoch': 5.76}         \n",
      " 60%|████████████████████████▌                | 198/330 [00:48<00:16,  7.90it/s][INFO|trainer.py:2777] 2023-09-07 05:14:06,445 >> Saving model checkpoint to output/checkpoint-198\n",
      "[INFO|configuration_utils.py:458] 2023-09-07 05:14:06,447 >> Configuration saved in output/checkpoint-198/config.json\n",
      "[INFO|modeling_utils.py:1851] 2023-09-07 05:14:07,110 >> Model weights saved in output/checkpoint-198/pytorch_model.bin\n",
      "[INFO|image_processing_utils.py:234] 2023-09-07 05:14:07,111 >> Image processor saved in output/checkpoint-198/preprocessor_config.json\n",
      "[INFO|trainer.py:2894] 2023-09-07 05:14:08,303 >> Deleting older checkpoint [output/checkpoint-165] due to args.save_total_limit\n",
      "{'loss': 0.1199, 'learning_rate': 7.87878787878788e-06, 'epoch': 6.06}          \n",
      "{'loss': 0.1402, 'learning_rate': 7.272727272727273e-06, 'epoch': 6.36}         \n",
      "{'loss': 0.1496, 'learning_rate': 6.666666666666667e-06, 'epoch': 6.67}         \n",
      "{'loss': 0.1043, 'learning_rate': 6.060606060606061e-06, 'epoch': 6.97}         \n",
      " 70%|████████████████████████████▋            | 231/330 [00:55<00:16,  6.17it/s][INFO|trainer.py:2777] 2023-09-07 05:14:13,212 >> Saving model checkpoint to output/checkpoint-231\n",
      "[INFO|configuration_utils.py:458] 2023-09-07 05:14:13,214 >> Configuration saved in output/checkpoint-231/config.json\n",
      "[INFO|modeling_utils.py:1851] 2023-09-07 05:14:13,740 >> Model weights saved in output/checkpoint-231/pytorch_model.bin\n",
      "[INFO|image_processing_utils.py:234] 2023-09-07 05:14:13,740 >> Image processor saved in output/checkpoint-231/preprocessor_config.json\n",
      "[INFO|trainer.py:2894] 2023-09-07 05:14:14,801 >> Deleting older checkpoint [output/checkpoint-198] due to args.save_total_limit\n",
      "{'loss': 0.1043, 'learning_rate': 5.4545454545454545e-06, 'epoch': 7.27}        \n",
      "{'loss': 0.1445, 'learning_rate': 4.848484848484849e-06, 'epoch': 7.58}         \n",
      "{'loss': 0.107, 'learning_rate': 4.242424242424243e-06, 'epoch': 7.88}          \n",
      " 80%|████████████████████████████████▊        | 264/330 [01:02<00:08,  7.54it/s][INFO|trainer.py:2777] 2023-09-07 05:14:19,648 >> Saving model checkpoint to output/checkpoint-264\n",
      "[INFO|configuration_utils.py:458] 2023-09-07 05:14:19,650 >> Configuration saved in output/checkpoint-264/config.json\n",
      "[INFO|modeling_utils.py:1851] 2023-09-07 05:14:20,191 >> Model weights saved in output/checkpoint-264/pytorch_model.bin\n",
      "[INFO|image_processing_utils.py:234] 2023-09-07 05:14:20,191 >> Image processor saved in output/checkpoint-264/preprocessor_config.json\n",
      "[INFO|trainer.py:2894] 2023-09-07 05:14:21,259 >> Deleting older checkpoint [output/checkpoint-231] due to args.save_total_limit\n",
      "{'loss': 0.0822, 'learning_rate': 3.6363636363636366e-06, 'epoch': 8.18}        \n",
      "{'loss': 0.1258, 'learning_rate': 3.0303030303030305e-06, 'epoch': 8.48}        \n",
      "{'loss': 0.0967, 'learning_rate': 2.4242424242424244e-06, 'epoch': 8.79}        \n",
      " 90%|████████████████████████████████████▉    | 297/330 [01:08<00:04,  7.85it/s][INFO|trainer.py:2777] 2023-09-07 05:14:26,115 >> Saving model checkpoint to output/checkpoint-297\n",
      "[INFO|configuration_utils.py:458] 2023-09-07 05:14:26,116 >> Configuration saved in output/checkpoint-297/config.json\n",
      "[INFO|modeling_utils.py:1851] 2023-09-07 05:14:26,696 >> Model weights saved in output/checkpoint-297/pytorch_model.bin\n",
      "[INFO|image_processing_utils.py:234] 2023-09-07 05:14:26,697 >> Image processor saved in output/checkpoint-297/preprocessor_config.json\n",
      "[INFO|trainer.py:2894] 2023-09-07 05:14:27,817 >> Deleting older checkpoint [output/checkpoint-264] due to args.save_total_limit\n",
      "{'loss': 0.1195, 'learning_rate': 1.8181818181818183e-06, 'epoch': 9.09}        \n",
      "{'loss': 0.1012, 'learning_rate': 1.2121212121212122e-06, 'epoch': 9.39}        \n",
      "{'loss': 0.1148, 'learning_rate': 6.060606060606061e-07, 'epoch': 9.7}          \n",
      "{'loss': 0.1236, 'learning_rate': 0.0, 'epoch': 10.0}                           \n",
      "100%|█████████████████████████████████████████| 330/330 [01:15<00:00,  7.99it/s][INFO|trainer.py:2777] 2023-09-07 05:14:32,690 >> Saving model checkpoint to output/checkpoint-330\n",
      "[INFO|configuration_utils.py:458] 2023-09-07 05:14:32,691 >> Configuration saved in output/checkpoint-330/config.json\n",
      "[INFO|modeling_utils.py:1851] 2023-09-07 05:14:33,234 >> Model weights saved in output/checkpoint-330/pytorch_model.bin\n",
      "[INFO|image_processing_utils.py:234] 2023-09-07 05:14:33,234 >> Image processor saved in output/checkpoint-330/preprocessor_config.json\n",
      "[INFO|trainer.py:2894] 2023-09-07 05:14:34,291 >> Deleting older checkpoint [output/checkpoint-297] due to args.save_total_limit\n",
      "[INFO|trainer.py:1934] 2023-09-07 05:14:34,367 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 77.0132, 'train_samples_per_second': 68.56, 'train_steps_per_second': 4.285, 'train_loss': 0.2523378314393939, 'epoch': 10.0}\n",
      "100%|█████████████████████████████████████████| 330/330 [01:17<00:00,  4.29it/s]\n",
      "[INFO|trainer.py:2777] 2023-09-07 05:14:34,371 >> Saving model checkpoint to output\n",
      "[INFO|configuration_utils.py:458] 2023-09-07 05:14:34,372 >> Configuration saved in output/config.json\n",
      "[INFO|modeling_utils.py:1851] 2023-09-07 05:14:36,487 >> Model weights saved in output/pytorch_model.bin\n",
      "[INFO|image_processing_utils.py:234] 2023-09-07 05:14:36,488 >> Image processor saved in output/preprocessor_config.json\n",
      "***** train metrics *****\n",
      "  epoch                    =       10.0\n",
      "  train_loss               =     0.2523\n",
      "  train_runtime            = 0:01:17.01\n",
      "  train_samples_per_second =      68.56\n",
      "  train_steps_per_second   =      4.285\n",
      "[INFO|trainer.py:3081] 2023-09-07 05:14:36,489 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3083] 2023-09-07 05:14:36,489 >>   Num examples = 80\n",
      "[INFO|trainer.py:3086] 2023-09-07 05:14:36,489 >>   Batch size = 16\n",
      "2023-09-07 05:14:37.000080: INFO ||NCC_WRAPPER||: Compile cache path: ./compiler_cache\n",
      "2023-09-07 05:14:37.000139: INFO ||NCC_WRAPPER||: Using a cached neff at ./compiler_cache/neuronxcc-2.8.0.25+a3ad0f342/MODULE_2854566206946126162+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-09-07 05:14:37.000877: INFO ||NCC_WRAPPER||: Compile cache path: ./compiler_cache\n",
      "2023-09-07 05:14:37.000878: INFO ||NCC_WRAPPER||: Using a cached neff at ./compiler_cache/neuronxcc-2.8.0.25+a3ad0f342/MODULE_4411201967751710878+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-09-07 05:14:37.000878: INFO ||NCC_WRAPPER||: Compile cache path: ./compiler_cache\n",
      "2023-09-07 05:14:37.000879: INFO ||NCC_WRAPPER||: Using a cached neff at ./compiler_cache/neuronxcc-2.8.0.25+a3ad0f342/MODULE_16407023877168380706+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-09-07 05:14:38.000038: INFO ||NCC_WRAPPER||: Compile cache path: ./compiler_cache\n",
      "2023-09-07 05:14:38.000039: INFO ||NCC_WRAPPER||: Using a cached neff at ./compiler_cache/neuronxcc-2.8.0.25+a3ad0f342/MODULE_10989985540458594900+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-09-07 05:14:38.000042: INFO ||NCC_WRAPPER||: Compile cache path: ./compiler_cache\n",
      "2023-09-07 05:14:38.000043: INFO ||NCC_WRAPPER||: Using a cached neff at ./compiler_cache/neuronxcc-2.8.0.25+a3ad0f342/MODULE_12751598928191034930+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-09-07 05:14:38.000226: INFO ||NCC_WRAPPER||: Compile cache path: ./compiler_cache\n",
      "2023-09-07 05:14:38.000226: INFO ||NCC_WRAPPER||: Compile cache path: ./compiler_cache\n",
      "2023-09-07 05:14:38.000227: INFO ||NCC_WRAPPER||: Using a cached neff at ./compiler_cache/neuronxcc-2.8.0.25+a3ad0f342/MODULE_6439607758445774205+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-09-07 05:14:38.000227: INFO ||NCC_WRAPPER||: Using a cached neff at ./compiler_cache/neuronxcc-2.8.0.25+a3ad0f342/MODULE_8598648585105326671+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-09-07 05:14:38.000381: INFO ||NCC_WRAPPER||: Compile cache path: ./compiler_cache\n",
      "2023-09-07 05:14:38.000382: INFO ||NCC_WRAPPER||: Using a cached neff at ./compiler_cache/neuronxcc-2.8.0.25+a3ad0f342/MODULE_6571166010089848396+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-09-07 05:14:38.000382: INFO ||NCC_WRAPPER||: Compile cache path: ./compiler_cache\n",
      "2023-09-07 05:14:38.000383: INFO ||NCC_WRAPPER||: Using a cached neff at ./compiler_cache/neuronxcc-2.8.0.25+a3ad0f342/MODULE_12991406247443270585+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "  0%|                                                     | 0/5 [00:00<?, ?it/s]2023-09-07 05:14:38.000619: INFO ||NCC_WRAPPER||: Compile cache path: ./compiler_cache\n",
      "2023-09-07 05:14:38.000619: INFO ||NCC_WRAPPER||: Compile cache path: ./compiler_cache\n",
      "2023-09-07 05:14:38.000619: INFO ||NCC_WRAPPER||: Using a cached neff at ./compiler_cache/neuronxcc-2.8.0.25+a3ad0f342/MODULE_11120858686510186620+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-09-07 05:14:38.000620: INFO ||NCC_WRAPPER||: Using a cached neff at ./compiler_cache/neuronxcc-2.8.0.25+a3ad0f342/MODULE_2506019054446791935+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-09-07 05:14:38.000806: INFO ||NCC_WRAPPER||: Compile cache path: ./compiler_cache\n",
      "2023-09-07 05:14:38.000806: INFO ||NCC_WRAPPER||: Compile cache path: ./compiler_cache\n",
      "2023-09-07 05:14:38.000807: INFO ||NCC_WRAPPER||: Using a cached neff at ./compiler_cache/neuronxcc-2.8.0.25+a3ad0f342/MODULE_11401904622186785880+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-09-07 05:14:38.000807: INFO ||NCC_WRAPPER||: Using a cached neff at ./compiler_cache/neuronxcc-2.8.0.25+a3ad0f342/MODULE_11228636114469661807+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      " 40%|██████████████████                           | 2/5 [00:00<00:00,  4.73it/s]2023-09-07 05:14:38.000990: INFO ||NCC_WRAPPER||: Compile cache path: ./compiler_cache\n",
      "2023-09-07 05:14:38.000991: INFO ||NCC_WRAPPER||: Using a cached neff at ./compiler_cache/neuronxcc-2.8.0.25+a3ad0f342/MODULE_15664435241768717754+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-09-07 05:14:39.000193: INFO ||NCC_WRAPPER||: Compile cache path: ./compiler_cache\n",
      "2023-09-07 05:14:39.000194: INFO ||NCC_WRAPPER||: Using a cached neff at ./compiler_cache/neuronxcc-2.8.0.25+a3ad0f342/MODULE_5858131689908442359+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-09-07 05:14:39.000195: INFO ||NCC_WRAPPER||: Compile cache path: ./compiler_cache\n",
      "2023-09-07 05:14:39.000196: INFO ||NCC_WRAPPER||: Using a cached neff at ./compiler_cache/neuronxcc-2.8.0.25+a3ad0f342/MODULE_9695776144602196295+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-09-07 05:14:39.000381: INFO ||NCC_WRAPPER||: Compile cache path: ./compiler_cache\n",
      "2023-09-07 05:14:39.000382: INFO ||NCC_WRAPPER||: Using a cached neff at ./compiler_cache/neuronxcc-2.8.0.25+a3ad0f342/MODULE_14997005377111877814+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-09-07 05:14:39.000383: INFO ||NCC_WRAPPER||: Compile cache path: ./compiler_cache\n",
      "2023-09-07 05:14:39.000384: INFO ||NCC_WRAPPER||: Using a cached neff at ./compiler_cache/neuronxcc-2.8.0.25+a3ad0f342/MODULE_11633199607740551230+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      " 60%|███████████████████████████                  | 3/5 [00:00<00:00,  2.75it/s]2023-09-07 05:14:39.000568: INFO ||NCC_WRAPPER||: Compile cache path: ./compiler_cache\n",
      "2023-09-07 05:14:39.000569: INFO ||NCC_WRAPPER||: Using a cached neff at ./compiler_cache/neuronxcc-2.8.0.25+a3ad0f342/MODULE_10728869769052769180+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-09-07 05:14:39.000771: INFO ||NCC_WRAPPER||: Compile cache path: ./compiler_cache\n",
      "2023-09-07 05:14:39.000772: INFO ||NCC_WRAPPER||: Using a cached neff at ./compiler_cache/neuronxcc-2.8.0.25+a3ad0f342/MODULE_12967210791014275590+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-09-07 05:14:39.000774: INFO ||NCC_WRAPPER||: Compile cache path: ./compiler_cache\n",
      "2023-09-07 05:14:39.000775: INFO ||NCC_WRAPPER||: Using a cached neff at ./compiler_cache/neuronxcc-2.8.0.25+a3ad0f342/MODULE_1531972595600023427+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-09-07 05:14:39.000961: INFO ||NCC_WRAPPER||: Compile cache path: ./compiler_cache\n",
      "2023-09-07 05:14:39.000962: INFO ||NCC_WRAPPER||: Using a cached neff at ./compiler_cache/neuronxcc-2.8.0.25+a3ad0f342/MODULE_16526598599832905732+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-09-07 05:14:39.000964: INFO ||NCC_WRAPPER||: Compile cache path: ./compiler_cache\n",
      "2023-09-07 05:14:39.000965: INFO ||NCC_WRAPPER||: Using a cached neff at ./compiler_cache/neuronxcc-2.8.0.25+a3ad0f342/MODULE_16704114537050172472+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      " 80%|████████████████████████████████████         | 4/5 [00:01<00:00,  2.25it/s]2023-09-07 05:14:40.000149: INFO ||NCC_WRAPPER||: Compile cache path: ./compiler_cache\n",
      "2023-09-07 05:14:40.000150: INFO ||NCC_WRAPPER||: Using a cached neff at ./compiler_cache/neuronxcc-2.8.0.25+a3ad0f342/MODULE_4346820354563907805+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-09-07 05:14:40.000352: INFO ||NCC_WRAPPER||: Compile cache path: ./compiler_cache\n",
      "2023-09-07 05:14:40.000353: INFO ||NCC_WRAPPER||: Using a cached neff at ./compiler_cache/neuronxcc-2.8.0.25+a3ad0f342/MODULE_14015629166471374106+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-09-07 05:14:40.000356: INFO ||NCC_WRAPPER||: Compile cache path: ./compiler_cache\n",
      "2023-09-07 05:14:40.000357: INFO ||NCC_WRAPPER||: Using a cached neff at ./compiler_cache/neuronxcc-2.8.0.25+a3ad0f342/MODULE_14365343826444546122+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-09-07 05:14:40.000545: INFO ||NCC_WRAPPER||: Compile cache path: ./compiler_cache\n",
      "2023-09-07 05:14:40.000546: INFO ||NCC_WRAPPER||: Compile cache path: ./compiler_cache\n",
      "2023-09-07 05:14:40.000547: INFO ||NCC_WRAPPER||: Using a cached neff at ./compiler_cache/neuronxcc-2.8.0.25+a3ad0f342/MODULE_14102846367852446252+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-09-07 05:14:40.000547: INFO ||NCC_WRAPPER||: Using a cached neff at ./compiler_cache/neuronxcc-2.8.0.25+a3ad0f342/MODULE_8765945782161052574+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "100%|█████████████████████████████████████████████| 5/5 [00:02<00:00,  2.02it/s]2023-09-07 05:14:40.000736: INFO ||NCC_WRAPPER||: Compile cache path: ./compiler_cache\n",
      "2023-09-07 05:14:40.000737: INFO ||NCC_WRAPPER||: Using a cached neff at ./compiler_cache/neuronxcc-2.8.0.25+a3ad0f342/MODULE_1620854833423187426+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "100%|█████████████████████████████████████████████| 5/5 [00:02<00:00,  2.07it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =       10.0\n",
      "  eval_accuracy           =     0.9875\n",
      "  eval_loss               =     0.1033\n",
      "  eval_runtime            = 0:00:04.33\n",
      "  eval_samples_per_second =      18.47\n",
      "  eval_steps_per_second   =      0.693\n",
      "CPU times: user 1.24 s, sys: 307 ms, total: 1.55 s\n",
      "Wall time: 1min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!XLA_USE_BF16=1 NEURON_CC_FLAGS=\"--cache_dir=./compiler_cache\" \\\n",
    "torchrun --nproc_per_node=2 run_image_classification.py \\\n",
    "--model_name_or_path \"google/vit-base-patch16-224-in21k\" \\\n",
    "--dataset_name \"beans\" \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--num_train_epochs 10 \\\n",
    "--per_device_train_batch_size 16 \\\n",
    "--per_device_eval_batch_size 16 \\\n",
    "--learning_rate 2e-5 \\\n",
    "--logging_strategy steps \\\n",
    "--logging_steps 10 \\\n",
    "--save_strategy epoch \\\n",
    "--save_total_limit 1 \\\n",
    "--seed 1337 \\\n",
    "--remove_unused_columns False \\\n",
    "--overwrite_output_dir \\\n",
    "--output_dir \"output\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5101401-df06-4a6d-9780-8832b9f28e01",
   "metadata": {},
   "source": [
    "コンパイル時間を含んだ学習には `inf2.8xlarge` 上で実行した場合で 25\\~30分程度かかります.\n",
    "2度目以降の実行ではコンパイル済みのキャッシュが利用可能なため、1\\~2分程度で学習が完了します。\n",
    "\n",
    "`neuron_parallel_compile` コマンドを利用したコンパイル時間の削減方法については、[日本語BERTモデルのサンプル](https://github.com/AWShtokoyo/aws-ml-jp/tree/main/frameworks/aws-neuron-jp/bertj_finetuning_classification)を参照下さい。\n",
    "\n",
    "\n",
    "これで　AWS Inferentia2 上での ViT モデルのファインチューニングに成功しました。 \n",
    "`pytorch_model.bin` という名前のファインチューニングされた重みを持つモデル、`Trainer` の状態、モデル設定ファイル（`config.json`） を含むファイルのリストが表示されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9a484cc-a790-4544-ae04-793ea288c16a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 335268\n",
      "-rw-rw-r-- 1 ubuntu ubuntu      1614 Sep  7 05:14 README.md\n",
      "-rw-rw-r-- 1 ubuntu ubuntu       329 Sep  7 05:14 all_results.json\n",
      "drwxrwxr-x 2 ubuntu ubuntu      4096 Sep  7 05:14 checkpoint-330\n",
      "-rw-rw-r-- 1 ubuntu ubuntu       845 Sep  7 05:14 config.json\n",
      "-rw-rw-r-- 1 ubuntu ubuntu       185 Sep  7 05:14 eval_results.json\n",
      "-rw-rw-r-- 1 ubuntu ubuntu       325 Sep  7 05:14 preprocessor_config.json\n",
      "-rw-rw-r-- 1 ubuntu ubuntu 343269037 Sep  7 05:14 pytorch_model.bin\n",
      "-rw-rw-r-- 1 ubuntu ubuntu       165 Sep  7 05:14 train_results.json\n",
      "-rw-rw-r-- 1 ubuntu ubuntu      4548 Sep  7 05:14 trainer_state.json\n",
      "-rw-rw-r-- 1 ubuntu ubuntu      3963 Sep  7 05:14 training_args.bin\n"
     ]
    }
   ],
   "source": [
    "!ls -l ./output/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca27169e-de1c-438e-b095-089dca381925",
   "metadata": {},
   "source": [
    "# ViT 推論"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba773524-55dd-4999-a42d-41deec2e9077",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create model from provided checkpoint: ./output/\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "import torch\n",
    "import torch_neuronx\n",
    "from transformers import ViTImageProcessor, ViTForImageClassification\n",
    "\n",
    "# Create the feature extractor and model\n",
    "checkpoint_dir = './output/'\n",
    "print(f\"Create model from provided checkpoint: {checkpoint_dir}\")\n",
    "feature_extractor = ViTImageProcessor.from_pretrained(checkpoint_dir)\n",
    "model = ViTForImageClassification.from_pretrained(checkpoint_dir, torchscript=True)\n",
    "model.eval()\n",
    "\n",
    "# Get an example input\n",
    "url = \"https://datasets-server.huggingface.co/assets/beans/--/default/test/0/image/image.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
    "example = (inputs['pixel_values'],)\n",
    "\n",
    "# Run inference on CPU\n",
    "output_cpu = model(*example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6950e6-7e4d-40a5-84d0-fd65e6265f15",
   "metadata": {},
   "source": [
    "## 推論実行のためのモデルの事前コンパイル\n",
    "\n",
    "推論を AWS Inferentia2　または AWS Trainium 上で実行するためには、モデルを`torch_neuronx.trace` APIを用いて事前にトレース（コンパイル）する必要があります。トレース（コンパイル）した結果は保存することで再利用可能です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "936134c6-12bb-426f-9316-1f6b9e09bbad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compile model for neuron with torch tracing ...\n",
      "Save compiled model as: vit-model-neuron.pt\n",
      "CPU times: user 7.65 s, sys: 2.31 s, total: 9.96 s\n",
      "Wall time: 40.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Compile the model for neuron\n",
    "print(f\"Compile model for neuron with torch tracing ...\")\n",
    "model_neuron = torch_neuronx.trace(model, example)\n",
    "\n",
    "# Save the TorchScript for inference deployment\n",
    "filename = 'vit-model-neuron.pt'\n",
    "torch.jit.save(model_neuron, filename)\n",
    "print(f\"Save compiled model as: {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac19f68-ebe6-464a-937f-1bbcb9b9b076",
   "metadata": {},
   "source": [
    "期待通りの出力が得られるかどうか　CPU上での推論結果と比較します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8658aef-2c36-4d1d-bdd5-13bb38db0be8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the TorchScript compiled model\n",
    "print(f\"Load compiled model: {filename}\")\n",
    "model_neuron = torch.jit.load(filename)\n",
    "\n",
    "# Run inference using the Neuron model\n",
    "print(f\"Run inference on the test image: {url}\")\n",
    "output_neuron = model_neuron(*example)\n",
    "\n",
    "# Compare the results\n",
    "print(f\"--- Compare Neuron output against CPU output ----\")\n",
    "print(f\"CPU tensor:            {output_cpu[0][0][0:10]}\")\n",
    "print(f\"Neuron tensor:         {output_neuron[0][0][0:10]}\")\n",
    "print(f\"CPU prediction:    {model.config.id2label[output_cpu[0].argmax(-1).item()]}\")\n",
    "print(f\"Neuron prediction: {model.config.id2label[output_neuron[0].argmax(-1).item()]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bfaae7-da47-458b-b6fa-023e2e5a06f8",
   "metadata": {},
   "source": [
    "## Gradio API を用いた推論デモ\n",
    "\n",
    "モデルサービスのデモをセットアップする簡易な方法は、Gradio API を使用することです。画像をアップロードしてモデルに与え、推論結果を確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a024e5-b244-488f-9bee-7d7b74a59f5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "from torchvision.transforms import (\n",
    "    CenterCrop,\n",
    "    Compose,\n",
    "    Normalize,\n",
    "    RandomHorizontalFlip,\n",
    "    RandomResizedCrop,\n",
    "    Resize,\n",
    "    ToTensor,\n",
    ")\n",
    "\n",
    "id2label = {0: 'angular_leaf_spot　(角葉スポット)', 1: 'bean_rust　(豆さび病)', 2: 'healthy　(健康)'}\n",
    "\n",
    "def predict(raw_image):\n",
    "    size = (224, 224)\n",
    "    image_mean = [0.5, 0.5, 0.5]\n",
    "    image_std = [0.5, 0.5, 0.5]\n",
    "    normalize = Normalize(mean=image_mean, std=image_std)\n",
    "    \n",
    "    _val_transforms = Compose(\n",
    "        [\n",
    "            Resize(size),\n",
    "            CenterCrop(size),\n",
    "            ToTensor(),\n",
    "            normalize,\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    transformed_image = _val_transforms(raw_image.convert(\"RGB\"))\n",
    "    batched_transformed_image = transformed_image.unsqueeze(0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        prediction = model_neuron(batched_transformed_image)\n",
    "        pred = id2label[prediction[0].argmax(-1).item()]\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a1f39d-d001-46d1-a2cf-a19edcad9a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "demo = gr.Interface(fn=predict,\n",
    "             inputs=gr.Image(type=\"pil\"),\n",
    "             outputs=\"text\",\n",
    "             examples=[\n",
    "                 'image_samples/healthy_test.21.jpg',\n",
    "                 'image_samples/angular_leaf_spot_test.21.jpg',\n",
    "                 'image_samples/bean_rust_test.34.jpg'])\n",
    "\n",
    "demo.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
