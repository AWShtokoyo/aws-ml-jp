{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4718e6d-7761-415a-87a1-8946df7087f2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ViT Model Fine-tuning & Deployment on Inferentia2/Trainium\n",
    "\n",
    "ViT モデルは、テキストベースのタスク用に設計された transformer アーキテクチャに基づくビジュアルモデルです。\n",
    "\n",
    "ImageNet-21K データセットで事前学習された　ViT モデルを、Beans データセットでファインチューニングします。\n",
    "このモデルでは、数エポック学習することで、Beans(葉)の健康状態を3つのカテゴリに分類して予測可能です。 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362c27b4-1c5f-4295-8b48-ad423283f1b1",
   "metadata": {},
   "source": [
    "## 事前準備\n",
    "本 notebookは Neuron 2.14.0 がインストールされた Amazon EC2 inf2.xlarge 上で動作確認しています。\n",
    "（より大きいサイズの Inf2 インスタンス及び Trn1 インスタンス上でも実行可能です。）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115c03c9-5b62-4cd6-9a93-657a95e337ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install -U pip\n",
    "!pip install -U transformers==4.31.0 accelerate evaluate gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3324f68f-1307-4ef4-99c8-b0d2414bbea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip list | grep \"neuron\\|torch\\|transformers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf201a69-ce0a-493d-86dd-565d2c067a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "!dpkg --list | grep neuron\n",
    "# For Ubuntu Environment. Please use \"yum list installed\" for Amazon Linux Environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d12f43b-ee0c-4bcc-bda7-b62526202391",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo rmmod neuron; sudo modprobe neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccde899b-8e68-4cfe-8708-6a586e077082",
   "metadata": {},
   "source": [
    "## Trainer API を使用した トレーニング（ファインチューニング）実行\n",
    "Huggin Face 🤗Transformers には Trainer という便利なクラスがあり、Torch Neuron からも利用可能です。 ここでは Trainer API を利用してトレーニングを実行していきます。\n",
    "\n",
    "Neuron SDK では　Huggin Face 🤗Transformers 上の`run_image_classification.py`スクリプトを 変更せずにそのまま適用可能なので、あらかじめダウンロードします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1e62f5-789c-44d5-8cc4-c2e0e6c34f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/huggingface/transformers/v4.31.0/examples/pytorch/image-classification/run_image_classification.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4acb85-ec13-446e-ac78-e48472b2d851",
   "metadata": {},
   "source": [
    "`run_image_classification.py` スクリプトの内容を確認してみましょう。Trainer API を利用してトレーニングを実行していることが確認できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1daf4b96-f36b-4d67-aeee-2bc5d3f160b1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pygmentize run_image_classification.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8861405a-694d-4018-ae79-abe6c26d5aa5",
   "metadata": {},
   "source": [
    "- Hugging Face 🤗Transformers を使用して ViT モデルをファインチューニングします。\n",
    "- Neuron コア上で実行されるデータ型は、より効率を高めるために `fp32` ではなく `bf16` を使用します。\n",
    "- コンパイルされたモデルアーティファクトが保存されるモデルキャッシュディレクトリ（`./compiler_cache`）を指定します。\n",
    "- PyTorchの `torchrun` コマンドを使用してトレーニングジョブを起動します。\n",
    "- AWS Inferentia2 (もしくは AWS Trainium) アクセラレータチップを １つ搭載した　Inf2.xlarge (もしくは Trn1.2xlarge) 上での実行を想定しています。各チップは ２ つの Neuron コアを搭載しているため `num_workers=2` と設定、結果、トレーニングジョブは 2つの Neuron コア上で実行されます。\n",
    "- モデルを 10 エポック学習し、エポックごとにモデルのチェックポイントを保存します。保存できるチェックポイントは 1 つまでです。ロギング情報は 10 回ごとに出力します。\n",
    "-　`./output` ディレクトリには、ファインチューニングで生成されたモデルの重み、Config、その他のアーティファクトが格納されます"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27686c3c-1829-465d-b656-1c399755d401",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "!XLA_USE_BF16=1 NEURON_CC_FLAGS=\"--cache_dir=./compiler_cache\" \\\n",
    "torchrun --nproc_per_node=2 run_image_classification.py \\\n",
    "--model_name_or_path \"google/vit-base-patch16-224-in21k\" \\\n",
    "--dataset_name \"beans\" \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--num_train_epochs 10 \\\n",
    "--per_device_train_batch_size 16 \\\n",
    "--per_device_eval_batch_size 16 \\\n",
    "--learning_rate 2e-5 \\\n",
    "--logging_strategy steps \\\n",
    "--logging_steps 10 \\\n",
    "--save_strategy epoch \\\n",
    "--save_total_limit 1 \\\n",
    "--seed 1337 \\\n",
    "--remove_unused_columns False \\\n",
    "--overwrite_output_dir \\\n",
    "--output_dir \"output\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5101401-df06-4a6d-9780-8832b9f28e01",
   "metadata": {},
   "source": [
    "コンパイル時間を含んだ学習には `inf2.xlarge` 上で実行した場合で 25分程度かかります.\n",
    "2度目以降の実行ではコンパイル済みのキャッシュが利用可能なため、1\\~2分程度で学習が完了します。\n",
    "\n",
    "`neuron_parallel_compile` コマンドを利用したコンパイル時間の削減方法については、[日本語BERTモデルのサンプル](https://github.com/AWShtokoyo/aws-ml-jp/tree/main/frameworks/aws-neuron-jp/bertj_finetuning_classification)を参照下さい。\n",
    "\n",
    "\n",
    "これで　AWS Inferentia2 (AWS Trainium) 上での ViT モデルのファインチューニングに成功しました。 \n",
    "`pytorch_model.bin` という名前のファインチューニングされた重みを持つモデル、`Trainer` の状態、モデル設定ファイル（`config.json`） を含むファイルのリストが表示されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a484cc-a790-4544-ae04-793ea288c16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l ./output/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca27169e-de1c-438e-b095-089dca381925",
   "metadata": {},
   "source": [
    "# ViT 推論"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba773524-55dd-4999-a42d-41deec2e9077",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "import torch\n",
    "import torch_neuronx\n",
    "from transformers import ViTImageProcessor, ViTForImageClassification\n",
    "\n",
    "# Create the feature extractor and model\n",
    "checkpoint_dir = './output/'\n",
    "print(f\"Create model from provided checkpoint: {checkpoint_dir}\")\n",
    "feature_extractor = ViTImageProcessor.from_pretrained(checkpoint_dir)\n",
    "model = ViTForImageClassification.from_pretrained(checkpoint_dir, torchscript=True)\n",
    "model.eval()\n",
    "\n",
    "# Get an example input\n",
    "url = \"https://datasets-server.huggingface.co/assets/beans/--/default/test/0/image/image.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
    "example = (inputs['pixel_values'],)\n",
    "\n",
    "# Run inference on CPU\n",
    "output_cpu = model(*example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6950e6-7e4d-40a5-84d0-fd65e6265f15",
   "metadata": {},
   "source": [
    "## 推論実行のためのモデルの事前コンパイル\n",
    "\n",
    "推論を AWS Inferentia2 (もしくは AWS Trainium) 上で実行するためには、モデルを`torch_neuronx.trace` APIを用いて事前にトレース（コンパイル）する必要があります。トレース（コンパイル）した結果は保存することでデプロイ時に再利用可能です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936134c6-12bb-426f-9316-1f6b9e09bbad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Compile the model for neuron\n",
    "print(f\"Compile model for neuron with torch tracing ...\")\n",
    "model_neuron = torch_neuronx.trace(model, example)\n",
    "\n",
    "# Save the TorchScript for inference deployment\n",
    "filename = 'vit-model-neuron.pt'\n",
    "torch.jit.save(model_neuron, filename)\n",
    "print(f\"Save compiled model as: {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac19f68-ebe6-464a-937f-1bbcb9b9b076",
   "metadata": {},
   "source": [
    "期待通りの出力が得られるかどうか　CPU上での推論結果と比較します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8658aef-2c36-4d1d-bdd5-13bb38db0be8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the TorchScript compiled model\n",
    "print(f\"Load compiled model: {filename}\")\n",
    "model_neuron = torch.jit.load(filename)\n",
    "\n",
    "# Run inference using the Neuron model\n",
    "print(f\"Run inference on the test image: {url}\")\n",
    "output_neuron = model_neuron(*example)\n",
    "\n",
    "# Compare the results\n",
    "print(f\"--- Compare Neuron output against CPU output ----\")\n",
    "print(f\"CPU tensor:            {output_cpu[0][0][0:10]}\")\n",
    "print(f\"Neuron tensor:         {output_neuron[0][0][0:10]}\")\n",
    "print(f\"CPU prediction:    {model.config.id2label[output_cpu[0].argmax(-1).item()]}\")\n",
    "print(f\"Neuron prediction: {model.config.id2label[output_neuron[0].argmax(-1).item()]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bfaae7-da47-458b-b6fa-023e2e5a06f8",
   "metadata": {},
   "source": [
    "## Gradio API を用いた推論デモ\n",
    "\n",
    "モデルサービスのデモをセットアップする簡易な方法は、Gradio API を使用することです。画像をアップロードしてモデルに与え、推論結果を確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a024e5-b244-488f-9bee-7d7b74a59f5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "from torchvision.transforms import (\n",
    "    CenterCrop,\n",
    "    Compose,\n",
    "    Normalize,\n",
    "    RandomHorizontalFlip,\n",
    "    RandomResizedCrop,\n",
    "    Resize,\n",
    "    ToTensor,\n",
    ")\n",
    "\n",
    "id2label = {0: 'angular_leaf_spot　(角葉スポット)', 1: 'bean_rust　(豆さび病)', 2: 'healthy　(健康)'}\n",
    "\n",
    "def predict(raw_image):\n",
    "    size = (224, 224)\n",
    "    image_mean = [0.5, 0.5, 0.5]\n",
    "    image_std = [0.5, 0.5, 0.5]\n",
    "    normalize = Normalize(mean=image_mean, std=image_std)\n",
    "    \n",
    "    _val_transforms = Compose(\n",
    "        [\n",
    "            Resize(size),\n",
    "            CenterCrop(size),\n",
    "            ToTensor(),\n",
    "            normalize,\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    transformed_image = _val_transforms(raw_image.convert(\"RGB\"))\n",
    "    batched_transformed_image = transformed_image.unsqueeze(0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        prediction = model_neuron(batched_transformed_image)\n",
    "        pred = id2label[prediction[0].argmax(-1).item()]\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a1f39d-d001-46d1-a2cf-a19edcad9a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "demo = gr.Interface(fn=predict,\n",
    "             inputs=gr.Image(type=\"pil\"),\n",
    "             outputs=\"text\",\n",
    "             examples=[\n",
    "                 'image_samples/healthy_test.21.jpg',\n",
    "                 'image_samples/angular_leaf_spot_test.21.jpg',\n",
    "                 'image_samples/bean_rust_test.34.jpg'])\n",
    "\n",
    "demo.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
