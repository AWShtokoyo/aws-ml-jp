{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb457992-5926-4ad4-986a-944cd5d7411a",
   "metadata": {},
   "source": [
    "## 音声認識モデル Whisper を SageMaker 上でデプロイして試してみる\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742c9146-bd9e-43b1-94e3-ee47eca24d96",
   "metadata": {},
   "source": [
    "## 環境のセットアップ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7102577-c9e3-4c00-b7c1-d853bc71ae63",
   "metadata": {},
   "source": [
    "### モジュールのインストール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9fe16419-878f-4f18-8b7d-b321b1f96726",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: boto3 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (1.24.93)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from boto3) (0.6.0)\n",
      "Requirement already satisfied: botocore<1.28.0,>=1.27.93 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from boto3) (1.27.93)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from boto3) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from botocore<1.28.0,>=1.27.93->boto3) (2.8.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from botocore<1.28.0,>=1.27.93->boto3) (1.26.7)\n",
      "Requirement already satisfied: six>=1.5 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.28.0,>=1.27.93->boto3) (1.16.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.3 is available.\n",
      "You should consider upgrading via the '/home/studio-lab-user/.conda/envs/default/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: sagemaker in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (2.112.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from sagemaker) (1.19.5)\n",
      "Requirement already satisfied: protobuf<4.0,>=3.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from sagemaker) (3.19.1)\n",
      "Requirement already satisfied: protobuf3-to-dict<1.0,>=0.1.5 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from sagemaker) (0.1.5)\n",
      "Requirement already satisfied: attrs<23,>=20.3.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from sagemaker) (21.2.0)\n",
      "Requirement already satisfied: boto3<2.0,>=1.20.21 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from sagemaker) (1.24.93)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from sagemaker) (21.3)\n",
      "Requirement already satisfied: pathos in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from sagemaker) (0.2.8)\n",
      "Requirement already satisfied: importlib-metadata<5.0,>=1.4.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from sagemaker) (4.8.2)\n",
      "Requirement already satisfied: schema in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from sagemaker) (0.7.5)\n",
      "Requirement already satisfied: pandas in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from sagemaker) (1.1.5)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from sagemaker) (1.0.1)\n",
      "Requirement already satisfied: google-pasta in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied: botocore<1.28.0,>=1.27.93 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from boto3<2.0,>=1.20.21->sagemaker) (1.27.93)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from boto3<2.0,>=1.20.21->sagemaker) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from boto3<2.0,>=1.20.21->sagemaker) (0.6.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from importlib-metadata<5.0,>=1.4.0->sagemaker) (3.6.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from packaging>=20.0->sagemaker) (3.0.6)\n",
      "Requirement already satisfied: six in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from protobuf3-to-dict<1.0,>=0.1.5->sagemaker) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from pandas->sagemaker) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from pandas->sagemaker) (2021.3)\n",
      "Requirement already satisfied: dill>=0.3.4 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from pathos->sagemaker) (0.3.4)\n",
      "Requirement already satisfied: multiprocess>=0.70.12 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from pathos->sagemaker) (0.70.12.2)\n",
      "Requirement already satisfied: pox>=0.3.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from pathos->sagemaker) (0.3.0)\n",
      "Requirement already satisfied: ppft>=1.6.6.4 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from pathos->sagemaker) (1.6.6.4)\n",
      "Requirement already satisfied: contextlib2>=0.5.5 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from schema->sagemaker) (21.6.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from botocore<1.28.0,>=1.27.93->boto3<2.0,>=1.20.21->sagemaker) (1.26.7)\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.3 is available.\n",
      "You should consider upgrading via the '/home/studio-lab-user/.conda/envs/default/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: transformers in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (4.12.2)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.23.1-py3-none-any.whl (5.3 MB)\n",
      "     |████████████████████████████████| 5.3 MB 3.9 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from transformers) (2021.11.10)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from transformers) (4.49.0)\n",
      "Collecting huggingface-hub<1.0,>=0.10.0\n",
      "  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n",
      "     |████████████████████████████████| 163 kB 63.2 MB/s            \n",
      "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
      "     |████████████████████████████████| 7.6 MB 42.1 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: filelock in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from transformers) (3.4.0)\n",
      "Requirement already satisfied: requests in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from transformers) (2.26.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.0.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from packaging>=20.0->transformers) (3.0.6)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests->transformers) (2.0.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests->transformers) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests->transformers) (2021.10.8)\n",
      "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.10.3\n",
      "    Uninstalling tokenizers-0.10.3:\n",
      "      Successfully uninstalled tokenizers-0.10.3\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.0.19\n",
      "    Uninstalling huggingface-hub-0.0.19:\n",
      "      Successfully uninstalled huggingface-hub-0.0.19\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.12.2\n",
      "    Uninstalling transformers-4.12.2:\n",
      "      Successfully uninstalled transformers-4.12.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "datasets 1.6.2 requires huggingface-hub<0.1.0, but you have huggingface-hub 0.10.1 which is incompatible.\u001b[0m\n",
      "Successfully installed huggingface-hub-0.10.1 tokenizers-0.13.1 transformers-4.23.1\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.3 is available.\n",
      "You should consider upgrading via the '/home/studio-lab-user/.conda/envs/default/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -U boto3\n",
    "!pip install -U sagemaker\n",
    "!pip install -U transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9125d292-4432-4c86-bfa9-80bb7c78dca0",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "### IAM ユーザーの作成と認証情報の設定\n",
    "今回のサンプルでは、Studio Lab で動かしている Notebook 上ではなく AWS 環境上で Whisper をデプロイして使い方を確かめてみます。そのためには、Notebook から AWS 環境にアクセスする必要があるためその認証情報をこれから設定します。\n",
    "\n",
    "IAM ユーザーを作成し、そこから得られるアクセスキーとシークレットキーを登録します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce65ec6c-73d8-4edd-afd1-e0ce6bb2afa6",
   "metadata": {
    "tags": []
   },
   "source": [
    "まずは、AWS のコンソール画面を開いて左上の検索窓で「IAM」と検索します。トップに出てくる IAM をクリックして IAM のサービスページを開きます。\n",
    "\n",
    "![](./imgs/001_IAM_search.png)\n",
    "\n",
    "左側メニューから、「ユーザー」をクリックして IAM ユーザーの設定画面に遷移します。\n",
    "\n",
    "![](./imgs/002_IAM_user.png)\n",
    "\n",
    "次に、「ユーザーを追加」をクリックしてユーザーの作成を開始します。\n",
    "\n",
    "![](./imgs/003_user_create.png)\n",
    "\n",
    "ユーザー名に「whisper-sample-user」（他の名称でも大丈夫です）、「アクセスキー - プログラムによるアクセス」にチェックをつけます。\n",
    "\n",
    "![](./imgs/004_user_info.png)\n",
    "\n",
    "「次のステップ」をクリックします。  \n",
    "その後「既存のポリシーを直接アタッチ」を選択し、ポリシーの検索で「SageMakerFullAccess」と入力します。そうすると「AWSSageMakerFullAccess」のポリシー候補が現れるのでこれを選択します。\n",
    "\n",
    "![](./imgs/005_user_policy.png)\n",
    "\n",
    "次に、検索窓に「PowerUserAccess」と検索し候補に出てきた「PowerUserAccess」を選択します。  \n",
    "\n",
    "![](./imgs/006_user_poweruseraccess.png)\n",
    "\n",
    "「次のステップ」をクリックするとタグの設定画面が出てきますが、ここは特に入力せずにスキップします。  \n",
    "\n",
    "これまでに設定した項目の確認ページが出てくるので問題なければ「ユーザーの作成」をクリックします。\n",
    "\n",
    "![](./imgs/007_user_confirmation.png)  \n",
    "\n",
    "無事ユーザーが作成されるとユーザーキーとシークレットキーが表示されるのでメモに残しておきます。これらの情報を使って Studio Lab 経由で AWS 環境にアクセスを行います。  \n",
    "**ここで取得されるクレデンシャル情報の扱いには十分注意してください**。\n",
    "\n",
    "![](./imgs/008_user_credentials.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dadf6c3-538b-4a45-9c2c-3731fca3d10f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "次に、 Studio Lab の画面に戻って先ほど取得したアクセスキーなどの情報を登録していきます。  \n",
    "\n",
    "画面上部のメニューから 「File -> New -> Terminal」 と選択してターミナルの起動をします。  \n",
    "![](./imgs/009_start_terminal.png)\n",
    "\n",
    "開かれたターミナルで `aws configure` を実行します。\n",
    "そこでアクセスキーとシークレットキーを聞かれるので先ほどメモした値を入力します。  \n",
    "\n",
    "![](./imgs/010_aws_configure.png)\n",
    "\n",
    "以上で、認証情報の設定は完了です。ではこれから実際にモデルを動かしていきましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7125d1-f5be-4077-8bb7-c1cabcb15f99",
   "metadata": {
    "tags": []
   },
   "source": [
    "### SageMaker Inference Instance が使用する IAM ロールを作成する\n",
    "\n",
    "今度は、SageMaker 側でモデルをデプロイする際にデプロイされたインスタンスに付与される IAM ロールを作成します。  \n",
    "\n",
    "下記のハンズオン資料の「2-3. SageMaker Training Instance が利用する IAM ロールを作成する」で紹介されている手順とほぼ同じです。  \n",
    "- https://github.com/aws-samples/aws-ml-enablement-workshop/blob/main/notebooks/scenario_churn/customer_churn_sagemaker.ipynb \n",
    "\n",
    "AWS のコンソール画面に戻ります。  \n",
    "先ほどと同様の手順で IAM のサービス画面を開き、「ロール」を左側のメニューから選択します。 \n",
    "IAM ロールの画面が開かれたら「ロールの作成」ボタンをクリックします。  \n",
    "\n",
    "![](./imgs/011_role_create.png)  \n",
    "\n",
    "ロールの作成画面が表示されたら信頼されるエンティティタプとして「AWS のサービス」を選択し、ユースケースのところは下の検索欄から「SageMaker」などと検索して SageMaker を選択します。\n",
    "\n",
    "![](./imgs/012_role_entity.png)\n",
    "\n",
    "「次へ」をクリックし、「AmazonSageMakerFullAccess」のポリシーがアタッチされていることを確認します。  \n",
    "\n",
    "![](./imgs/013_role_policy.png)\n",
    "\n",
    "「次へ」をクリックし、Role 名を設定します。「StudioLabWhisperExecutionRole」と入力し、他の項目はいじらずに「ロールを作成」をクリックします。\n",
    "\n",
    "![](./imgs/014_role_name.png)\n",
    "\n",
    "作成した IAM ロールのリソースネームである ARN を取得します。  \n",
    "IAM ロールの画面から、検索欄で「StudioLabWhisper」などと入力して先ほど作成した IAM ロールを探して選択します。  \n",
    "\n",
    "![](./imgs/015_role_search.png)  \n",
    "\n",
    "IAM ロールの詳細情報が表示されるので、ARN の隣にあるコピーボタンをクリックして ARN をコピーします。\n",
    "![](./imgs/016_role_arn_copy.png)  \n",
    "\n",
    "コピペした値を置き換えて role の値を設定します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12110312-6048-48bd-abfd-0806ca64e17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "role = \"arn:aws:iam::392304288222:role/StudioLabWhisperExecutionRole\"  # コピペした値で置き換える"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e272c23-d3b3-47de-8aac-c7955660263d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Whisper のデプロイ\n",
    "\n",
    "まずは、 SageMaker SDK の HuggingFace 拡張を使って簡単にモデルをデプロイしてみましょう。  \n",
    "以下のページで公開されている「whisper-base」と呼ばれているモデルを使っていきます。  \n",
    "\n",
    "- https://huggingface.co/openai/whisper-base\n",
    "\n",
    "この HuggingFace のページ上で SageMaker でモデルをデプロイするためのコードを手軽に生成できます。  \n",
    "\n",
    "まずは、ページにある「Deploy」ボタンをクリックします。  \n",
    "\n",
    "![](./imgs/101_deploy_button.png)\n",
    "\n",
    "いくつかデプロイの選択肢が出てくるので今回は「Amazon SageMaker」を選択します。  \n",
    "\n",
    "![](./imgs/102_deploy_select_sagemaker.png)\n",
    "\n",
    "Task を「Automatic Speech Recognition」、Configuration を「AWS」に設定すると deploy 用のコードが生成されます。  \n",
    "\n",
    "![](./imgs/103_deploy_generate_code.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2215719b-ea07-46e7-b65e-b93086b73873",
   "metadata": {},
   "source": [
    "コピーしたコードはデプロイのコードと推論のコードが含まれています。  \n",
    "推論部分はいじる必要があるため、コメントアウトします。また、デプロイ部分も IAM ロールの取得部分だけコメントアウトをしておきます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738d8ee7-78cb-45f6-8fb2-5ff98e2480c9",
   "metadata": {
    "tags": []
   },
   "source": [
    "```python\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "import sagemaker\n",
    "\n",
    "# 下の行をコメントアウト。先ほど作成したロールを使う。\n",
    "# role = sagemaker.get_execution_role()\n",
    "# Hub Model configuration. https://huggingface.co/models\n",
    "hub = {\n",
    "\t'HF_MODEL_ID':'openai/whisper-base',\n",
    "\t'HF_TASK':'automatic-speech-recognition'\n",
    "}\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "\ttransformers_version='4.23.1',  # バージョンは 4.23 以降に\n",
    "\tpytorch_version='1.10.2',\n",
    "\tpy_version='py38',\n",
    "\tenv=hub,\n",
    "\trole=role, \n",
    ")\n",
    "\n",
    "# deploy model to SageMaker Inference\n",
    "predictor = huggingface_model.deploy(\n",
    "\tinitial_instance_count=1, # number of instances\n",
    "\tinstance_type='ml.m5.xlarge' # ec2 instance type\n",
    ")\n",
    "\n",
    "# 推論部分も別途実装するためコメントアウト\n",
    "# predictor.predict({\n",
    "# \t'inputs': \"sample1.flac\"\n",
    "# })\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5af732d5-811b-4b71-bc20-a31e6c5ad25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "9f260115-dd7f-441f-95d0-6a781f210721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting code/inference.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile code/inference.py\n",
    "\n",
    "from transformers import pipeline\n",
    "from transformers.pipelines import AutomaticSpeechRecognitionPipeline\n",
    "import numpy as np\n",
    "\n",
    "def model_fn(model_dir) -> AutomaticSpeechRecognitionPipeline:\n",
    "    return pipeline(model=\"facebook/wav2vec2-base-960h\")  \n",
    "\n",
    "def predict_fn(data, pipeline):\n",
    "    inputs = data.pop(\"inputs\", data)\n",
    "    parameters = data.pop(\"parameters\", None)\n",
    "    if type(inputs) == list:\n",
    "        inputs = np.array(inputs, dtype=np.float)\n",
    "    print(\"inputs are: \", inputs)\n",
    "    # pass inputs with all kwargs in data\n",
    "    if parameters is not None:\n",
    "        prediction = pipeline(inputs, **parameters)\n",
    "    else:\n",
    "        prediction = pipeline(inputs)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "77c58331-044f-4ef4-b00c-21d545b0275b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/huggingface/notebooks/blob/main/sagemaker/17_custom_inference_script/sagemaker-notebook.ipynb を参考に\n",
    "s3_bucket_name = \"sagemaker-ap-northeast-1-392304288222\"\n",
    "# repository = \"openai/whisper-tiny\"\n",
    "repository = \"facebook/wav2vec2-base-960h\"\n",
    "model_id = repository.split(\"/\")[-1]\n",
    "s3_location = f\"s3://{s3_bucket_name}/custom_inference/{model_id}/model.tar.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "e17c8f24-6fa9-4c77-a035-7ec8e93708f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/studio-lab-user/aws-ml-jp/sagemaker/studio-lab-whisper\n"
     ]
    }
   ],
   "source": [
    "%cd ~/aws-ml-jp/sagemaker/studio-lab-whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "3c14c6f2-05a6-4453-ab51-5ffdb82b6b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated git hooks.\n",
      "Git LFS initialized.\n",
      "fatal: destination path 'wav2vec2-base-960h' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git lfs install\n",
    "!git clone https://huggingface.co/$repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "9bab7282-2672-4095-8143-db777c4887ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp -r code/ $model_id/code/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "f27c7f3e-6791-4fbe-9bb4-4b08bbf45eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/studio-lab-user/aws-ml-jp/sagemaker/studio-lab-whisper/wav2vec2-base-960h\n",
      "README.md\n",
      "code/\n",
      "code/inference.py\n",
      "code/.ipynb_checkpoints/\n",
      "code/code/\n",
      "code/code/inference.py\n",
      "config.json\n",
      "feature_extractor_config.json\n",
      "preprocessor_config.json\n",
      "pytorch_model.bin\n",
      "special_tokens_map.json\n",
      "tf_model.h5\n",
      "tokenizer_config.json\n",
      "vocab.json\n"
     ]
    }
   ],
   "source": [
    "%cd $model_id\n",
    "!rm model.tar.gz\n",
    "!tar zcvf model.tar.gz *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c254f0-302a-4d66-acf0-352933a64e6b",
   "metadata": {},
   "source": [
    "圧縮したモデルと推論コードを S3 にアップロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "fae82f62-edeb-4683-9afe-c34e4237d212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ./model.tar.gz to s3://sagemaker-ap-northeast-1-392304288222/custom_inference/wav2vec2-base-960h/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp model.tar.gz $s3_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "beb12253-7f5c-45ec-b05d-2e63c2f207db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----!"
     ]
    }
   ],
   "source": [
    "huggingface_model = HuggingFaceModel(\n",
    "    model_data=s3_location,\n",
    "    role=role,\n",
    "    transformers_version=\"4.17.0\",\n",
    "    pytorch_version=\"1.10.2\",\n",
    "    py_version=\"py38\"\n",
    ")\n",
    "\n",
    "predictor = huggingface_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.m5.2xlarge\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818ce858-849c-46ff-a7cc-e0cfe754913d",
   "metadata": {},
   "source": [
    "`ffmpeg -i <infile> -ac 2 -f wav <outfile>`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f33fadb-ca7b-452c-bc94-cf5f27cd51b8",
   "metadata": {},
   "source": [
    "## デプロイしたモデルに音声認識させてみる"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8e824f-a40a-435b-981d-25fd0d97107f",
   "metadata": {},
   "source": [
    "では次に、デプロイしたモデルに対して音声認識をさせてみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "e3aaa010-f30c-4190-8cc8-deeb1e657013",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "292a209a-eefb-4a15-88f6-c5188537a434",
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_file = scipy.io.wavfile.read(\"audio_message.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "59cf6755-3594-48d7-a5e1-1b40d87b1171",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(219136,)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wav_file[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "1b55c4a5-da0d-41a9-b2d2-b221ddc21d64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'UROUND ME ON YOU E'}"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_array = np.random.randn(10000)\n",
    "\n",
    "predictor.predict({\n",
    "    'inputs': wav_file[1]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d356d5a-10d0-44c9-9782-84e15bc5b8c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----!"
     ]
    }
   ],
   "source": [
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "import sagemaker\n",
    "\n",
    "# 下の行をコメントアウト。先ほど作成したロールを使う。\n",
    "# role = sagemaker.get_execution_role()\n",
    "# Hub Model configuration. https://huggingface.co/models\n",
    "hub = {\n",
    "    # 'HF_MODEL_ID':'openai/whisper-base',\n",
    "    'HF_MODEL_ID': 'facebook/wav2vec2-base-960h',\n",
    "    'HF_TASK':'automatic-speech-recognition'\n",
    "}\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "    transformers_version='4.17.0',\n",
    "    pytorch_version='1.10.2',\n",
    "    py_version='py38',\n",
    "    env=hub,\n",
    "    role=role, \n",
    ")\n",
    "\n",
    "# deploy model to SageMaker Inference\n",
    "predictor = huggingface_model.deploy(\n",
    "    initial_instance_count=1, # number of instances\n",
    "    instance_type='ml.m5.xlarge' # ec2 instance type\n",
    ")\n",
    "\n",
    "# predictor.predict({\n",
    "#     'inputs': \"sample1.flac\"\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cbe180cb-088d-47b7-9d87-520905051117",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.serializer = sagemaker.serializers.NumpySerializer()\n",
    "predictor.deserializer = sagemaker.deserializers.JSONDeserializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "efa1b7d7-40ef-4968-946f-db372bf8f693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.pipelines.automatic_speech_recognition.AutomaticSpeechRecognitionPipeline"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "52d166bf-a81c-437d-9ddb-a323fd28eb0f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModelError",
     "evalue": "An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (400) from primary with message \"{\n  \"code\": 400,\n  \"type\": \"InternalServerException\",\n  \"message\": \"\\u0027numpy.ndarray\\u0027 object has no attribute \\u0027pop\\u0027\"\n}\n\". See https://ap-northeast-1.console.aws.amazon.com/cloudwatch/home?region=ap-northeast-1#logEventViewer:group=/aws/sagemaker/Endpoints/huggingface-pytorch-inference-2022-10-19-05-44-57-194 in account 392304288222 for more information.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModelError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-3ce09975fefc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0minput_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m predictor.predict({\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;34m'inputs'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minput_array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m })\n",
      "\u001b[0;32m~/.conda/envs/default/lib/python3.9/site-packages/sagemaker/predictor.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, data, initial_args, target_model, target_variant, inference_id)\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_variant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minference_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         )\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_runtime_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mrequest_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/default/lib/python3.9/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    512\u001b[0m                 )\n\u001b[1;32m    513\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 514\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/default/lib/python3.9/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    936\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    937\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 938\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    939\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    940\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModelError\u001b[0m: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (400) from primary with message \"{\n  \"code\": 400,\n  \"type\": \"InternalServerException\",\n  \"message\": \"\\u0027numpy.ndarray\\u0027 object has no attribute \\u0027pop\\u0027\"\n}\n\". See https://ap-northeast-1.console.aws.amazon.com/cloudwatch/home?region=ap-northeast-1#logEventViewer:group=/aws/sagemaker/Endpoints/huggingface-pytorch-inference-2022-10-19-05-44-57-194 in account 392304288222 for more information."
     ]
    }
   ],
   "source": [
    "import json\n",
    "input_array = np.random.randn(1, 10000)\n",
    "\n",
    "predictor.predict({\n",
    "    'inputs': input_array\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c485e2b-38ad-49b5-90fe-322d84ee4193",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76dcf88b-7b44-411e-ad90-a3ff36ce5abc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.10.0'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1562b765-64b8-456d-9d9f-e657b9c9c157",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (2.6.1)\n",
      "Requirement already satisfied: pandas in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from datasets) (1.1.5)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from datasets) (4.64.1)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from datasets) (2022.8.2)\n",
      "Requirement already satisfied: aiohttp in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from datasets) (3.8.1)\n",
      "Requirement already satisfied: packaging in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from datasets) (2.26.0)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: multiprocess in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from datasets) (0.70.12.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from datasets) (1.19.5)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from datasets) (0.10.1)\n",
      "Requirement already satisfied: dill<0.3.6 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from datasets) (0.3.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: xxhash in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: responses<0.19 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.1)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from aiohttp->datasets) (2.0.8)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from aiohttp->datasets) (5.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from aiohttp->datasets) (21.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from aiohttp->datasets) (1.7.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: filelock in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.0.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from packaging->datasets) (3.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (3.3)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from pandas->datasets) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.3 is available.\n",
      "You should consider upgrading via the '/home/studio-lab-user/.conda/envs/default/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting soundfile\n",
      "  Downloading soundfile-0.11.0-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: cffi>=1.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from soundfile) (1.15.0)\n",
      "Requirement already satisfied: pycparser in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from cffi>=1.0->soundfile) (2.21)\n",
      "Installing collected packages: soundfile\n",
      "Successfully installed soundfile-0.11.0\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.3 is available.\n",
      "You should consider upgrading via the '/home/studio-lab-user/.conda/envs/default/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -U datasets\n",
    "!pip install -U soundfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39497e3f-6a36-4806-9559-92c3f5594740",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.6.2'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "datasets.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d741fb9d-c0bc-45da-81d8-19e4d56616aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "input_array = torch.from_numpy(np.random.randn(10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81071ccf-57f5-4db2-b1de-c739d9159a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It is strongly recommended to pass the `sampling_rate` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    }
   ],
   "source": [
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "# load model and processor\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-tiny\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny\")\n",
    "\n",
    "# load dummy dataset and read soundfiles\n",
    "\n",
    "# ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    "input_features = processor(input_array, return_tensors=\"pt\").input_features \n",
    "\n",
    "# Generate logits\n",
    "\n",
    "logits = model(input_features, decoder_input_ids = torch.tensor([[50258]])).logits \n",
    "\n",
    "# take argmax and decode\n",
    "\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "transcription = processor.batch_decode(predicted_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63985361-f490-43fb-a570-f4df388e59c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54810792551346c6ae22b7fad7b54b98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/3.85k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(model=\"openai/whisper-tiny\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac1613b6-e958-4f21-a3c3-fc8ea1d59dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/transformers/generation_utils.py:1296: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': ' you'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(np.random.randn(10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64431ffa-9321-4e89-847f-d2b5f6578b09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m      \u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mType:\u001b[0m           AutomaticSpeechRecognitionPipeline\n",
       "\u001b[0;31mString form:\u001b[0m    <transformers.pipelines.automatic_speech_recognition.AutomaticSpeechRecognitionPipeline object at 0x7f915021d400>\n",
       "\u001b[0;31mFile:\u001b[0m           ~/.conda/envs/default/lib/python3.9/site-packages/transformers/pipelines/automatic_speech_recognition.py\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Pipeline that aims at extracting spoken text contained within some audio.\n",
       "\n",
       "The input can be either a raw waveform or a audio file. In case of the audio file, ffmpeg should be installed for\n",
       "to support multiple audio formats\n",
       "\n",
       "Arguments:\n",
       "    model ([`PreTrainedModel`] or [`TFPreTrainedModel`]):\n",
       "        The model that will be used by the pipeline to make predictions. This needs to be a model inheriting from\n",
       "        [`PreTrainedModel`] for PyTorch and [`TFPreTrainedModel`] for TensorFlow.\n",
       "    tokenizer ([`PreTrainedTokenizer`]):\n",
       "        The tokenizer that will be used by the pipeline to encode data for the model. This object inherits from\n",
       "        [`PreTrainedTokenizer`].\n",
       "    feature_extractor ([`SequenceFeatureExtractor`]):\n",
       "        The feature extractor that will be used by the pipeline to encode waveform for the model.\n",
       "    chunk_length_s (`float`, *optional*, defaults to 0):\n",
       "        The input length for in each chunk. If `chunk_length_s = 0` then chunking is disabled (default). Only\n",
       "        available for CTC models, e.g. [`Wav2Vec2ForCTC`].\n",
       "\n",
       "        <Tip>\n",
       "\n",
       "        For more information on how to effectively use `chunk_length_s`, please have a look at the [ASR chunking\n",
       "        blog post](https://huggingface.co/blog/asr-chunking).\n",
       "\n",
       "        </Tip>\n",
       "\n",
       "    stride_length_s (`float`, *optional*, defaults to `chunk_length_s / 6`):\n",
       "        The length of stride on the left and right of each chunk. Used only with `chunk_length_s > 0`. This enables\n",
       "        the model to *see* more context and infer letters better than without this context but the pipeline\n",
       "        discards the stride bits at the end to make the final reconstitution as perfect as possible.\n",
       "\n",
       "        <Tip>\n",
       "\n",
       "        For more information on how to effectively use `stride_length_s`, please have a look at the [ASR chunking\n",
       "        blog post](https://huggingface.co/blog/asr-chunking).\n",
       "\n",
       "        </Tip>\n",
       "\n",
       "    framework (`str`, *optional*):\n",
       "        The framework to use, either `\"pt\"` for PyTorch or `\"tf\"` for TensorFlow. The specified framework must be\n",
       "        installed. If no framework is specified, will default to the one currently installed. If no framework is\n",
       "        specified and both frameworks are installed, will default to the framework of the `model`, or to PyTorch if\n",
       "        no model is provided.\n",
       "    device (`int`, *optional*, defaults to -1):\n",
       "        Device ordinal for CPU/GPU supports. Setting this to -1 will leverage CPU, a positive will run the model on\n",
       "        the associated CUDA device id.\n",
       "    decoder (`pyctcdecode.BeamSearchDecoderCTC`, *optional*):\n",
       "        [PyCTCDecode's\n",
       "        BeamSearchDecoderCTC](https://github.com/kensho-technologies/pyctcdecode/blob/2fd33dc37c4111417e08d89ccd23d28e9b308d19/pyctcdecode/decoder.py#L180)\n",
       "        can be passed for language model boosted decoding. See [`Wav2Vec2ProcessorWithLM`] for more information.\n",
       "\u001b[0;31mCall docstring:\u001b[0m\n",
       "Classify the sequence(s) given as inputs. See the [`AutomaticSpeechRecognitionPipeline`] documentation for more\n",
       "information.\n",
       "\n",
       "Args:\n",
       "    inputs (`np.ndarray` or `bytes` or `str` or `dict`):\n",
       "        The inputs is either :\n",
       "            - `str` that is the filename of the audio file, the file will be read at the correct sampling rate\n",
       "              to get the waveform using *ffmpeg*. This requires *ffmpeg* to be installed on the system.\n",
       "            - `bytes` it is supposed to be the content of an audio file and is interpreted by *ffmpeg* in the\n",
       "              same way.\n",
       "            - (`np.ndarray` of shape (n, ) of type `np.float32` or `np.float64`)\n",
       "                Raw audio at the correct sampling rate (no further check will be done)\n",
       "            - `dict` form can be used to pass raw audio sampled at arbitrary `sampling_rate` and let this\n",
       "              pipeline do the resampling. The dict must be in the format `{\"sampling_rate\": int, \"raw\":\n",
       "              np.array}` with optionally a `\"stride\": (left: int, right: int)` than can ask the pipeline to\n",
       "              treat the first `left` samples and last `right` samples to be ignored in decoding (but used at\n",
       "              inference to provide more context to the model). Only use `stride` with CTC models.\n",
       "    return_timestamps (*optional*, `str`):\n",
       "        Only available for pure CTC models. If set to `\"char\"`, the pipeline will return `timestamps` along the\n",
       "        text for every character in the text. For instance if you get `[{\"text\": \"h\", \"timestamps\": (0.5,0.6),\n",
       "        {\"text\": \"i\", \"timestamps\": (0.7, .9)}]`, then it means the model predicts that the letter \"h\" was\n",
       "        pronounced after `0.5` and before `0.6` seconds. If set to `\"word\"`, the pipeline will return\n",
       "        `timestamps` along the text for every word in the text. For instance if you get `[{\"text\": \"hi \",\n",
       "        \"timestamps\": (0.5,0.9), {\"text\": \"there\", \"timestamps\": (1.0, .1.5)}]`, then it means the model\n",
       "        predicts that the word \"hi\" was pronounced after `0.5` and before `0.9` seconds.\n",
       "\n",
       "Return:\n",
       "    `Dict`: A dictionary with the following keys:\n",
       "        - **text** (`str` ) -- The recognized text.\n",
       "        - **chunks** (*optional(, `List[Dict]`)\n",
       "                When using `return_timestamps`, the `chunks` will become a list containing all the various text\n",
       "                chunks identified by the model, *e.g.* `[{\"text\": \"hi \", \"timestamps\": (0.5,0.9), {\"text\":\n",
       "                \"there\", \"timestamps\": (1.0, 1.5)}]`. The original full text can roughly be recovered by doing\n",
       "                `\"\".join(chunk[\"text\"] for chunk in output[\"chunks\"])`.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dcef7172-0f0f-4938-a084-9d5489e3bdb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 51865])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64cce911-3902-457a-870d-38826f09e39e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|nocaptions|>']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bfa785a6-6459-4213-bb29-54faa64613f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.12.2'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ad85e02-c492-4907-a071-013e0140b087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sagemaker.huggingface.model.HuggingFacePredictor at 0x7f9d22e69a90>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20ad09aa-19b6-4c68-9282-67c3cc3d0bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dde6e366-ef5e-49af-9aa9-6999a970411b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception while loading config file /etc/jupyter/jupyter_notebook_config.py\n",
      "    Traceback (most recent call last):\n",
      "      File \"/etc/jupyter/jupyter_notebook_config.py\", line 2, in <module>\n",
      "        from amzn_sagemaker_studiolab.managers.kernelspec_managers import SageMakerStudioLabKernelSpecManager as KernelSpecManager\n",
      "    ModuleNotFoundError: No module named 'amzn_sagemaker_studiolab'\n",
      "    \n",
      "    During handling of the above exception, another exception occurred:\n",
      "    \n",
      "    Traceback (most recent call last):\n",
      "      File \"/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/traitlets/config/application.py\", line 738, in _load_config_files\n",
      "        config = loader.load_config()\n",
      "      File \"/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/traitlets/config/loader.py\", line 614, in load_config\n",
      "        self._read_file_as_dict()\n",
      "      File \"/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/traitlets/config/loader.py\", line 646, in _read_file_as_dict\n",
      "        exec(compile(f.read(), conf_filename, 'exec'), namespace, namespace)\n",
      "      File \"/etc/jupyter/jupyter_notebook_config.py\", line 4, in <module>\n",
      "        from nb_conda_kernels import CondaKernelSpecManager as KernelSpecManager\n",
      "    ModuleNotFoundError: No module named 'nb_conda_kernels'\n",
      "Enabling notebook extension jupyter-js-widgets/extension...\n",
      "      - Validating: \u001b[32mOK\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip install torchaudio ipywebrtc\n",
    "# !conda install -c conda-forge ffmpeg\n",
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b42c24c4-08ef-42e0-aefa-de955474d914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): - ^C\n",
      "failed\n",
      "\n",
      "CondaError: KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda install -c conda-forge python-sounddevice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4dc2509c-7559-4b82-85db-2677af25199d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5b9919a4b6c481eb3798bade4c2516b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "AudioRecorder(audio=Audio(value=b'', format='webm'), stream=CameraStream(constraints={'audio': True, 'video': …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywebrtc import AudioRecorder, CameraStream\n",
    "import torchaudio\n",
    "from IPython.display import Audio\n",
    "\n",
    "camera = CameraStream(constraints={'audio': True,'video':False})\n",
    "recorder = AudioRecorder(stream=camera)\n",
    "recorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72de7924-6d3d-4736-89db-d4d8aa6a7bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/sh: 1: ffmpeg: not found\n"
     ]
    }
   ],
   "source": [
    "!ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae4381dd-b205-47d8-ab23-e4ad30f91f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/sh: 1: ffmpeg: not found\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to load audio from file.wav",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-af362dbf90a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecorder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ffmpeg -i recording.webm -ac 1 -f wav file.wav -y -hide_banner -loglevel panic'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchaudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"file.wav\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mAudio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/default/lib/python3.9/site-packages/torchaudio/backend/sox_io_backend.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(filepath, frame_offset, num_frames, normalize, channels_first, format)\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_fallback_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_frames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannels_first\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/default/lib/python3.9/site-packages/torchaudio/backend/sox_io_backend.py\u001b[0m in \u001b[0;36m_fail_load\u001b[0;34m(filepath, frame_offset, num_frames, normalize, channels_first, format)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mformat\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m ) -> Tuple[torch.Tensor, int]:\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Failed to load audio from {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to load audio from file.wav"
     ]
    }
   ],
   "source": [
    "with open('recording.webm', 'wb') as f:\n",
    "    f.write(recorder.audio.value)\n",
    "!ffmpeg -i recording.webm -ac 1 -f wav file.wav -y -hide_banner -loglevel panic\n",
    "sig, sr = torchaudio.load(\"file.wav\")\n",
    "print(sig.shape)\n",
    "Audio(data=sig, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "344233f5-9ddf-4208-a7bd-8681de776990",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Javascript\n",
    "from base64 import b64decode\n",
    "\n",
    "RECORD = \"\"\"\n",
    "const sleep = time => new Promise(resolve => setTimeout(resolve, time))\n",
    "const b2text = blob => new Promise(resolve => {\n",
    "  const reader = new FileReader()\n",
    "  reader.onloadend = e => resolve(e.srcElement.result)\n",
    "  reader.readAsDataURL(blob)\n",
    "})\n",
    "var record = time => new Promise(async resolve => {\n",
    "  stream = await navigator.mediaDevices.getUserMedia({ audio: true })\n",
    "  recorder = new MediaRecorder(stream)\n",
    "  chunks = []\n",
    "  recorder.ondataavailable = e => chunks.push(e.data)\n",
    "  recorder.start()\n",
    "  await sleep(time)\n",
    "  recorder.onstop = async ()=>{\n",
    "    blob = new Blob(chunks)\n",
    "    text = await b2text(blob)\n",
    "    resolve(text)\n",
    "  }\n",
    "  recorder.stop()\n",
    "})\n",
    "\"\"\"\n",
    "\n",
    "def record(sec, filename='audio.wav'):\n",
    "  display(Javascript(RECORD))\n",
    "  \n",
    "  s = output.eval_js('record(%d)' % (sec * 1000))\n",
    "  b = b64decode(s.split(',')[1])\n",
    "  with open(filename, 'wb+') as f:\n",
    "    f.write(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ff3761f-940f-4f89-8c58-f7049faf4701",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "const sleep = time => new Promise(resolve => setTimeout(resolve, time))\n",
       "const b2text = blob => new Promise(resolve => {\n",
       "  const reader = new FileReader()\n",
       "  reader.onloadend = e => resolve(e.srcElement.result)\n",
       "  reader.readAsDataURL(blob)\n",
       "})\n",
       "var record = time => new Promise(async resolve => {\n",
       "  stream = await navigator.mediaDevices.getUserMedia({ audio: true })\n",
       "  recorder = new MediaRecorder(stream)\n",
       "  chunks = []\n",
       "  recorder.ondataavailable = e => chunks.push(e.data)\n",
       "  recorder.start()\n",
       "  await sleep(time)\n",
       "  recorder.onstop = async ()=>{\n",
       "    blob = new Blob(chunks)\n",
       "    text = await b2text(blob)\n",
       "    resolve(text)\n",
       "  }\n",
       "  recorder.stop()\n",
       "})\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'output' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-6d0c58582c7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrecord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-2ae131502465>\u001b[0m in \u001b[0;36mrecord\u001b[0;34m(sec, filename)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'audio.wav'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m   \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mJavascript\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRECORD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m   \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_js\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'record(%d)'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msec\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m   \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb64decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb+'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'output' is not defined"
     ]
    }
   ],
   "source": [
    "record(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7399d25-a412-4768-80e4-e1ab700efab1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
